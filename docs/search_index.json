[
["index.html", "Notas de clase del curso de introducción a Data Science Práctica Guiada Introducción", " Notas de clase del curso de introducción a Data Science Práctica Guiada Diego Kozlowski y Natsumi Shokida 2019-10-11 Introducción Presentación En los últimos años se han difundido muchas herramientas estadísticas novedosas para el análisis de información socioeconómica y geográfica. En particular el software denominado “R”, por tratarse de un software libre, se extiende cada vez más en diferentes disciplinas y recibe el aporte de investigadores e investigadoras en todo el mundo, multiplicando sistemáticamente sus capacidades. Este programa se destaca, entre otras cosas, por su capacidad de trabajar con grandes volúmenes de información, utilizar múltiples bases de datos en simultáneo, generar reportes, realizar gráficos a nivel de publicación y por su comunidad de usuarios que publican sus sintaxis y comparten sus problemas, hecho que potencia la capacidad de consulta y de crecimiento. A su vez, la expresividad del lenguaje permite diseñar funciones específicas que permiten optimizar de forma personalizada el trabajo cotidiano con R. Objetivos del curso El presente Taller tiene como objetivo principal introducir a los participantes en la ciencia de datos, sobre la base de la utilización del lenguaje R aplicado procesamiento de diferentes bases de datos provistas por el programa de Gobierno Abierto y la Encuesta Permanente de Hogares (EPH) - INDEC. Se apunta a brindar las herramientas necesarias para la gestión de la información, presentación de resultados y algunas técnicas de modelado de datos, de forma tal que los participantes puedan luego avanzar por su cuenta a técnicas más avanzadas. Webpage Temario: Eje 1. Programación en R clase 1: Introducción al entorno R: Descripción del programa “R”. Lógica sintáctica del lenguaje y comandos básicos Presentación de la plataforma RStudio para trabajar en “R” Caracteres especiales en “R” Operadores lógicos y aritméticos Definición de Objetos: Valores, Vectores y DataFrames Tipos de variable (numérica, de caracteres, lógicas) Lectura y Escritura de Archivos clase 2: Tidyverse: Limpieza de Base de datos: Renombrar y recodificar variables, tratamiento de valores faltantes (missing values/ NA´s) Seleccionar variables, ordenar y agrupar la base de datos para realizar cálculos Creación de nuevas variables Aplicar filtros sobre la base de datos Construir medidas de resumen de la información Tratamiento de variables numéricas (edad, ingresos, horas de trabajo, cantidad de hijos / componentes del hogar, entre otras). clase 3: Programación funcional Estructuras de código condicionales Loops Creación de funciones a medida del usuario Librería purrr para programación funcional Eje 2. Presentación de resultados clase 4: Visualización de la información Gráficos básicos de R (función “plot”): Comandos para la visualización ágil de la información Gráficos elaborados en R (función “ggplot”): Gráficos de línea, barras, Boxplots y distribuciones de densidad Parámetros de los gráficos: Leyendas, ejes, títulos, notas, colores Gráficos con múltiples cruces de variables. clase 5: Documentación en R Manejo de las extensiones del software “Rmarkdown” y “RNotebook” para elaborar documentos de trabajo, presentaciones interactivas e informes: Opciones para mostrar u ocultar código en los reportes Definición de tamaño, títulos y formato con el cual se despliegan los gráficos y tablas en el informe Caracteres especiales para incluir múltiples recursos en el texto del informe: Links a páginas web, notas al pie, enumeraciones, cambios en el formato de letra (tamaño, negrita, cursiva) Código embebido en el texto para automatización de reportes clase 6: Shiny Shiny como reportes dinámicos Su utilidad para el análisis exploratorio Lógica de servidor- interfaz de usuario Inputs- Outputs, funciones reactivas, widgets. Eje 3. Estadística clase 7: Estadística descriptiva Introducción a probabilidad Introducción a distribuciones El problema de la inversión Estadística Población y muestra Estimadores puntuales, tests de hipótesis Boxplots, histogramas y kernels clase 8: Correlación y Modelo Lineal Análisis de correlación. Presentación conceptual del modelo lineal El modelo lineal desde una perspectiva computacional Supuestos del modelo lineal Modelo lineal en R Modelo lineal en el tidyverse Eje 4. Clases temáticas clase 9: Análisis de encuestas Introducción al diseño de encuestas Presentación de la Encuesta Permanente de Hogares Generación de estadísticos de resumen en muestras estratificadas Utilización de los ponderadores clase 10: Mapas Utilización de información geográfica en R Elaboración de mapas gestión de shapefiles clase 11: Text Mining Introducción al análisis de textos Limpieza Preprocesamiento BoW Stopwords TF-IDF Wordcloud Escrapeo de Twitter Bibliografía de consulta GWickham, H., &amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. &quot; O’Reilly Media, Inc.&quot;. https://es.r4ds.hadley.nz/ James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning. New York: springer. http://faculty.marshall.usc.edu/gareth-james/ISL/ Wickham, Hadley. ggplot2: elegant graphics for data analysis. Springer, 2016. https://ggplot2-book.org/ Librerias a instalar install.packages(c(&quot;tidyverse&quot;,&quot;openxlsx&quot;,&quot;xlsx&quot;,&#39;ggplot2&#39;,&#39;GGally&#39;,&#39;ggridges&#39;,&#39;treemapify&#39;,&#39;esquisse&#39;,&#39;cowplot&#39;,&#39;ggthemes&#39;, &#39;ggrepel&#39;, &#39;ggalt&#39;, &#39;kableExtra&#39;, &#39;fs&#39;, &#39;purrr&#39;, &#39;rmarkdown&#39;, &#39;modelr&#39;, &#39;plot3D&#39;)) "],
["introduccion-a-r.html", "Capítulo-1 Introducción a R", " Capítulo-1 Introducción a R En esta primera clase revisaremos los fundamentos de R base y el entorno de RStudio. El objetivo es poder comenzar a utilizar el programa, abrir archivos y empezar a experimentar para ganar confianza. Descripción del programa R. Lógica sintáctica del lenguaje y comandos básicos Presentación de la plataforma RStudio para trabajar en R Caracteres especiales en R Operadores lógicos y aritméticos Definición de objetos: valores, vectores y DataFrames Tipos de variable (numéricas, de caracteres, lógicas) Lectura y escritura de archivos "],
["explicacion.html", "1.1 Explicación", " 1.1 Explicación https://cran.r-project.org/ 1.1.1 ¿Qué es R? Lenguaje para el procesamiento y análisis estadístico de datos Software Libre Sintaxis Básica: R base Sintaxis incremental1: El lenguaje se va ampliando por aportes de Universidades, investigadores/as, usuarios/as y empresas privadas, organizados en librerías (o paquetes) Comunidad web muy grande para realizar preguntas y despejar dudas. Por ejemplo, en el caso de Buenos Aires contamos con R-Ladies Buenos Aires y RenBaires. Gráficos con calidad de publicación fuente: https://gist.github.com/daroczig/3cf06d6db4be2bbe3368 https://www.rstudio.com/ Uno de los entornos más cómodos para utilizar el lenguaje R es el programa R studio. Rstudio es una empresa que produce productos asociados al lenguaje R, como el programa sobre el que corremos los comandos, y extensiones del lenguaje (librerías). El programa es gratuito y se puede bajar de la página oficial Pantalla Rstudio 1.1.2 Lógica sintáctica. 1.1.2.1 Definición de objetos Los Objetos/Elementos constituyen la categoría esencial del R. De hecho, todo en R es un objeto, y se almacena con un nombre específico que no debe poseer espacios. Un número, un vector, una función, la progresión de letras del abecedario, una base de datos, un gráfico, constituyen para R objetos de distinto tipo. Los objetos que vamos creando a medida que trabajamos pueden visualizarse en el panel derecho superior de la pantalla (el Environment). El operador &lt;- (Alt + Guión) sirve para definir un objeto. A la izquierda del &lt;- debe ubicarse el nombre que tomará el elemento a crear. Del lado derecho debe ir la definición del mismo. A &lt;- 1 Por ejemplo, podemos crear el elemento A, cuyo valor será 1. Para esto, debemos correr el código presionando Ctrl + Enter, con el cursor ubicado en cualquier parte de la línea. Al definir un elemento, el mismo queda guardado en el ambiente del programa, y podrá ser utilizado posteriormente para observar su contenido o para realizar una operación con el mismo. A ## [1] 1 A+6 ## [1] 7 Al correr una linea con el nombre del objeto, la consola del programa nos muestra su contenido. Entre corchetes observamos el número de orden del elemento en cuestión. Si corremos una operación, la consola nos muestra el resultado de la misma. El operador = es equivalente a &lt;-, pero en la práctica no se utiliza para la definición de objetos. B = 2 B ## [1] 2 &lt;- es un operador Unidireccional, es decir que: A &lt;- B implica que A va tomar como valor el contenido del objeto B, y no al revés. A &lt;- B A # Ahora A toma el valor de B, y B continúa conservando el mismo valor ## [1] 2 B ## [1] 2 1.1.3 R base Con R base nos referimos a los comandos básicos que vienen incorporados en el R, sin necesidad de cargar librerías. 1.1.3.1 Operadores lógicos: \\(&gt;\\) (mayor a-) \\(&gt;=\\) (mayor o igual a-) \\(&lt;\\) (menor a-) \\(&lt;=\\) (menor o igual a-) \\(==\\) (igual a-) \\(!=\\) (distinto a-) # Redefinimos los valores A y B A &lt;- 10 B &lt;- 20 # Realizamos comparaciones lógicas A &gt; B ## [1] FALSE A &gt;= B ## [1] FALSE A &lt; B ## [1] TRUE A &lt;= B ## [1] TRUE A == B ## [1] FALSE A != B ## [1] TRUE C &lt;- A != B C ## [1] TRUE Como muestra el último ejemplo, el resultado de una operación lógica puede almacenarse como el valor de un objeto. 1.1.3.2 Operadores aritméticos: #suma A &lt;- 5+6 A ## [1] 11 #Resta B &lt;- 6-8 B ## [1] -2 #cociente C &lt;- 6/2.5 C ## [1] 2.4 #multiplicacion D &lt;- 6*2.5 D ## [1] 15 1.1.3.3 Funciones: Las funciones son series de procedimientos estandarizados, que toman como imput determinados argumentos a fijar por el usuario, y devuelven un resultado acorde a la aplicación de dichos procedimientos. Su lógica de funcionamiento es: funcion(argumento1 = arg1, argumento2 = arg2) A lo largo del curso iremos viendo numerosas funciones, según lo requieran los distintos ejercicios. Sin embargo, veamos ahora algunos ejemplos para comprender su funcionamiento: paste() : concatena una serie de caracteres, pudiendo indicarse cómo separar a cada uno de ellos paste0(): concatena una serie de caracteres sin separar sum(): suma de todos los elementos de un vector mean() promedio aritmético de todos los elementos de un vector paste(&quot;Pega&quot;, &quot;estas&quot;, 4, &quot;palabras&quot;, sep = &quot; &quot;) ## [1] &quot;Pega estas 4 palabras&quot; #Puedo concatenar caracteres almacenados en objetos paste(A, B, C, sep = &quot;**&quot;) ## [1] &quot;11**-2**2.4&quot; # Paste0 pega los caracteres sin separador paste0(A, B, C) ## [1] &quot;11-22.4&quot; 1:5 ## [1] 1 2 3 4 5 sum(1:5) ## [1] 15 mean(1:5, na.rm = TRUE) ## [1] 3 1.1.3.4 Caracteres especiales R es sensible a mayúsculas y minúsculas, tanto para los nombres de las variables, como para las funciones y parámetros. Los espacios en blanco y los carriage return (enter) no son considerados por el lenguaje. Los podemos aprovechar para emprolijar el código y que la lectura sea más simple2. El numeral # se utiliza para hacer comentarios. Todo lo que se escribe después del # no es interpretado por R. Se debe utilizar un # por cada línea de código que se desea anular Los corchetes [] se utilizan para acceder a un objeto: en un vector[n° orden] en una tabla[fila, columna] en una lista[n° elemento] el signo $ también es un método de acceso. Particularmente, en los dataframes, nos permitira acceder a una determinada columna de una tabla Los paréntesis() se utilizan en las funciones para definir los parámetros. Las comas , se utilizan para separar los parametros al interior de una función. 1.1.4 Objetos: Existe una gran cantidad de objetos distintos en R, en lo que resepcta al curso trabajaremos principalmente con 3 de ellos: Valores Vectores Data Frames Listas 1.1.4.1 Valores Los valores y vectores pueden ser a su vez de distintas clases: Numeric A &lt;- 1 class(A) ## [1] &quot;numeric&quot; Character A &lt;- paste(&#39;Soy&#39;, &#39;una&#39;, &#39;concatenación&#39;, &#39;de&#39;, &#39;caracteres&#39;, sep = &quot; &quot;) A ## [1] &quot;Soy una concatenación de caracteres&quot; class(A) ## [1] &quot;character&quot; Factor A &lt;- factor(&quot;Soy un factor, con niveles fijos&quot;) class(A) ## [1] &quot;factor&quot; La diferencia entre un character y un factor es que el último tiene solo algunos valores permitidos (levels), con un orden interno predefinido (el cual, por ejemplo, se respetará a la hora de realizar un gráfico) 1.1.4.2 Vectores Para crear un vector utilizamos el comando c(), de combinar. C &lt;- c(1, 3, 4) C ## [1] 1 3 4 Podemos sumarle 2 a cada elemento del vector anterior C &lt;- C + 2 C ## [1] 3 5 6 O sumarle 1 al primer elemento, 2 al segundo, y 3 al tercer elemento del vector anterior D &lt;- C + 1:3 # esto es equivalente a hacer 3+1, 5+2, 6+9 D ## [1] 4 7 9 1:3 significa que queremos todos los números enteros desde 1 hasta 3. Podemos crear un vector que contenga las palabras: “Carlos”, “Federico”, “Pedro” E &lt;- c(&quot;Carlos&quot;, &quot;Federico&quot;, &quot;Pedro&quot;) E ## [1] &quot;Carlos&quot; &quot;Federico&quot; &quot;Pedro&quot; Para acceder a algún elemento del vector, podemos buscarlo por su número de orden, entre [ ] E[2] ## [1] &quot;Federico&quot; Si nos interesa almacenar dicho valor, al buscarlo lo asignamos a un nuevo objeto, dándole el nombre que deseemos elemento2 &lt;- E[2] elemento2 ## [1] &quot;Federico&quot; Para borrar un objeto del ambiente de trabajo, utilizamos el comando rm() rm(elemento2) elemento2 ## Error in eval(expr, envir, enclos): object &#39;elemento2&#39; not found También podemos cambiar el texto del segundo elemento de E, por el texto “Pablo” E[2] &lt;- &quot;Pablo&quot; E ## [1] &quot;Carlos&quot; &quot;Pablo&quot; &quot;Pedro&quot; 1.1.5 Data Frames Un Data Frame es una tabla de datos, donde cada columna representa una variable, y cada fila una observación. Este objeto suele ser central en el proceso de trabajo, y suele ser la forma en que se cargan datos externos para trabajar en el ambiente de R, y en que se exportan los resultados de nuestros trabajo. También se puede crear como la combinación de N vectores de igual tamaño. Por ejemplo, tomamos algunos valores del Indice de salarios INDICE &lt;- c(100, 100, 100, 101.8, 101.2, 100.73, 102.9, 102.4, 103.2) FECHA &lt;- c(&quot;Oct-16&quot;, &quot;Oct-16&quot;, &quot;Oct-16&quot;, &quot;Nov-16&quot;, &quot;Nov-16&quot;, &quot;Nov-16&quot;, &quot;Dic-16&quot;, &quot;Dic-16&quot;, &quot;Dic-16&quot;) GRUPO &lt;- c(&quot;Privado_Registrado&quot;,&quot;Público&quot;,&quot;Privado_No_Registrado&quot;, &quot;Privado_Registrado&quot;,&quot;Público&quot;,&quot;Privado_No_Registrado&quot;, &quot;Privado_Registrado&quot;,&quot;Público&quot;,&quot;Privado_No_Registrado&quot;) Datos &lt;- data.frame(INDICE, FECHA, GRUPO) Datos ## INDICE FECHA GRUPO ## 1 100.00 Oct-16 Privado_Registrado ## 2 100.00 Oct-16 Público ## 3 100.00 Oct-16 Privado_No_Registrado ## 4 101.80 Nov-16 Privado_Registrado ## 5 101.20 Nov-16 Público ## 6 100.73 Nov-16 Privado_No_Registrado ## 7 102.90 Dic-16 Privado_Registrado ## 8 102.40 Dic-16 Público ## 9 103.20 Dic-16 Privado_No_Registrado Tal como en un vector se ubica a los elementos mediante [ ], en un dataframe se obtienen sus elementos de la forma [fila, columna]. Otra opción es especificar la columna, mediante el operador $, y luego seleccionar dentro de esa columna el registro deseado mediante el número de orden. Datos$FECHA ## [1] Oct-16 Oct-16 Oct-16 Nov-16 Nov-16 Nov-16 Dic-16 Dic-16 Dic-16 ## Levels: Dic-16 Nov-16 Oct-16 Datos[3,2] ## [1] Oct-16 ## Levels: Dic-16 Nov-16 Oct-16 Datos$FECHA[3] ## [1] Oct-16 ## Levels: Dic-16 Nov-16 Oct-16 ¿que pasa si hacemos Datos$FECHA[3,2] ? Datos$FECHA[3,2] ## Error in `[.default`(Datos$FECHA, 3, 2): incorrect number of dimensions Nótese que el último comando tiene un número incorrecto de dimensiones, porque estamos refiriendonos 2 veces a la columna FECHA. Acorde a lo visto anteriormente, el acceso a los dataframes mediante [ ] puede utilizarse para realizar filtros sobre la base, especificando una condición para las filas. Por ejemplo, puedo utilizar los [ ] para conservar del dataframe Datos unicamente los registros con fecha de Diciembre 2016: Datos[Datos$FECHA==&quot;Dic-16&quot;,] ## INDICE FECHA GRUPO ## 7 102.9 Dic-16 Privado_Registrado ## 8 102.4 Dic-16 Público ## 9 103.2 Dic-16 Privado_No_Registrado La lógica del paso anterior sería: Accedo al dataframe Datos, pidiendo únicamente conservar las filas (por eso la condición se ubica a la izquierda de la ,) que cumplan el requisito de pertenecer a la categoría “Dic-16” de la variable FECHA. Aún más, podría aplicar el filtro y al mismo tiempo identificar una variable de interés para luego realizar un cálculo sobre aquella. Por ejemplo, podría calcular la media de los indices en el mes de Diciembre. ###Por separado Indices_Dic &lt;- Datos$INDICE[Datos$FECHA==&quot;Dic-16&quot;] Indices_Dic ## [1] 102.9 102.4 103.2 mean(Indices_Dic) ## [1] 102.8333 ### Todo junto mean(Datos$INDICE[Datos$FECHA==&quot;Dic-16&quot;]) ## [1] 102.8333 La lógica de esta sintaxis sería: “Me quedo con la variable INDICE, cuando la variable FECHA sea igual a &quot;Dic-16&quot;, luego calculo la media de dichos valores”. 1.1.6 Listas Contienen una concatenación de objetos de cualquier tipo. Así como un vector contiene valores, un dataframe contiene vectores, una lista puede contener dataframes, pero también vectores, o valores, y todo ello a la vez. superlista &lt;- list(A,B,C,D,E,FECHA, DF = Datos, INDICE, GRUPO) superlista ## [[1]] ## [1] Soy un factor, con niveles fijos ## Levels: Soy un factor, con niveles fijos ## ## [[2]] ## [1] -2 ## ## [[3]] ## [1] 3 5 6 ## ## [[4]] ## [1] 4 7 9 ## ## [[5]] ## [1] &quot;Carlos&quot; &quot;Pablo&quot; &quot;Pedro&quot; ## ## [[6]] ## [1] &quot;Oct-16&quot; &quot;Oct-16&quot; &quot;Oct-16&quot; &quot;Nov-16&quot; &quot;Nov-16&quot; &quot;Nov-16&quot; &quot;Dic-16&quot; &quot;Dic-16&quot; ## [9] &quot;Dic-16&quot; ## ## $DF ## INDICE FECHA GRUPO ## 1 100.00 Oct-16 Privado_Registrado ## 2 100.00 Oct-16 Público ## 3 100.00 Oct-16 Privado_No_Registrado ## 4 101.80 Nov-16 Privado_Registrado ## 5 101.20 Nov-16 Público ## 6 100.73 Nov-16 Privado_No_Registrado ## 7 102.90 Dic-16 Privado_Registrado ## 8 102.40 Dic-16 Público ## 9 103.20 Dic-16 Privado_No_Registrado ## ## [[8]] ## [1] 100.00 100.00 100.00 101.80 101.20 100.73 102.90 102.40 103.20 ## ## [[9]] ## [1] &quot;Privado_Registrado&quot; &quot;Público&quot; &quot;Privado_No_Registrado&quot; ## [4] &quot;Privado_Registrado&quot; &quot;Público&quot; &quot;Privado_No_Registrado&quot; ## [7] &quot;Privado_Registrado&quot; &quot;Público&quot; &quot;Privado_No_Registrado&quot; Para acceder un elemento de una lista, podemos utilizar el operador $, que se puede usar a su vez de forma iterativa. superlista$DF$FECHA[2] ## [1] Oct-16 ## Levels: Dic-16 Nov-16 Oct-16 1.1.7 Ambientes de trabajo Hay algunas cosas que tenemos que tener en cuenta respecto del orden del ambiente en el que trabajamos: Working Directory: Es el directorio de trabajo. Pueden ver el suyo con getwd(), es hacia donde apunta el código, por ejemplo, si quieren leer un archivo, la ruta del archivo tiene que estar explicitada como el recorrido desde el Working Directory. Environment: Esto engloba tanto la información que tenemos cargada en Data y Values, como las librerías que tenemos cargadas mientras trabajamos. Es importante que mantengamos bien delimitadas estas cosas entre diferentes trabajos, sino: El directorio queda referido a un lugar específico en nuestra computadora. Si se lo compartimos a otro se rompe Si cambiamos de computadora se rompe Si lo cambiamos de lugar se rompe Si primero abrimos otro script se rompe Tenemos mezclados resultados de diferentes trabajos: Nunca sabemos si esa variable/tabla/lista se creo en ese script y no otro Perdemos espacio de la memoria No estamos seguros de que el script cargue todas las librerías que necesita Rstudio tiene una herramienta muy útil de trabajo que son los proyectos. Estos permiten mantener un ambiente de trabajo delimitado por cada uno de nuestros trabajos. Es decir: El directorio de trabajo se refiere a donde esta ubicado el archivo .Rproj El Environment es específico de nuestro proyecto. Un proyecto no es un sólo script, sino toda una carpeta de trabajo. logo Rpoject Para crearlo, vamos al logo de nuevo projecto (Arriba a la derecha de la panatalla), y elegimos la carpeta de trabajo. 1.1.8 Tipos de archivos de R Script: Es un archivo de texto plano, donde podemos poner el código que utilizamos para preservarlo Rnotebook: También sirve para guardar el código, pero a diferencia de los scripts, se puede compilar, e intercalar código con resultados Rproject: Es un archivo que define la metadata del proyecto RDS y Rdata: Dos formatos de archivos propios de R para guardar datos. Más allá de los comandos elementales, comandos más sofisticados tienen muchas versiones, y algunas quedan en desuso en el tiempo.↩ veremos que existen ciertas excepciones con algunos paquetes más adelante.↩ "],
["practica-guiada.html", "1.2 Práctica Guiada", " 1.2 Práctica Guiada 1.2.1 Instalación de paquetes complementarios al R Base Hasta aquí hemos visto múltiples funciones que están contenidas dentro del lenguaje básico de R. Ahora bien, al tratarse de un software libre, los usuarios de R con más experiencia contribuyen sistemáticamente a expandir este lenguaje mediante la creación y actualización de paquetes complementarios. Lógicamente, los mismos no están incluidos en la instalación inicial del programa, pero podemos descargarlos e instalarlos al mismo tiempo con el siguiente comando: install.packages(&quot;nombre_del_paquete&quot;) Resulta recomendable ejecutar este comando desde la consola ya que sólo necesitaremos correrlo una vez en nuestra computadora. Al ejecutar el mismo, se descargarán de la pagina de CRAN los archivos correspondientes al paquete hacia el directorio en donde hayamos instalado el programa. Típicamente los archivos se encontrarán en C:\\Program Files\\R\\R-3.5.0\\library\\, siempre con la versión del programa correspondiente. Una vez instalado el paquete, cada vez que abramos una nueva sesión de R y querramos utilizar el mismo debemos cargarlo al ambiente de trabajo mediante la siguiente función: library(nombre_del_paquete) Nótese que al cargar/activar el paquete no son necesarias las comillas. 1.2.2 Lectura y escritura de archivos 1.2.2.1 .csv y .txt Hay muchas funciones para leer archivos de tipo .txt y .csv. La mayoría sólo cambia los parámetros que vienen por default. Es importante tener en cuenta que una base de datos que proviene de archivos .txt, o .csv puede presentar diferencias en cuanto a los siguientes parámetros: encabezado delimitador (,, tab, ;) separador decimal dataframe &lt;- read.delim(file, header = TRUE, sep = &quot;\\t&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) Ejemplo. Levantar la base de sueldos de funcionarios En el parametro file tengo que especificar el nombre completo del archivo, incluyendo el directorio donde se encuentra. Lo más sencillo es abrir comillas, apretar Tab y se despliega el menú de las cosas que tenemos en el directorio de trabajo. Si queremos movernos hacia arriba, agregamos ../ sueldos_funcionarios &lt;- read.table(file = &#39;fuentes/sueldo_funcionarios_2019.csv&#39;,sep=&quot;,&quot;, header = TRUE) sueldos_funcionarios[1:10,] ## cuil anio mes funcionario_apellido funcionario_nombre ## 1 20-17692128-6 2019 1 RODRIGUEZ LARRETA HORACIO ANTONIO ## 2 20-17735449-0 2019 1 SANTILLI DIEGO CESAR ## 3 27-24483014-0 2019 1 ACUÑA MARIA SOLEDAD ## 4 20-13872301-2 2019 1 ASTARLOA GABRIEL MARIA ## 5 20-25641207-2 2019 1 AVOGADRO ENRIQUE LUIS ## 6 27-13221055-7 2019 1 BOU PEREZ ANA MARIA ## 7 27-13092400-5 2019 1 FREDA MONICA BEATRIZ ## 8 20-17110752-1 2019 1 MACCHIAVELLI EDUARDO ALBERTO ## 9 20-22293873-3 2019 1 MIGUEL FELIPE OSCAR ## 10 20-14699669-9 2019 1 MOCCIA FRANCO ## repartición asignacion_por_cargo_i ## 1 Jefe de Gobierno 197745.8 ## 2 Vicejefatura de Gobierno 197745.8 ## 3 Ministerio de Educación e Innovación 224516.6 ## 4 Procuración General de la Ciudad de Buenos Aires 224516.6 ## 5 Ministerio de Cultura 224516.6 ## 6 Ministerio de Salud 224516.6 ## 7 Sindicatura General de la Ciudad de Buenos Aires 224516.6 ## 8 Ministerio de Ambiente y Espacio Público 224516.6 ## 9 Jefatura de Gabinete de Ministros 224516.6 ## 10 Ministerio de Desarrollo Urbano y Transporte 224516.6 ## aguinaldo_ii total_salario_bruto_i_._ii observaciones ## 1 0 197745.8 ## 2 0 197745.8 ## 3 0 224516.6 ## 4 0 224516.6 ## 5 0 224516.6 ## 6 0 224516.6 ## 7 0 224516.6 ## 8 0 224516.6 ## 9 0 224516.6 ## 10 0 224516.6 Como puede observarse aquí, la base cuenta con 94 registros y 10 variables. Al trabajar con bases de microdatos, resulta conveniente contar con algunos comandos para tener una mirada rápida de la base, antes de comenzar a realizar los procesamientos que deseemos. Veamos algunos de ellos: #view(sueldos_funcionarios) names(sueldos_funcionarios) ## [1] &quot;cuil&quot; &quot;anio&quot; ## [3] &quot;mes&quot; &quot;funcionario_apellido&quot; ## [5] &quot;funcionario_nombre&quot; &quot;repartición&quot; ## [7] &quot;asignacion_por_cargo_i&quot; &quot;aguinaldo_ii&quot; ## [9] &quot;total_salario_bruto_i_._ii&quot; &quot;observaciones&quot; summary(sueldos_funcionarios) ## cuil anio mes funcionario_apellido ## 20-13872301-2: 3 Min. :2019 Min. :1.00 ACUÑA : 3 ## 20-14699669-9: 3 1st Qu.:2019 1st Qu.:2.00 ASTARLOA : 3 ## 20-16891528-5: 3 Median :2019 Median :3.00 AVELLANEDA: 3 ## 20-16891539-0: 3 Mean :2019 Mean :3.34 AVOGADRO : 3 ## 20-17110752-1: 3 3rd Qu.:2019 3rd Qu.:5.00 BENEGAS : 3 ## 20-17692128-6: 3 Max. :2019 Max. :6.00 BOU PEREZ : 3 ## (Other) :76 (Other) :76 ## funcionario_nombre ## ANA MARIA : 3 ## BRUNO GUIDO : 3 ## CHRISTIAN : 3 ## DIEGO CESAR : 3 ## DIEGO HERNAN : 3 ## EDUARDO ALBERTO: 3 ## (Other) :76 ## repartición ## Consejo de los Derechos de Niñas, Niños y Adoles - Presidencia: 3 ## Ente de Turismo Ley Nº 2627 : 3 ## Jefatura de Gabinete de Ministros : 3 ## Jefe de Gobierno : 3 ## Ministerio de Ambiente y Espacio Público : 3 ## Ministerio de Cultura : 3 ## (Other) :76 ## asignacion_por_cargo_i aguinaldo_ii total_salario_bruto_i_._ii ## Min. :197746 Min. : 0 Min. :197746 ## 1st Qu.:217520 1st Qu.: 0 1st Qu.:217805 ## Median :226866 Median : 0 Median :226866 ## Mean :224718 Mean : 14843 Mean :239560 ## 3rd Qu.:231168 3rd Qu.: 0 3rd Qu.:248033 ## Max. :249662 Max. :113433 Max. :340300 ## ## observaciones ## :93 ## baja 28/2/2019: 1 ## ## ## ## ## head(sueldos_funcionarios)[,1:5] ## cuil anio mes funcionario_apellido funcionario_nombre ## 1 20-17692128-6 2019 1 RODRIGUEZ LARRETA HORACIO ANTONIO ## 2 20-17735449-0 2019 1 SANTILLI DIEGO CESAR ## 3 27-24483014-0 2019 1 ACUÑA MARIA SOLEDAD ## 4 20-13872301-2 2019 1 ASTARLOA GABRIEL MARIA ## 5 20-25641207-2 2019 1 AVOGADRO ENRIQUE LUIS ## 6 27-13221055-7 2019 1 BOU PEREZ ANA MARIA 1.2.2.2 Excel Para leer y escribir archivos excel podemos utilizar los comandos que vienen con la librería openxlsx # install.packages(&quot;openxlsx&quot;) # por única vez library(openxlsx) #activamos la librería # creamos una tabla cualquiera de prueba x &lt;- 1:10 y &lt;- 11:20 tabla_de_R &lt;- data.frame(x,y) # escribimos el archivo write.xlsx(x = tabla_de_R, file = &quot;resultados/archivo.xlsx&quot;, row.names = FALSE) # Donde lo guardó? Hay un directorio por default en caso de que no hayamos definido alguno. # getwd() # Si queremos exportar multiples dataframes a un Excel, debemos armar previamente una lista de ellos. Cada dataframe se guardará en una pestaña de excel, cuyo nombre corresponderá al que definamos para cada Dataframe a la hora de crear la lista. Lista_a_exportar &lt;- list(&quot;sueldos funcionarios&quot; = sueldos_funcionarios, &quot;Tabla Numeros&quot; = tabla_de_R) write.xlsx(x = Lista_a_exportar, file = &quot;resultados/archivo_2_hojas.xlsx&quot;, row.names = FALSE) # leemos el archivo especificando la ruta (o el directorio por default) y el nombre de la hoja que contiene los datos Indices_Salario &lt;- read.xlsx(xlsxFile = &quot;resultados/archivo_2_hojas.xlsx&quot;, sheet = &quot;sueldos funcionarios&quot;) # alternativamente podemos especificar el número de orden de la hoja que deseamos levantar Indices_Salario &lt;- read.xlsx(xlsxFile = &quot;resultados/archivo_2_hojas.xlsx&quot;, sheet = 1) Indices_Salario[1:10,] ## cuil anio mes funcionario_apellido funcionario_nombre ## 1 20-17692128-6 2019 1 RODRIGUEZ LARRETA HORACIO ANTONIO ## 2 20-17735449-0 2019 1 SANTILLI DIEGO CESAR ## 3 27-24483014-0 2019 1 ACUÑA MARIA SOLEDAD ## 4 20-13872301-2 2019 1 ASTARLOA GABRIEL MARIA ## 5 20-25641207-2 2019 1 AVOGADRO ENRIQUE LUIS ## 6 27-13221055-7 2019 1 BOU PEREZ ANA MARIA ## 7 27-13092400-5 2019 1 FREDA MONICA BEATRIZ ## 8 20-17110752-1 2019 1 MACCHIAVELLI EDUARDO ALBERTO ## 9 20-22293873-3 2019 1 MIGUEL FELIPE OSCAR ## 10 20-14699669-9 2019 1 MOCCIA FRANCO ## repartición asignacion_por_cargo_i ## 1 Jefe de Gobierno 197745.8 ## 2 Vicejefatura de Gobierno 197745.8 ## 3 Ministerio de Educación e Innovación 224516.6 ## 4 Procuración General de la Ciudad de Buenos Aires 224516.6 ## 5 Ministerio de Cultura 224516.6 ## 6 Ministerio de Salud 224516.6 ## 7 Sindicatura General de la Ciudad de Buenos Aires 224516.6 ## 8 Ministerio de Ambiente y Espacio Público 224516.6 ## 9 Jefatura de Gabinete de Ministros 224516.6 ## 10 Ministerio de Desarrollo Urbano y Transporte 224516.6 ## aguinaldo_ii total_salario_bruto_i_._ii observaciones ## 1 0 197745.8 ## 2 0 197745.8 ## 3 0 224516.6 ## 4 0 224516.6 ## 5 0 224516.6 ## 6 0 224516.6 ## 7 0 224516.6 ## 8 0 224516.6 ## 9 0 224516.6 ## 10 0 224516.6 "],
["tidyverse.html", "Capítulo-2 Tidyverse", " Capítulo-2 Tidyverse Limpieza de Base de datos: Renombrar y recodificar variables, tratamiento de valores faltantes (missing values/ NA´s) Seleccionar variables, ordenar y agrupar la base de datos para realizar cálculos Creación de nuevas variables Aplicar filtros sobre la base de datos Construir medidas de resumen de la información Tratamiento de variables numéricas (edad, ingresos, horas de trabajo, cantidad de hijos / componentes del hogar, entre otras). "],
["explicacion-1.html", "2.1 Explicación", " 2.1 Explicación A lo largo de esta clase, trabajaremos con el paquete tidyverse. El mismo agrupa una serie de paquetes que tienen una misma lógica en su diseño y por ende funcionan en armonía. Entre ellos, usaremos principalmente dplyr y tidyr para realizar transformaciones sobre nuestro set de datos. En una futura clase utilizaremos ggplot para realizar gráficos. A continuación cargamos la librería a nuestro ambiente. Para ello debe estar previamente instalada en nuestra pc. library(tidyverse) Para mostrar el funcionamiento básico de tidyverse utilizaremos a modo de ejemplo datos del Informe de Mercado de Trabajo del INDEC. INDICADOR &lt;- c(&quot;Tasa de Actividad&quot;, &quot;Tasa de Empleo&quot;, &quot;Tasa de Desocupación&quot;, &quot;Tasa de Actividad&quot;, &quot;Tasa de Empleo&quot;, &quot;Tasa de Desocupación&quot;, &quot;Tasa de Actividad&quot;, &quot;Tasa de Empleo&quot;, &quot;Tasa de Desocupación&quot;) FECHA &lt;- c(&quot;2018.3T&quot;, &quot;2018.3T&quot;, &quot;2018.3T&quot;, &quot;2018.4T&quot;, &quot;2018.4T&quot;, &quot;2018.4T&quot;, &quot;2019.1T&quot;, &quot;2019.1T&quot;, &quot;2019.1T&quot;) TASA &lt;- c(46.7, 42.5, 9, 46.5, 42.2, 9.1, 47, 42.3, 10.1) Datos &lt;- data.frame(INDICADOR, FECHA, TASA) Datos ## INDICADOR FECHA TASA ## 1 Tasa de Actividad 2018.3T 46.7 ## 2 Tasa de Empleo 2018.3T 42.5 ## 3 Tasa de Desocupación 2018.3T 9.0 ## 4 Tasa de Actividad 2018.4T 46.5 ## 5 Tasa de Empleo 2018.4T 42.2 ## 6 Tasa de Desocupación 2018.4T 9.1 ## 7 Tasa de Actividad 2019.1T 47.0 ## 8 Tasa de Empleo 2019.1T 42.3 ## 9 Tasa de Desocupación 2019.1T 10.1 2.1.1 Dplyr El caracter principal para utilizar este paquete es %&gt;% , pipe (de tubería). Los %&gt;% toman el set de datos a su izquierda, y los transforman mediante los comandos a su derecha, en los cuales los elementos de la izquierda están implícitos. En otros términos: \\(f(x,y)\\) es equivalente a \\(x\\) %&gt;% \\(f(.,y)\\) Veamos las principales funciones que pueden utilizarse con la lógica de este paquete: 2.1.1.1 glimpse Permite ver la estructura de la tabla. Nos muestra: número de filas número de columnas nombre de las columnas tipo de dato de cada columna las primeras observaciones de la tabla glimpse(Datos) 2.1.1.2 filter Permite filtrar la tabla de acuerdo al cumplimiento de condiciones lógicas. Datos %&gt;% filter(TASA &gt; 10 , INDICADOR == &quot;Tasa de Desocupación&quot;) ## INDICADOR FECHA TASA ## 1 Tasa de Desocupación 2019.1T 10.1 Nótese que en este caso al separar con una , las condiciones se exige el cumplimiento de ambas. En caso de desear que se cumpla alguna de las condiciones debe utilizarse el caracter |. Datos %&gt;% filter(TASA &gt; 10 | INDICADOR == &quot;Tasa de Desocupación&quot;) ## INDICADOR FECHA TASA ## 1 Tasa de Actividad 2018.3T 46.7 ## 2 Tasa de Empleo 2018.3T 42.5 ## 3 Tasa de Desocupación 2018.3T 9.0 ## 4 Tasa de Actividad 2018.4T 46.5 ## 5 Tasa de Empleo 2018.4T 42.2 ## 6 Tasa de Desocupación 2018.4T 9.1 ## 7 Tasa de Actividad 2019.1T 47.0 ## 8 Tasa de Empleo 2019.1T 42.3 ## 9 Tasa de Desocupación 2019.1T 10.1 2.1.1.3 rename Permite renombrar una columna de la tabla. Funciona de la siguiente manera: Data %&gt;% rename(nuevo_nombre = viejo_nombre) Datos %&gt;% rename(Periodo = FECHA) ## INDICADOR Periodo TASA ## 1 Tasa de Actividad 2018.3T 46.7 ## 2 Tasa de Empleo 2018.3T 42.5 ## 3 Tasa de Desocupación 2018.3T 9.0 ## 4 Tasa de Actividad 2018.4T 46.5 ## 5 Tasa de Empleo 2018.4T 42.2 ## 6 Tasa de Desocupación 2018.4T 9.1 ## 7 Tasa de Actividad 2019.1T 47.0 ## 8 Tasa de Empleo 2019.1T 42.3 ## 9 Tasa de Desocupación 2019.1T 10.1 Nótese que, a diferencia del ejemplo de la función filter donde utilizábamos == para comprobar una condición lógica, en este caso se utiliza sólo un = ya que lo estamos haciendo es asignar un nombre. 2.1.1.4 mutate Permite agregar una variable a la tabla (especificando el nombre que tomará ésta), que puede ser el resultado de operaciones sobre otras variables de la misma tabla. En caso de especificar el nombre de una columna existente, el resultado de la operación realizada “sobre-escribirá” la información de la columna con dicho nombre. Datos &lt;- Datos %&gt;% mutate(PROPORCION = TASA / 100) Datos ## INDICADOR FECHA TASA PROPORCION ## 1 Tasa de Actividad 2018.3T 46.7 0.467 ## 2 Tasa de Empleo 2018.3T 42.5 0.425 ## 3 Tasa de Desocupación 2018.3T 9.0 0.090 ## 4 Tasa de Actividad 2018.4T 46.5 0.465 ## 5 Tasa de Empleo 2018.4T 42.2 0.422 ## 6 Tasa de Desocupación 2018.4T 9.1 0.091 ## 7 Tasa de Actividad 2019.1T 47.0 0.470 ## 8 Tasa de Empleo 2019.1T 42.3 0.423 ## 9 Tasa de Desocupación 2019.1T 10.1 0.101 2.1.1.5 case_when Permite definir una variable, de forma tal que tome un valor particular para cada condición establecida. En caso de no cumplir con ninguna de las condiciones establecidas, la variable tomará valor NA. La sintaxis de la función es: case_when(condicion lógica1 ~ valor asignado1) Datos &lt;- Datos %&gt;% mutate(CODIGO = case_when(INDICADOR == &quot;Tasa de Actividad&quot; ~ &quot;ACT&quot;, INDICADOR == &quot;Tasa de Empleo&quot; ~ &quot;EMP&quot;, INDICADOR == &quot;Tasa de Desocupación&quot; ~ &quot;DES&quot;)) Datos ## INDICADOR FECHA TASA PROPORCION CODIGO ## 1 Tasa de Actividad 2018.3T 46.7 0.467 ACT ## 2 Tasa de Empleo 2018.3T 42.5 0.425 EMP ## 3 Tasa de Desocupación 2018.3T 9.0 0.090 DES ## 4 Tasa de Actividad 2018.4T 46.5 0.465 ACT ## 5 Tasa de Empleo 2018.4T 42.2 0.422 EMP ## 6 Tasa de Desocupación 2018.4T 9.1 0.091 DES ## 7 Tasa de Actividad 2019.1T 47.0 0.470 ACT ## 8 Tasa de Empleo 2019.1T 42.3 0.423 EMP ## 9 Tasa de Desocupación 2019.1T 10.1 0.101 DES Si querémos asignar un valor a todo lo que no cumple ningúna de las condiciones anteriores, podemos poner TRUE ~ valor 2.1.1.6 select Permite especificar la serie de columnas que se desea conservar de un DataFrame. También pueden especificarse las columnas que se desean descartar (agregándoles un - adelante). Muy útil para agilizar el trabajo en bases de datos de gran tamaño. Datos2 &lt;- Datos %&gt;% select(CODIGO, FECHA, PROPORCION) Datos2 ## CODIGO FECHA PROPORCION ## 1 ACT 2018.3T 0.467 ## 2 EMP 2018.3T 0.425 ## 3 DES 2018.3T 0.090 ## 4 ACT 2018.4T 0.465 ## 5 EMP 2018.4T 0.422 ## 6 DES 2018.4T 0.091 ## 7 ACT 2019.1T 0.470 ## 8 EMP 2019.1T 0.423 ## 9 DES 2019.1T 0.101 Datos &lt;- Datos %&gt;% select(-c(PROPORCION, CODIGO)) Datos ## INDICADOR FECHA TASA ## 1 Tasa de Actividad 2018.3T 46.7 ## 2 Tasa de Empleo 2018.3T 42.5 ## 3 Tasa de Desocupación 2018.3T 9.0 ## 4 Tasa de Actividad 2018.4T 46.5 ## 5 Tasa de Empleo 2018.4T 42.2 ## 6 Tasa de Desocupación 2018.4T 9.1 ## 7 Tasa de Actividad 2019.1T 47.0 ## 8 Tasa de Empleo 2019.1T 42.3 ## 9 Tasa de Desocupación 2019.1T 10.1 2.1.1.7 arrange Permite ordenar la tabla según los valores de determinada/s variable/s. Es útil cuando luego deben hacerse otras operaciones que requieran del ordenamiento de la tabla, o para mostrar resultados de forma ordenada. Datos &lt;- Datos %&gt;% arrange(INDICADOR, FECHA) Datos ## INDICADOR FECHA TASA ## 1 Tasa de Actividad 2018.3T 46.7 ## 2 Tasa de Actividad 2018.4T 46.5 ## 3 Tasa de Actividad 2019.1T 47.0 ## 4 Tasa de Desocupación 2018.3T 9.0 ## 5 Tasa de Desocupación 2018.4T 9.1 ## 6 Tasa de Desocupación 2019.1T 10.1 ## 7 Tasa de Empleo 2018.3T 42.5 ## 8 Tasa de Empleo 2018.4T 42.2 ## 9 Tasa de Empleo 2019.1T 42.3 2.1.1.8 summarise Crea una nueva tabla que resuma la información original. Para ello, definimos las variables de resumen y las formas de agregación. Datos %&gt;% filter(INDICADOR == &quot;Tasa de Desocupación&quot;) %&gt;% summarise(INDICE_MAX = max(TASA), INDICE_MIN = min(TASA), INDICE_PROM = mean(TASA)) ## INDICE_MAX INDICE_MIN INDICE_PROM ## 1 10.1 9 9.4 2.1.1.9 group_by Esta función permite realizar operaciones de forma agrupada. Lo que hace la función es “separar” a la tabla según los valores de la variable indicada y realizar las operaciones que se especifican a continuación, de manera independiente para cada una de las “subtablas”. En nuestro ejemplo, podría ser útil para calcular el promedio de las tasas por INDICADOR. Datos %&gt;% group_by(INDICADOR) %&gt;% summarise(INDICE_PROM = mean(TASA)) ## # A tibble: 3 x 2 ## INDICADOR INDICE_PROM ## &lt;fct&gt; &lt;dbl&gt; ## 1 Tasa de Actividad 46.7 ## 2 Tasa de Desocupación 9.4 ## 3 Tasa de Empleo 42.3 2.1.2 Joins Otra implementación muy importante del paquete dplyr son las funciones para unir tablas (joins). fuente: http://rstudio-pubs-static.s3.amazonaws.com/227171_618ebdce0b9d44f3af65700e833593db.html 2.1.2.1 left_join Veamos un ejemplo de la función left_join (una de las más utilizadas en la práctica). Para ello crearemos previamente un Dataframe que contenga las cantidades de población total y población económicamente activa para cada uno de los períodos del Dataframe Datos. Poblaciones &lt;- data.frame(FECHA = c(&quot;2018.3T&quot;, &quot;2018.4T&quot;, &quot;2019.1T&quot;), POBLACION_miles = c(27842, 27914, 28261), PEA_miles = c(12990, 12979, 13285)) Poblaciones ## FECHA POBLACION_miles PEA_miles ## 1 2018.3T 27842 12990 ## 2 2018.4T 27914 12979 ## 3 2019.1T 28261 13285 Unimos nuestras dos tablas. La siguiente forma de realizarlo es equivalente a: Datos_join &lt;- left_join(Datos, Poblaciones, by = &quot;FECHA&quot;) Datos_join &lt;- Datos %&gt;% left_join(Poblaciones, by = &quot;FECHA&quot;) Datos_join ## INDICADOR FECHA TASA POBLACION_miles PEA_miles ## 1 Tasa de Actividad 2018.3T 46.7 27842 12990 ## 2 Tasa de Actividad 2018.4T 46.5 27914 12979 ## 3 Tasa de Actividad 2019.1T 47.0 28261 13285 ## 4 Tasa de Desocupación 2018.3T 9.0 27842 12990 ## 5 Tasa de Desocupación 2018.4T 9.1 27914 12979 ## 6 Tasa de Desocupación 2019.1T 10.1 28261 13285 ## 7 Tasa de Empleo 2018.3T 42.5 27842 12990 ## 8 Tasa de Empleo 2018.4T 42.2 27914 12979 ## 9 Tasa de Empleo 2019.1T 42.3 28261 13285 Finalmente, podemos calcular la cantidad de personas desocupadas en cada uno de los períodos con los que contamos. Datos_join %&gt;% filter(INDICADOR == &quot;Tasa de Desocupación&quot;) %&gt;% group_by(FECHA) %&gt;% summarise(DESOCUP_miles = round(TASA/100 * PEA_miles, 0)) ## # A tibble: 3 x 2 ## FECHA DESOCUP_miles ## &lt;fct&gt; &lt;dbl&gt; ## 1 2018.3T 1169 ## 2 2018.4T 1181 ## 3 2019.1T 1342 2.1.3 Tidyr El paquete tidyr está pensado para facilitar el emprolijamiento de los datos. Gather es una función que nos permite pasar los datos de forma horizontal a una forma vertical. spread es una función que nos permite pasar los datos de forma vertical a una forma horizontal. fuente: http://www.gis-blog.com/data-management-with-r-tidyr-part-1/ # Utilizamos un conjunto de datos que viene con la librería datasets library(datasets) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa iris &lt;- iris %&gt;% mutate(id = 1:nrow(.)) %&gt;% # le agrego un ID select(id, everything()) # lo acomodo para que el id este primero. head(iris) ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 1 5.1 3.5 1.4 0.2 setosa ## 2 2 4.9 3.0 1.4 0.2 setosa ## 3 3 4.7 3.2 1.3 0.2 setosa ## 4 4 4.6 3.1 1.5 0.2 setosa ## 5 5 5.0 3.6 1.4 0.2 setosa ## 6 6 5.4 3.9 1.7 0.4 setosa 2.1.3.1 Gather y Spread iris_vertical &lt;- iris %&gt;% gather(., # el . llama a lo que esta atras del %&gt;% key = Variables, value = Valores, 2:5) #le indico qué columnas juntar head(iris_vertical) ## id Species Variables Valores ## 1 1 setosa Sepal.Length 5.1 ## 2 2 setosa Sepal.Length 4.9 ## 3 3 setosa Sepal.Length 4.7 ## 4 4 setosa Sepal.Length 4.6 ## 5 5 setosa Sepal.Length 5.0 ## 6 6 setosa Sepal.Length 5.4 Podemos deshacer el gather con un Spread iris_horizontal &lt;- iris_vertical %&gt;% spread(. , key = Variables, # la llave es la variable que va a dar los nombres de columna value = Valores) # los valores con que se llenan las celdas head(iris_horizontal) ## id Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## 1 1 setosa 1.4 0.2 5.1 3.5 ## 2 2 setosa 1.4 0.2 4.9 3.0 ## 3 3 setosa 1.3 0.2 4.7 3.2 ## 4 4 setosa 1.5 0.2 4.6 3.1 ## 5 5 setosa 1.4 0.2 5.0 3.6 ## 6 6 setosa 1.7 0.4 5.4 3.9 2.1.4 Lubridate El paquete lubridate está pensado para trabajar con los datos tipo fecha (date) o fecha-hora (datetime) para cambiarles el formato, realizar operaciones y extraer información library(lubridate) 2.1.4.1 Cambio de formato Existe una gran cantidad de funciones para realizar esto. La idea general es poder llevar los objetos datetime a un formato común compuesto de los elementos: año, mes, día, hora, minuto y segundo (también se puede setear el huso horario) fecha &lt;- &quot;04/12/92 17:35:16&quot; fecha ## [1] &quot;04/12/92 17:35:16&quot; Con la función dmy_hms podemos convertir este string a una fecha: estamos indicando que el formato de la fecha es día(d), mes(m), año(y), hora(h), minuto(m) y segundo(s). fecha &lt;- dmy_hms(fecha) fecha ## [1] &quot;1992-12-04 17:35:16 UTC&quot; Muchas funciones de lubridate operan con esta misma lógica. Otra función para realizar un cambio de formato es parse_date_time. Permite construir objetos datetime a partir de datos más complejos, como por ejemplo cuando aparece el nombre del mes y el año. En el parámetro x pasamos el dato de la fecha y en el parámetro orders especificamos el orden en el cual se encuentra la información de la fecha. fecha2 &lt;- &quot;Dec-92&quot; fecha2 &lt;- parse_date_time(fecha2, orders = &#39;my&#39;) fecha2 ## [1] &quot;1992-12-01 UTC&quot; 2.1.4.2 Extracción de información Existen muchas funciones muy sencillas para extraer información de un objeto datetime. Algunas son: year(fecha) # Obtener el año ## [1] 1992 month(fecha) # Obtener el mes ## [1] 12 day(fecha) # Obtener el día ## [1] 4 wday(fecha, label = TRUE) # Obtener el nombre del día ## [1] vie ## Levels: dom &lt; lun &lt; mar &lt; mié &lt; jue &lt; vie &lt; sáb hour(fecha) # Obtener la hora ## [1] 17 2.1.4.3 Operaciones Podemos sumar o restarle cualquier período de tiempo a un objeto datetime # Sumo dos días fecha + days(2) ## [1] &quot;1992-12-06 17:35:16 UTC&quot; # Resto 1 semana y dos horas fecha - (weeks(1) + hours(2)) ## [1] &quot;1992-11-27 15:35:16 UTC&quot; "],
["practica-guiada-1.html", "2.2 Práctica Guiada", " 2.2 Práctica Guiada En esta ocasión utilizaremos los datos de la librería gapminder para utilizar todo lo que aprendimos sobre el tidyverse. library(tidyverse) library(gapminder) glimpse(gapminder) ## Observations: 1,704 ## Variables: 6 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Af… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, … ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854… ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 148803… ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.… 2.2.1 Ejemplo 1 Calcular el promedio, el máximo y el mínimo de la esperanza de vida de cada continente en el año 2007. Presentar los datos ordenados según la esperanza de vida promedio. Necesitamos filtrar los datos tal que sólo queden aquellos correspondientes a 2007. Luego, agrupamos los casos de acuerdo a su continente, y calculamos los indicadores agregados solicitados. Luego, ordenamos los resultados. ejercicio1 &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarise(esp_vida_prom = mean(lifeExp), esp_vida_max = max(lifeExp), esp_vida_min = min(lifeExp)) %&gt;% arrange(esp_vida_prom) ejercicio1 ## # A tibble: 5 x 4 ## continent esp_vida_prom esp_vida_max esp_vida_min ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 54.8 76.4 39.6 ## 2 Asia 70.7 82.6 43.8 ## 3 Americas 73.6 80.7 60.9 ## 4 Europe 77.6 81.8 71.8 ## 5 Oceania 80.7 81.2 80.2 2.2.2 Ejemplo 2 Construir una nueva variable en el dataset que contenga una estimación del PBI. Estimar la mediana del PBI, y construir otra variable que tome valor “ALTO” cuando el PBI supera ese valor, y “BAJO” cuando no. Calculamos el PBI como el producto entre la población y el PBI per cápita para cada uno de los países y años. A continuación, guardamos el cálculo de la mediana del PBI en un valor llamado mediana_GDP. Por último, utilizamos la función case_when para poder construir la variable de nivel de PBI de acuerdo a la condición lógica solicitada. Nótese que el dataframe ejercicio2 ha sido re-escrito. ejercicio2 &lt;- gapminder %&gt;% mutate(GDP = pop * gdpPercap) mediana_GDP &lt;- median(ejercicio2$GDP) ejercicio2 &lt;- ejercicio2 %&gt;% mutate(GDP_level = case_when(GDP &gt; mediana_GDP ~ &quot;ALTO&quot;, GDP &lt; mediana_GDP ~ &quot;BAJO&quot;)) head(ejercicio2) ## # A tibble: 6 x 8 ## country continent year lifeExp pop gdpPercap GDP GDP_level ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanist… Asia 1952 28.8 8.43e6 779. 6.57e 9 BAJO ## 2 Afghanist… Asia 1957 30.3 9.24e6 821. 7.59e 9 BAJO ## 3 Afghanist… Asia 1962 32.0 1.03e7 853. 8.76e 9 BAJO ## 4 Afghanist… Asia 1967 34.0 1.15e7 836. 9.65e 9 BAJO ## 5 Afghanist… Asia 1972 36.1 1.31e7 740. 9.68e 9 BAJO ## 6 Afghanist… Asia 1977 38.4 1.49e7 786. 1.17e10 BAJO 2.2.3 Ejemplo 3 Crear una copia de la base donde sólo se conserven las variables country, year y lifeExp, pero con los nombres pais, anio y espVida. Utilizamos select() para quedarnos con las columnas solicitadas, y rename() para cambiar sus nombres. ejercicio3 &lt;- gapminder %&gt;% select(country, year, lifeExp) %&gt;% rename(pais = country, anio = year, espVida = lifeExp) head(ejercicio3) ## # A tibble: 6 x 3 ## pais anio espVida ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 28.8 ## 2 Afghanistan 1957 30.3 ## 3 Afghanistan 1962 32.0 ## 4 Afghanistan 1967 34.0 ## 5 Afghanistan 1972 36.1 ## 6 Afghanistan 1977 38.4 2.2.4 Ejemplo 4 Crear una copia de la base donde sólo se conserven las variables country, year y gdpPercap, pero con los nombres pais, anio y pbiPercap. ejercicio4 &lt;- gapminder %&gt;% select(country, year, gdpPercap) %&gt;% rename(pais = country, anio = year, pbiPercap = gdpPercap) head(ejercicio4) ## # A tibble: 6 x 3 ## pais anio pbiPercap ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 779. ## 2 Afghanistan 1957 821. ## 3 Afghanistan 1962 853. ## 4 Afghanistan 1967 836. ## 5 Afghanistan 1972 740. ## 6 Afghanistan 1977 786. 2.2.5 Ejemplo 5 Crear una nueva tabla que contenga los datos de las tablas ejercicio3 y ejercicio4. Deben unirse de acuerdo al pais y al anio. Podemos utilizar la función left_join(). ejercicio5 &lt;- left_join(ejercicio3, ejercicio4, by = c(&quot;pais&quot;, &quot;anio&quot;)) head(ejercicio5) ## # A tibble: 6 x 4 ## pais anio espVida pbiPercap ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 28.8 779. ## 2 Afghanistan 1957 30.3 821. ## 3 Afghanistan 1962 32.0 853. ## 4 Afghanistan 1967 34.0 836. ## 5 Afghanistan 1972 36.1 740. ## 6 Afghanistan 1977 38.4 786. 2.2.6 Ejemplo 6 Presentar los datos de la tabla ejercicio1 de forma tal que esp_vida_prom, esp_vida_max y esp_vida_min sean valores de una variable llamada indicador, y los valores se encuentren en la variable valor. Utilizamos gather(), porque queremos transformar los datos de un formato “horizontal” a uno “vertical”. ejercicio6 &lt;- ejercicio1 %&gt;% gather(., key = indicador, value = valor, 2:4) head(ejercicio6) ## # A tibble: 6 x 3 ## continent indicador valor ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Africa esp_vida_prom 54.8 ## 2 Asia esp_vida_prom 70.7 ## 3 Americas esp_vida_prom 73.6 ## 4 Europe esp_vida_prom 77.6 ## 5 Oceania esp_vida_prom 80.7 ## 6 Africa esp_vida_max 76.4 "],
["programacion-funcional.html", "Capítulo-3 Programacion Funcional", " Capítulo-3 Programacion Funcional El objetivo de esta clase es introducir a los alumnos en el uso de la programación funcional. Es decir, en la utilización de funciones y el uso de controles de flujo de la información para la organización de su código. Estructuras de código condicionales Loops Creación de funciones a medida del usuario Librería purrr para programación funcional "],
["explicacion-2.html", "3.1 Explicación", " 3.1 Explicación library(tidyverse) 3.1.1 Loops Un loop es una estructura de código que nos permite aplicar iterativamente un mismo conjunto de comandos, variando el valor de una variable. Por ejemplo: for(i in 1:10){ print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 ## [1] 100 Esto se lee como : “Recorre cada uno de los valores (i) del vector numérico 1 a 10, y para cada uno de ellos imprimí el cuadrado (i^2)”. Uno puede especificar la palabra que desee que tomé cada uno de los valores que debe tomar. En el ejemplo anterior fue i, pero bien podría ser la “Valores” for(Valores in 1:10){ print(Valores^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 ## [1] 100 Un loop puede iterar sobre cualquier tipo de vector, independientemente de lo que contenga. Los loops son una estructura básica que existen en cualquier lenguaje de programación. En R no recomendamos abusar de ellos porque hacen que el código sea más lento. 3.1.2 Estructuras Condicionales Las estructuras condiconales nos permiten ejecutar una porción de código en caso de que cumplan una condición lógica 3.1.2.1 if Su funcionamiento es el siguiente: if(condicion){codigo a ejecutar si se cumple la condición} if( 2+2 == 4){ print(&quot;Menos Mal&quot;) } ## [1] &quot;Menos Mal&quot; if( 2+2 == 148.24){ print(&quot;R, tenemos un problema&quot;) } 3.1.2.2 ifelse La función if_else() sirve para crear o modificar dicotómicamente un objeto/variable/vector a partir del cumplimiento de una o más condiciones lógicas. Su funcionamiento es el siguiente: if_else(condicion,función a aplicar si se cumple la condición,función a aplicar si no se cumple la condición) if_else(2+2==4, true = &quot;Joya&quot;,false = &quot;Error&quot;) ## [1] &quot;Joya&quot; 3.1.3 Funciones La creación de funciones propias nos permite automatizar todas aquellas partes del código que se repiten mucho. Una vez diseñadas, funcionan igual que cualquier comando. Por ejemplo, podemos definir la suma de dos elementos como suma &lt;- function(valor1, valor2) { valor1+valor2 } suma(5,6) ## [1] 11 Obviamente las funciones no son sólo para variables numéricas. Por ejemplo, podemos pegar dos strings con una flecha en el medio funcion_prueba &lt;- function(parametro1,parametro2) { paste(parametro1, parametro2, sep = &quot; &lt;--&gt; &quot;) } funcion_prueba(parametro1 = &quot;A ver&quot;, parametro2 = &quot;Que pasa&quot;) ## [1] &quot;A ver &lt;--&gt; Que pasa&quot; También podemos asignar un valor por default para los parametros en caso de que el usuario no defina su valor al utilizar la función. Otra_funcion_prueba &lt;- function(parametro1 ,parametro2 = &quot;String default&quot;) { paste(parametro1, parametro2, sep = &quot; &lt;--&gt; &quot;) } Otra_funcion_prueba(parametro1 = &quot;Valor 1 &quot;) ## [1] &quot;Valor 1 &lt;--&gt; String default&quot; Las funciones que creamos nosotros permanecen en el ambiente de R temporariamente. Cuando removemos los objetos del ambiente, la función deja de existir. Por ende, debemos incorporarla en cada uno de los scripts en la cual la necesitemos. Una buena práctica, es incorporar nuestras funciones útiles al comienzo de cada script junto a la carga de las librerías. Vale mencionar que lo que ocurre en una función, queda en la función excepto que explícitamente pidamos que devuelva el resultado, con el comando print(). Las funciones siempre devuelven el último objeto que se crea en ellas, o si explicitamente se utiliza el comando return() 3.1.4 PURRR3 MAP es la forma tidy de hacer loops. Además de ser más prolijo el código, es mucho más eficiente. La función map toma un input, una función para aplicar, y alguna otra cosa (por ejemplo parametros que necesite la función) map(.x, .f, …) map(VECTOR_O_LIST_INPUT, FUNCTION_A_APLICAR, OTROS_OPCIONALES) Usamos map2 cuando tenemos que pasar dos input, que se aplican sobre una función: map2(.x, .y, .f, …) map2(INPUT_UNO, INPUT_DOS, FUNCTION_A_APLICAR, OTROS_OPCIONALES) Si tenemos más de dos… pmap(.l, .f, …) pmap(VECTOR_O_LIST_INPUT, FUNCTION_A_APLICAR, OTROS_OPCIONALES) Por ejemplo. Si queremos utilizar la función prueba sobre los datos del dataframe ABC_123 ABC_123 &lt;- data.frame(Letras = LETTERS[1:20],Num = 1:20) funcion_prueba ## function(parametro1,parametro2) { ## paste(parametro1, parametro2, sep = &quot; &lt;--&gt; &quot;) ## } Si el resultado que queremos es que junte cada fila, necesitamos pasarle dos parámetros: utilizamos map2() resultado &lt;- map2(ABC_123$Letras,ABC_123$Num,funcion_prueba) resultado[1:3] ## [[1]] ## [1] &quot;A &lt;--&gt; 1&quot; ## ## [[2]] ## [1] &quot;B &lt;--&gt; 2&quot; ## ## [[3]] ## [1] &quot;C &lt;--&gt; 3&quot; La salida de los map() es una lista, no un vector, por lo que si lo metemos dentro de un dataframe se vería así: ABC_123 %&gt;% mutate(resultado= map2(Letras,Num,funcion_prueba)) ## Letras Num resultado ## 1 A 1 A &lt;--&gt; 1 ## 2 B 2 B &lt;--&gt; 2 ## 3 C 3 C &lt;--&gt; 3 ## 4 D 4 D &lt;--&gt; 4 ## 5 E 5 E &lt;--&gt; 5 ## 6 F 6 F &lt;--&gt; 6 ## 7 G 7 G &lt;--&gt; 7 ## 8 H 8 H &lt;--&gt; 8 ## 9 I 9 I &lt;--&gt; 9 ## 10 J 10 J &lt;--&gt; 10 ## 11 K 11 K &lt;--&gt; 11 ## 12 L 12 L &lt;--&gt; 12 ## 13 M 13 M &lt;--&gt; 13 ## 14 N 14 N &lt;--&gt; 14 ## 15 O 15 O &lt;--&gt; 15 ## 16 P 16 P &lt;--&gt; 16 ## 17 Q 17 Q &lt;--&gt; 17 ## 18 R 18 R &lt;--&gt; 18 ## 19 S 19 S &lt;--&gt; 19 ## 20 T 20 T &lt;--&gt; 20 al ponerlo dentro del dataframe desarma la lista y guarda cada elemento por separado. La magia de eso es que podemos guardar cualquier cosa en el dataframe no sólo valores, sino también listas, funciones, dataframes, etc. Si queremos recuperar los valores originales en este caso podemos usar unlist() resultado[1:3] %&gt;% unlist() ## [1] &quot;A &lt;--&gt; 1&quot; &quot;B &lt;--&gt; 2&quot; &quot;C &lt;--&gt; 3&quot; ABC_123 %&gt;% mutate(resultado= unlist(map2(Letras,Num,funcion_prueba))) ## Letras Num resultado ## 1 A 1 A &lt;--&gt; 1 ## 2 B 2 B &lt;--&gt; 2 ## 3 C 3 C &lt;--&gt; 3 ## 4 D 4 D &lt;--&gt; 4 ## 5 E 5 E &lt;--&gt; 5 ## 6 F 6 F &lt;--&gt; 6 ## 7 G 7 G &lt;--&gt; 7 ## 8 H 8 H &lt;--&gt; 8 ## 9 I 9 I &lt;--&gt; 9 ## 10 J 10 J &lt;--&gt; 10 ## 11 K 11 K &lt;--&gt; 11 ## 12 L 12 L &lt;--&gt; 12 ## 13 M 13 M &lt;--&gt; 13 ## 14 N 14 N &lt;--&gt; 14 ## 15 O 15 O &lt;--&gt; 15 ## 16 P 16 P &lt;--&gt; 16 ## 17 Q 17 Q &lt;--&gt; 17 ## 18 R 18 R &lt;--&gt; 18 ## 19 S 19 S &lt;--&gt; 19 ## 20 T 20 T &lt;--&gt; 20 Si lo que queríamos era que la función nos haga todas las combinaciones de letras y número, entonces lo que necesitamos es pasarle el segúndo parametro como algo fijo, poniendolo después de la función. map(ABC_123$Letras,funcion_prueba,ABC_123$Num)[1:2] ## [[1]] ## [1] &quot;A &lt;--&gt; 1&quot; &quot;A &lt;--&gt; 2&quot; &quot;A &lt;--&gt; 3&quot; &quot;A &lt;--&gt; 4&quot; &quot;A &lt;--&gt; 5&quot; ## [6] &quot;A &lt;--&gt; 6&quot; &quot;A &lt;--&gt; 7&quot; &quot;A &lt;--&gt; 8&quot; &quot;A &lt;--&gt; 9&quot; &quot;A &lt;--&gt; 10&quot; ## [11] &quot;A &lt;--&gt; 11&quot; &quot;A &lt;--&gt; 12&quot; &quot;A &lt;--&gt; 13&quot; &quot;A &lt;--&gt; 14&quot; &quot;A &lt;--&gt; 15&quot; ## [16] &quot;A &lt;--&gt; 16&quot; &quot;A &lt;--&gt; 17&quot; &quot;A &lt;--&gt; 18&quot; &quot;A &lt;--&gt; 19&quot; &quot;A &lt;--&gt; 20&quot; ## ## [[2]] ## [1] &quot;B &lt;--&gt; 1&quot; &quot;B &lt;--&gt; 2&quot; &quot;B &lt;--&gt; 3&quot; &quot;B &lt;--&gt; 4&quot; &quot;B &lt;--&gt; 5&quot; ## [6] &quot;B &lt;--&gt; 6&quot; &quot;B &lt;--&gt; 7&quot; &quot;B &lt;--&gt; 8&quot; &quot;B &lt;--&gt; 9&quot; &quot;B &lt;--&gt; 10&quot; ## [11] &quot;B &lt;--&gt; 11&quot; &quot;B &lt;--&gt; 12&quot; &quot;B &lt;--&gt; 13&quot; &quot;B &lt;--&gt; 14&quot; &quot;B &lt;--&gt; 15&quot; ## [16] &quot;B &lt;--&gt; 16&quot; &quot;B &lt;--&gt; 17&quot; &quot;B &lt;--&gt; 18&quot; &quot;B &lt;--&gt; 19&quot; &quot;B &lt;--&gt; 20&quot; En este caso, el map itera sobre cada elemento de letras, y para cada elemento i hace funcion_prueba(i,ABC$Num) y guarda el resultado en la lista si lo queremos meter en el dataframe ABC_123 %&gt;% mutate(resultado= map(Letras,funcion_prueba,Num)) ## Letras Num ## 1 A 1 ## 2 B 2 ## 3 C 3 ## 4 D 4 ## 5 E 5 ## 6 F 6 ## 7 G 7 ## 8 H 8 ## 9 I 9 ## 10 J 10 ## 11 K 11 ## 12 L 12 ## 13 M 13 ## 14 N 14 ## 15 O 15 ## 16 P 16 ## 17 Q 17 ## 18 R 18 ## 19 S 19 ## 20 T 20 ## resultado ## 1 A &lt;--&gt; 1, A &lt;--&gt; 2, A &lt;--&gt; 3, A &lt;--&gt; 4, A &lt;--&gt; 5, A &lt;--&gt; 6, A &lt;--&gt; 7, A &lt;--&gt; 8, A &lt;--&gt; 9, A &lt;--&gt; 10, A &lt;--&gt; 11, A &lt;--&gt; 12, A &lt;--&gt; 13, A &lt;--&gt; 14, A &lt;--&gt; 15, A &lt;--&gt; 16, A &lt;--&gt; 17, A &lt;--&gt; 18, A &lt;--&gt; 19, A &lt;--&gt; 20 ## 2 B &lt;--&gt; 1, B &lt;--&gt; 2, B &lt;--&gt; 3, B &lt;--&gt; 4, B &lt;--&gt; 5, B &lt;--&gt; 6, B &lt;--&gt; 7, B &lt;--&gt; 8, B &lt;--&gt; 9, B &lt;--&gt; 10, B &lt;--&gt; 11, B &lt;--&gt; 12, B &lt;--&gt; 13, B &lt;--&gt; 14, B &lt;--&gt; 15, B &lt;--&gt; 16, B &lt;--&gt; 17, B &lt;--&gt; 18, B &lt;--&gt; 19, B &lt;--&gt; 20 ## 3 C &lt;--&gt; 1, C &lt;--&gt; 2, C &lt;--&gt; 3, C &lt;--&gt; 4, C &lt;--&gt; 5, C &lt;--&gt; 6, C &lt;--&gt; 7, C &lt;--&gt; 8, C &lt;--&gt; 9, C &lt;--&gt; 10, C &lt;--&gt; 11, C &lt;--&gt; 12, C &lt;--&gt; 13, C &lt;--&gt; 14, C &lt;--&gt; 15, C &lt;--&gt; 16, C &lt;--&gt; 17, C &lt;--&gt; 18, C &lt;--&gt; 19, C &lt;--&gt; 20 ## 4 D &lt;--&gt; 1, D &lt;--&gt; 2, D &lt;--&gt; 3, D &lt;--&gt; 4, D &lt;--&gt; 5, D &lt;--&gt; 6, D &lt;--&gt; 7, D &lt;--&gt; 8, D &lt;--&gt; 9, D &lt;--&gt; 10, D &lt;--&gt; 11, D &lt;--&gt; 12, D &lt;--&gt; 13, D &lt;--&gt; 14, D &lt;--&gt; 15, D &lt;--&gt; 16, D &lt;--&gt; 17, D &lt;--&gt; 18, D &lt;--&gt; 19, D &lt;--&gt; 20 ## 5 E &lt;--&gt; 1, E &lt;--&gt; 2, E &lt;--&gt; 3, E &lt;--&gt; 4, E &lt;--&gt; 5, E &lt;--&gt; 6, E &lt;--&gt; 7, E &lt;--&gt; 8, E &lt;--&gt; 9, E &lt;--&gt; 10, E &lt;--&gt; 11, E &lt;--&gt; 12, E &lt;--&gt; 13, E &lt;--&gt; 14, E &lt;--&gt; 15, E &lt;--&gt; 16, E &lt;--&gt; 17, E &lt;--&gt; 18, E &lt;--&gt; 19, E &lt;--&gt; 20 ## 6 F &lt;--&gt; 1, F &lt;--&gt; 2, F &lt;--&gt; 3, F &lt;--&gt; 4, F &lt;--&gt; 5, F &lt;--&gt; 6, F &lt;--&gt; 7, F &lt;--&gt; 8, F &lt;--&gt; 9, F &lt;--&gt; 10, F &lt;--&gt; 11, F &lt;--&gt; 12, F &lt;--&gt; 13, F &lt;--&gt; 14, F &lt;--&gt; 15, F &lt;--&gt; 16, F &lt;--&gt; 17, F &lt;--&gt; 18, F &lt;--&gt; 19, F &lt;--&gt; 20 ## 7 G &lt;--&gt; 1, G &lt;--&gt; 2, G &lt;--&gt; 3, G &lt;--&gt; 4, G &lt;--&gt; 5, G &lt;--&gt; 6, G &lt;--&gt; 7, G &lt;--&gt; 8, G &lt;--&gt; 9, G &lt;--&gt; 10, G &lt;--&gt; 11, G &lt;--&gt; 12, G &lt;--&gt; 13, G &lt;--&gt; 14, G &lt;--&gt; 15, G &lt;--&gt; 16, G &lt;--&gt; 17, G &lt;--&gt; 18, G &lt;--&gt; 19, G &lt;--&gt; 20 ## 8 H &lt;--&gt; 1, H &lt;--&gt; 2, H &lt;--&gt; 3, H &lt;--&gt; 4, H &lt;--&gt; 5, H &lt;--&gt; 6, H &lt;--&gt; 7, H &lt;--&gt; 8, H &lt;--&gt; 9, H &lt;--&gt; 10, H &lt;--&gt; 11, H &lt;--&gt; 12, H &lt;--&gt; 13, H &lt;--&gt; 14, H &lt;--&gt; 15, H &lt;--&gt; 16, H &lt;--&gt; 17, H &lt;--&gt; 18, H &lt;--&gt; 19, H &lt;--&gt; 20 ## 9 I &lt;--&gt; 1, I &lt;--&gt; 2, I &lt;--&gt; 3, I &lt;--&gt; 4, I &lt;--&gt; 5, I &lt;--&gt; 6, I &lt;--&gt; 7, I &lt;--&gt; 8, I &lt;--&gt; 9, I &lt;--&gt; 10, I &lt;--&gt; 11, I &lt;--&gt; 12, I &lt;--&gt; 13, I &lt;--&gt; 14, I &lt;--&gt; 15, I &lt;--&gt; 16, I &lt;--&gt; 17, I &lt;--&gt; 18, I &lt;--&gt; 19, I &lt;--&gt; 20 ## 10 J &lt;--&gt; 1, J &lt;--&gt; 2, J &lt;--&gt; 3, J &lt;--&gt; 4, J &lt;--&gt; 5, J &lt;--&gt; 6, J &lt;--&gt; 7, J &lt;--&gt; 8, J &lt;--&gt; 9, J &lt;--&gt; 10, J &lt;--&gt; 11, J &lt;--&gt; 12, J &lt;--&gt; 13, J &lt;--&gt; 14, J &lt;--&gt; 15, J &lt;--&gt; 16, J &lt;--&gt; 17, J &lt;--&gt; 18, J &lt;--&gt; 19, J &lt;--&gt; 20 ## 11 K &lt;--&gt; 1, K &lt;--&gt; 2, K &lt;--&gt; 3, K &lt;--&gt; 4, K &lt;--&gt; 5, K &lt;--&gt; 6, K &lt;--&gt; 7, K &lt;--&gt; 8, K &lt;--&gt; 9, K &lt;--&gt; 10, K &lt;--&gt; 11, K &lt;--&gt; 12, K &lt;--&gt; 13, K &lt;--&gt; 14, K &lt;--&gt; 15, K &lt;--&gt; 16, K &lt;--&gt; 17, K &lt;--&gt; 18, K &lt;--&gt; 19, K &lt;--&gt; 20 ## 12 L &lt;--&gt; 1, L &lt;--&gt; 2, L &lt;--&gt; 3, L &lt;--&gt; 4, L &lt;--&gt; 5, L &lt;--&gt; 6, L &lt;--&gt; 7, L &lt;--&gt; 8, L &lt;--&gt; 9, L &lt;--&gt; 10, L &lt;--&gt; 11, L &lt;--&gt; 12, L &lt;--&gt; 13, L &lt;--&gt; 14, L &lt;--&gt; 15, L &lt;--&gt; 16, L &lt;--&gt; 17, L &lt;--&gt; 18, L &lt;--&gt; 19, L &lt;--&gt; 20 ## 13 M &lt;--&gt; 1, M &lt;--&gt; 2, M &lt;--&gt; 3, M &lt;--&gt; 4, M &lt;--&gt; 5, M &lt;--&gt; 6, M &lt;--&gt; 7, M &lt;--&gt; 8, M &lt;--&gt; 9, M &lt;--&gt; 10, M &lt;--&gt; 11, M &lt;--&gt; 12, M &lt;--&gt; 13, M &lt;--&gt; 14, M &lt;--&gt; 15, M &lt;--&gt; 16, M &lt;--&gt; 17, M &lt;--&gt; 18, M &lt;--&gt; 19, M &lt;--&gt; 20 ## 14 N &lt;--&gt; 1, N &lt;--&gt; 2, N &lt;--&gt; 3, N &lt;--&gt; 4, N &lt;--&gt; 5, N &lt;--&gt; 6, N &lt;--&gt; 7, N &lt;--&gt; 8, N &lt;--&gt; 9, N &lt;--&gt; 10, N &lt;--&gt; 11, N &lt;--&gt; 12, N &lt;--&gt; 13, N &lt;--&gt; 14, N &lt;--&gt; 15, N &lt;--&gt; 16, N &lt;--&gt; 17, N &lt;--&gt; 18, N &lt;--&gt; 19, N &lt;--&gt; 20 ## 15 O &lt;--&gt; 1, O &lt;--&gt; 2, O &lt;--&gt; 3, O &lt;--&gt; 4, O &lt;--&gt; 5, O &lt;--&gt; 6, O &lt;--&gt; 7, O &lt;--&gt; 8, O &lt;--&gt; 9, O &lt;--&gt; 10, O &lt;--&gt; 11, O &lt;--&gt; 12, O &lt;--&gt; 13, O &lt;--&gt; 14, O &lt;--&gt; 15, O &lt;--&gt; 16, O &lt;--&gt; 17, O &lt;--&gt; 18, O &lt;--&gt; 19, O &lt;--&gt; 20 ## 16 P &lt;--&gt; 1, P &lt;--&gt; 2, P &lt;--&gt; 3, P &lt;--&gt; 4, P &lt;--&gt; 5, P &lt;--&gt; 6, P &lt;--&gt; 7, P &lt;--&gt; 8, P &lt;--&gt; 9, P &lt;--&gt; 10, P &lt;--&gt; 11, P &lt;--&gt; 12, P &lt;--&gt; 13, P &lt;--&gt; 14, P &lt;--&gt; 15, P &lt;--&gt; 16, P &lt;--&gt; 17, P &lt;--&gt; 18, P &lt;--&gt; 19, P &lt;--&gt; 20 ## 17 Q &lt;--&gt; 1, Q &lt;--&gt; 2, Q &lt;--&gt; 3, Q &lt;--&gt; 4, Q &lt;--&gt; 5, Q &lt;--&gt; 6, Q &lt;--&gt; 7, Q &lt;--&gt; 8, Q &lt;--&gt; 9, Q &lt;--&gt; 10, Q &lt;--&gt; 11, Q &lt;--&gt; 12, Q &lt;--&gt; 13, Q &lt;--&gt; 14, Q &lt;--&gt; 15, Q &lt;--&gt; 16, Q &lt;--&gt; 17, Q &lt;--&gt; 18, Q &lt;--&gt; 19, Q &lt;--&gt; 20 ## 18 R &lt;--&gt; 1, R &lt;--&gt; 2, R &lt;--&gt; 3, R &lt;--&gt; 4, R &lt;--&gt; 5, R &lt;--&gt; 6, R &lt;--&gt; 7, R &lt;--&gt; 8, R &lt;--&gt; 9, R &lt;--&gt; 10, R &lt;--&gt; 11, R &lt;--&gt; 12, R &lt;--&gt; 13, R &lt;--&gt; 14, R &lt;--&gt; 15, R &lt;--&gt; 16, R &lt;--&gt; 17, R &lt;--&gt; 18, R &lt;--&gt; 19, R &lt;--&gt; 20 ## 19 S &lt;--&gt; 1, S &lt;--&gt; 2, S &lt;--&gt; 3, S &lt;--&gt; 4, S &lt;--&gt; 5, S &lt;--&gt; 6, S &lt;--&gt; 7, S &lt;--&gt; 8, S &lt;--&gt; 9, S &lt;--&gt; 10, S &lt;--&gt; 11, S &lt;--&gt; 12, S &lt;--&gt; 13, S &lt;--&gt; 14, S &lt;--&gt; 15, S &lt;--&gt; 16, S &lt;--&gt; 17, S &lt;--&gt; 18, S &lt;--&gt; 19, S &lt;--&gt; 20 ## 20 T &lt;--&gt; 1, T &lt;--&gt; 2, T &lt;--&gt; 3, T &lt;--&gt; 4, T &lt;--&gt; 5, T &lt;--&gt; 6, T &lt;--&gt; 7, T &lt;--&gt; 8, T &lt;--&gt; 9, T &lt;--&gt; 10, T &lt;--&gt; 11, T &lt;--&gt; 12, T &lt;--&gt; 13, T &lt;--&gt; 14, T &lt;--&gt; 15, T &lt;--&gt; 16, T &lt;--&gt; 17, T &lt;--&gt; 18, T &lt;--&gt; 19, T &lt;--&gt; 20 Ahora cada fila tiene un vector de 20 elementos guardado en la columna resultado 3.1.5 Funciones implícitas no es necesario que definamos la función de antemano. Podemos usar funciones implícitas map_dbl(c(1:10), function(x) x^2) ## [1] 1 4 9 16 25 36 49 64 81 100 map2_dbl(c(1:10),c(11:20), function(x,y) x*y) ## [1] 11 24 39 56 75 96 119 144 171 200 3.1.6 Funciones lambda incluso más conciso que las funciones implíictas son las funciones lambda donde definimos las variables como .x .y, etc. La flexibilidad de estas expresiones es limitada, pero puede ser útil en algunos casos. map_dbl(c(1:10),~.x^2) ## [1] 1 4 9 16 25 36 49 64 81 100 map2_dbl(c(1:10),c(11:20),~.x*.y) ## [1] 11 24 39 56 75 96 119 144 171 200 3.1.7 Walk Las funciones Walk Tienen la misma forma que los map, pero se usan cuando lo que queremos iterar no genera una salida, sino que nos interesan los efectos secundarios que generan. map2(ABC_123$Letras,ABC_123$Num,funcion_prueba)[1:3] ## [[1]] ## [1] &quot;A &lt;--&gt; 1&quot; ## ## [[2]] ## [1] &quot;B &lt;--&gt; 2&quot; ## ## [[3]] ## [1] &quot;C &lt;--&gt; 3&quot; walk2(ABC_123$Letras,ABC_123$Num,funcion_prueba) imprimir_salida &lt;- function(x,y){ print(funcion_prueba(x,y)) } walk2(ABC_123$Letras,ABC_123$Num,imprimir_salida) ## [1] &quot;A &lt;--&gt; 1&quot; ## [1] &quot;B &lt;--&gt; 2&quot; ## [1] &quot;C &lt;--&gt; 3&quot; ## [1] &quot;D &lt;--&gt; 4&quot; ## [1] &quot;E &lt;--&gt; 5&quot; ## [1] &quot;F &lt;--&gt; 6&quot; ## [1] &quot;G &lt;--&gt; 7&quot; ## [1] &quot;H &lt;--&gt; 8&quot; ## [1] &quot;I &lt;--&gt; 9&quot; ## [1] &quot;J &lt;--&gt; 10&quot; ## [1] &quot;K &lt;--&gt; 11&quot; ## [1] &quot;L &lt;--&gt; 12&quot; ## [1] &quot;M &lt;--&gt; 13&quot; ## [1] &quot;N &lt;--&gt; 14&quot; ## [1] &quot;O &lt;--&gt; 15&quot; ## [1] &quot;P &lt;--&gt; 16&quot; ## [1] &quot;Q &lt;--&gt; 17&quot; ## [1] &quot;R &lt;--&gt; 18&quot; ## [1] &quot;S &lt;--&gt; 19&quot; ## [1] &quot;T &lt;--&gt; 20&quot; Eso que vemos es el efecto secundario dentro de la función (imprimir) 3.1.8 Cuando usar estas herramientas? A lo largo del curso vimos diferentes técnicas para manipulación de datos. En particular, la librería dplyr nos permitía fácilmente modificar y crear nuevas variables, agrupando. Cuando usamos dplyr y cuando usamos purrr. Si trabajamos sobre un DF simple, sin variables anidadas (lo que conocíamos hasta hoy) podemos usar dplyr Si queremos trabajar con DF anidados, con cosas que no son DF, o si el resultado de la operación que vamos a realizar a nivel file es algo distinto a un valor único, nos conviene usar map y purrr Las funciones walk son útiles por ejemplo para escribir archivos en disco de forma iterativa. Algo que no genera una salida basado en https://jennybc.github.io/purrr-tutorial/ls03_map-function-syntax.html↩ "],
["practica-guiada-2.html", "3.2 Práctica Guiada", " 3.2 Práctica Guiada library(fs) library(tidyverse) library(openxlsx) library(glue) 3.2.1 Ejemplo 1: Iterando en la EPH Lo primero que necesitamos es definir un vector o lista sobre el que iterar. Por ejemplo, podemos armar un vector con los path a las bases individuales, con el comando fs::dir_ls bases_individuales_path &lt;- dir_ls(path = &#39;fuentes/&#39;, regexp= &#39;individual&#39;) bases_individuales_path ## fuentes/usu_individual_t119.txt fuentes/usu_individual_t418.txt Luego, como en la función que usamos para leer las bases definimos muchos parametros, nos podemos armar una función wrapper que sólo necesite un parámetro, y que simplifique la escritura del map leer_base_eph &lt;- function(path) { read.table(path,sep=&quot;;&quot;, dec=&quot;,&quot;, header = TRUE, fill = TRUE) %&gt;% select(ANO4,TRIMESTRE,REGION,P21,CH04, CH06) } bases_df &lt;- tibble(bases_individuales_path) %&gt;% mutate(base = map(bases_individuales_path, leer_base_eph)) bases_df ## # A tibble: 2 x 2 ## bases_individuales_path base ## &lt;fs::path&gt; &lt;list&gt; ## 1 fuentes/usu_individual_t119.txt &lt;df[,6] [59,369 × 6]&gt; ## 2 fuentes/usu_individual_t418.txt &lt;df[,6] [57,418 × 6]&gt; El resultado es un DF donde la columna base tiene en cada fila, otro DF con la base de la EPH de ese período. Esto es lo que llamamos un nested DF o dataframe nesteado pa les pibes. Si queremos juntar todo, podemos usar unnest() bases_df &lt;- bases_df %&gt;% unnest() bases_df ## # A tibble: 116,787 x 7 ## bases_individuales_path ANO4 TRIMESTRE REGION P21 CH04 CH06 ## &lt;fs::path&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 fuentes/usu_individual_t119.txt 2019 1 41 0 2 28 ## 2 fuentes/usu_individual_t119.txt 2019 1 41 0 2 13 ## 3 fuentes/usu_individual_t119.txt 2019 1 41 0 1 1 ## 4 fuentes/usu_individual_t119.txt 2019 1 41 5000 2 41 ## 5 fuentes/usu_individual_t119.txt 2019 1 41 0 2 9 ## 6 fuentes/usu_individual_t119.txt 2019 1 41 8000 1 51 ## 7 fuentes/usu_individual_t119.txt 2019 1 41 0 1 63 ## 8 fuentes/usu_individual_t119.txt 2019 1 41 0 2 62 ## 9 fuentes/usu_individual_t119.txt 2019 1 41 0 2 24 ## 10 fuentes/usu_individual_t119.txt 2019 1 41 3000 1 74 ## # … with 116,777 more rows ¿Qué pasa si los DF que tenemos nesteados no tienen la misma cantidad de columnas? Esto mismo lo podemos usar para fragmentar el datastet por alguna variable, con el group_by() bases_df %&gt;% group_by(REGION) %&gt;% nest() ## # A tibble: 6 x 2 ## # Groups: REGION [6] ## REGION data ## &lt;int&gt; &lt;list&lt;df[,6]&gt;&gt; ## 1 41 [11,509 × 6] ## 2 44 [14,204 × 6] ## 3 42 [11,150 × 6] ## 4 43 [34,702 × 6] ## 5 40 [24,432 × 6] ## 6 1 [20,790 × 6] Así, para cada región tenemos un DF. ¿ De qué sirve todo esto? No todo en la vida es un Dataframe. Hay estucturas de datos que no se pueden normalizar a filas y columnas. En esos casos recurríamos tradicionalmente a los loops. Con MAP podemos tener los elementos agrupados en un sólo objeto y aún conservar sus formas diferentes. 3.2.2 Ejemplo 2. Regresión lineal Si bien no nos vamos a meter en el detalle del modelo lineal hoy, es útil usarlo como ejemplo de lo que podemos hacer con MAP. Planteamos el modelo \\[ P21 = \\beta_0 + \\beta_1*CH04 + \\beta_2*CH06 \\] Osea, un modleo que explica el ingreso según sexo y edad lmfit &lt;- lm(P21~factor(CH04)+CH06,data = bases_df) summary(lmfit) ## ## Call: ## lm(formula = P21 ~ factor(CH04) + CH06, data = bases_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15472 -6606 -3367 2148 590198 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4853.196 74.509 65.14 &lt;2e-16 *** ## factor(CH04)2 -4063.112 72.200 -56.27 &lt;2e-16 *** ## CH06 103.095 1.612 63.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12300 on 116784 degrees of freedom ## Multiple R-squared: 0.05511, Adjusted R-squared: 0.0551 ## F-statistic: 3406 on 2 and 116784 DF, p-value: &lt; 2.2e-16 (al final de la clase podemos charlar sobre los resultados, si hay interés :-) ) De forma Tidy, la librería broom nos da los resultados en un DF. broom::tidy(lmfit) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4853. 74.5 65.1 0 ## 2 factor(CH04)2 -4063. 72.2 -56.3 0 ## 3 CH06 103. 1.61 64.0 0 Si lo queremos hacer por region 3.2.2.1 Loopeando resultados &lt;- tibble() for (region in unique(bases_df$REGION)) { data &lt;- bases_df %&gt;% filter(REGION==region) lmfit &lt;- lm(P21~factor(CH04)+CH06,data = data) lmtidy &lt;- broom::tidy(lmfit) lmtidy$region &lt;- region resultados &lt;- bind_rows(resultados,lmtidy) } resultados ## # A tibble: 18 x 6 ## term estimate std.error statistic p.value region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 (Intercept) 3768. 185. 20.3 3.15e- 90 41 ## 2 factor(CH04)2 -3814. 180. -21.2 6.00e- 98 41 ## 3 CH06 106. 4.18 25.3 1.12e-137 41 ## 4 (Intercept) 7156. 291. 24.6 1.09e-130 44 ## 5 factor(CH04)2 -5938. 278. -21.4 1.42e- 99 44 ## 6 CH06 145. 6.32 23.0 1.40e-114 44 ## 7 (Intercept) 4930. 231. 21.4 2.15e- 99 42 ## 8 factor(CH04)2 -4007. 224. -17.9 1.71e- 70 42 ## 9 CH06 97.8 4.95 19.7 2.68e- 85 42 ## 10 (Intercept) 5107. 131. 39.0 0. 43 ## 11 factor(CH04)2 -3949. 127. -31.1 5.02e-209 43 ## 12 CH06 83.5 2.78 30.0 3.87e-195 43 ## 13 (Intercept) 3329. 128. 26.0 4.12e-147 40 ## 14 factor(CH04)2 -3239. 125. -25.9 3.74e-146 40 ## 15 CH06 122. 2.89 42.2 0. 40 ## 16 (Intercept) 5196. 197. 26.4 3.45e-151 1 ## 17 factor(CH04)2 -4051. 189. -21.4 1.80e-100 1 ## 18 CH06 88.2 4.12 21.4 1.98e-100 1 3.2.2.2 Usando MAP Primero me armo una funcion que me simplifica el codigo fun&lt;-function(porcion,...) { broom::tidy(lm(P21~factor(CH04)+CH06,data = porcion))} bases_df_lm &lt;- bases_df %&gt;% group_by(REGION) %&gt;% nest() %&gt;% mutate(lm = map(data,fun)) bases_df_lm ## # A tibble: 6 x 3 ## # Groups: REGION [6] ## REGION data lm ## &lt;int&gt; &lt;list&lt;df[,6]&gt;&gt; &lt;list&gt; ## 1 41 [11,509 × 6] &lt;tibble [3 × 5]&gt; ## 2 44 [14,204 × 6] &lt;tibble [3 × 5]&gt; ## 3 42 [11,150 × 6] &lt;tibble [3 × 5]&gt; ## 4 43 [34,702 × 6] &lt;tibble [3 × 5]&gt; ## 5 40 [24,432 × 6] &lt;tibble [3 × 5]&gt; ## 6 1 [20,790 × 6] &lt;tibble [3 × 5]&gt; bases_df_lm %&gt;% unnest(lm) ## # A tibble: 18 x 7 ## # Groups: REGION [6] ## REGION data term estimate std.error statistic p.value ## &lt;int&gt; &lt;list&lt;df[,6]&gt;&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 41 [11,509 × 6] (Intercept) 3768. 185. 20.3 3.15e- 90 ## 2 41 [11,509 × 6] factor(CH0… -3814. 180. -21.2 6.00e- 98 ## 3 41 [11,509 × 6] CH06 106. 4.18 25.3 1.12e-137 ## 4 44 [14,204 × 6] (Intercept) 7156. 291. 24.6 1.09e-130 ## 5 44 [14,204 × 6] factor(CH0… -5938. 278. -21.4 1.42e- 99 ## 6 44 [14,204 × 6] CH06 145. 6.32 23.0 1.40e-114 ## 7 42 [11,150 × 6] (Intercept) 4930. 231. 21.4 2.15e- 99 ## 8 42 [11,150 × 6] factor(CH0… -4007. 224. -17.9 1.71e- 70 ## 9 42 [11,150 × 6] CH06 97.8 4.95 19.7 2.68e- 85 ## 10 43 [34,702 × 6] (Intercept) 5107. 131. 39.0 0. ## 11 43 [34,702 × 6] factor(CH0… -3949. 127. -31.1 5.02e-209 ## 12 43 [34,702 × 6] CH06 83.5 2.78 30.0 3.87e-195 ## 13 40 [24,432 × 6] (Intercept) 3329. 128. 26.0 4.12e-147 ## 14 40 [24,432 × 6] factor(CH0… -3239. 125. -25.9 3.74e-146 ## 15 40 [24,432 × 6] CH06 122. 2.89 42.2 0. ## 16 1 [20,790 × 6] (Intercept) 5196. 197. 26.4 3.45e-151 ## 17 1 [20,790 × 6] factor(CH0… -4051. 189. -21.4 1.80e-100 ## 18 1 [20,790 × 6] CH06 88.2 4.12 21.4 1.98e-100 O incluso más facil, utilizando group_modify (que es un atajo que solo acepta DF) bases_df %&gt;% group_by(REGION) %&gt;% group_modify(fun) ## # A tibble: 18 x 6 ## # Groups: REGION [6] ## REGION term estimate std.error statistic p.value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 (Intercept) 5196. 197. 26.4 3.45e-151 ## 2 1 factor(CH04)2 -4051. 189. -21.4 1.80e-100 ## 3 1 CH06 88.2 4.12 21.4 1.98e-100 ## 4 40 (Intercept) 3329. 128. 26.0 4.12e-147 ## 5 40 factor(CH04)2 -3239. 125. -25.9 3.74e-146 ## 6 40 CH06 122. 2.89 42.2 0. ## 7 41 (Intercept) 3768. 185. 20.3 3.15e- 90 ## 8 41 factor(CH04)2 -3814. 180. -21.2 6.00e- 98 ## 9 41 CH06 106. 4.18 25.3 1.12e-137 ## 10 42 (Intercept) 4930. 231. 21.4 2.15e- 99 ## 11 42 factor(CH04)2 -4007. 224. -17.9 1.71e- 70 ## 12 42 CH06 97.8 4.95 19.7 2.68e- 85 ## 13 43 (Intercept) 5107. 131. 39.0 0. ## 14 43 factor(CH04)2 -3949. 127. -31.1 5.02e-209 ## 15 43 CH06 83.5 2.78 30.0 3.87e-195 ## 16 44 (Intercept) 7156. 291. 24.6 1.09e-130 ## 17 44 factor(CH04)2 -5938. 278. -21.4 1.42e- 99 ## 18 44 CH06 145. 6.32 23.0 1.40e-114 Pero MAP sirve para operar con cualquier objeto de R. Por ejemplo podemos guardar el objeto S3:lm que es la regresion lineal entrenada. Ese objeto no es ni un vector, ni una lista, ni un DF. No es una estructura de datos, sino que es algo distinto, con propiedades como predict() para predecir, el summary() que vimos, etc. fun&lt;-function(porcion,grupo) { lm(P21~factor(CH04)+CH06,data = porcion)} bases_df %&gt;% group_by(REGION) %&gt;% nest() %&gt;% mutate(lm = map(data,fun)) ## # A tibble: 6 x 3 ## # Groups: REGION [6] ## REGION data lm ## &lt;int&gt; &lt;list&lt;df[,6]&gt;&gt; &lt;list&gt; ## 1 41 [11,509 × 6] &lt;lm&gt; ## 2 44 [14,204 × 6] &lt;lm&gt; ## 3 42 [11,150 × 6] &lt;lm&gt; ## 4 43 [34,702 × 6] &lt;lm&gt; ## 5 40 [24,432 × 6] &lt;lm&gt; ## 6 1 [20,790 × 6] &lt;lm&gt; 3.2.3 Ejemplo 3: Gráficos en serie Veamos un tercer ejemplo con otra base de datos que ya conocemos: Gapminder, que muestra algunos datos sobre la población de los países por año. El objetivo de este ejercicio es hacer un gráfico por país de forma automática. Primero veamos los datos library(gapminder) gapminder_unfiltered %&gt;% sample_n(10) ## # A tibble: 10 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Solomon Islands Oceania 1987 55.8 300694 1784. ## 2 Mauritania Africa 1967 46.3 1230542 1421. ## 3 Iraq Asia 2007 59.5 27499638 4471. ## 4 Chile Americas 2007 78.6 16284741 13172. ## 5 Korea, Dem. Rep. Asia 1957 54.1 9411381 1571. ## 6 Taiwan Asia 1988 73.2 19976096 11822. ## 7 Maldives Asia 1982 57.1 164210 985. ## 8 Korea, Dem. Rep. Asia 1982 69.1 17647518 4107. ## 9 Somalia Africa 1967 39.0 3428839 1285. ## 10 Nigeria Africa 1967 41.0 47287752 1015. la base tiene la siguiente info: country: Nombre del país continent: Nombre del continente year: año lifeExp: Esperanza de vida al nacer pop: Población gdpPercap Vamos a hacer un gráfico sencillo para Argentina data_argentina &lt;- gapminder_unfiltered %&gt;% filter(country==&#39;Argentina&#39;) ggplot(data_argentina, aes(year, lifeExp, size= pop, color=gdpPercap))+ geom_point()+ geom_line(alpha=0.6)+ labs(title = unique(data_argentina$country)) Ahora que tenemos una idea de lo que queremos gráficar lo podemos poner adentro de una función que grafique. # definimos la función graficar_pais &lt;- function(data, pais){ ggplot(data, aes(year, lifeExp, size= pop, color=gdpPercap))+ geom_point()+ geom_line(alpha=0.6)+ labs(title = pais) } probamos la función para un caso graficar_pais(data_argentina, &#39;Argentina&#39;) Nos armamos un dataset nesteado gapminder_nest &lt;- gapminder_unfiltered %&gt;% group_by(country) %&gt;% nest() gapminder_nest[1:10,] ## # A tibble: 10 x 2 ## # Groups: country [187] ## country data ## &lt;fct&gt; &lt;list&lt;df[,5]&gt;&gt; ## 1 Afghanistan [12 × 5] ## 2 Albania [12 × 5] ## 3 Algeria [12 × 5] ## 4 Angola [12 × 5] ## 5 Argentina [12 × 5] ## 6 Armenia [4 × 5] ## 7 Aruba [8 × 5] ## 8 Australia [56 × 5] ## 9 Austria [57 × 5] ## 10 Azerbaijan [4 × 5] Ahora podemos crear una nueva columna que contenga los gráficos gapminder_nest &lt;- gapminder_nest %&gt;% mutate(grafico= map2(.x = data, .y = country,.f = graficar_pais)) gapminder_nest[1:10,] ## # A tibble: 10 x 3 ## # Groups: country [187] ## country data grafico ## &lt;fct&gt; &lt;list&lt;df[,5]&gt;&gt; &lt;list&gt; ## 1 Afghanistan [12 × 5] &lt;gg&gt; ## 2 Albania [12 × 5] &lt;gg&gt; ## 3 Algeria [12 × 5] &lt;gg&gt; ## 4 Angola [12 × 5] &lt;gg&gt; ## 5 Argentina [12 × 5] &lt;gg&gt; ## 6 Armenia [4 × 5] &lt;gg&gt; ## 7 Aruba [8 × 5] &lt;gg&gt; ## 8 Australia [56 × 5] &lt;gg&gt; ## 9 Austria [57 × 5] &lt;gg&gt; ## 10 Azerbaijan [4 × 5] &lt;gg&gt; Veamos un ejemplo gapminder_nest$grafico[2] ## [[1]] Ahora podemos guardar todos los gráficos en un archivo PDF pdf(&#39;resultados/graficos_gapminder.pdf&#39;) gapminder_nest$grafico dev.off() "],
["visualizacion-de-la-informacion.html", "Capítulo-4 Visualización de la información ", " Capítulo-4 Visualización de la información "],
["explicacion-3.html", "4.1 Explicación", " 4.1 Explicación 4.1.1 Gráficos Básicos en R Rbase tiene algunos comandos genéricos para realizar gráficos, que se adaptan al tipo de información que se le pide graficar, por ejemplo: plot() hist() # iris es un set de datos clásico, que ya viene incorporado en R iris[1:10,] ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 1 5.1 3.5 1.4 0.2 setosa ## 2 2 4.9 3.0 1.4 0.2 setosa ## 3 3 4.7 3.2 1.3 0.2 setosa ## 4 4 4.6 3.1 1.5 0.2 setosa ## 5 5 5.0 3.6 1.4 0.2 setosa ## 6 6 5.4 3.9 1.7 0.4 setosa ## 7 7 4.6 3.4 1.4 0.3 setosa ## 8 8 5.0 3.4 1.5 0.2 setosa ## 9 9 4.4 2.9 1.4 0.2 setosa ## 10 10 4.9 3.1 1.5 0.1 setosa plot(iris) #Al especificar una variable, puedo ver el valor que toma cada uno de sus registros (Index) plot(iris$Sepal.Length,type = &quot;p&quot;) # Un punto por cada valor plot(iris$Sepal.Length,type = &quot;l&quot;) # Una linea que una cada valor plot(iris$Sepal.Length,type = &quot;b&quot;) #Ambas hist(iris$Sepal.Length, col = &quot;lightsalmon1&quot;, main = &quot;Histograma&quot;) 4.1.1.1 png La función png() nos permite grabar una imagen en el disco. Lleva como argumento principal la ruta completa a donde se desea guardar la misma, incluyendo el nombre que queremos dar al archivo. A su vez pueden especificarse otros argumentos como el ancho y largo de la imagen, entre otros. ruta_archivo &lt;- &quot;resultados/grafico1.PNG&quot; ruta_archivo ## [1] &quot;resultados/grafico1.PNG&quot; png(ruta_archivo) plot(iris$Sepal.Length,type = &quot;b&quot;) dev.off() ## png ## 2 La función png() abre el dispositivo de imagen en el directorio especificado. Luego creamos el gráfico que deseamos (o llamamos a uno previamente construido), el cual se desplegará en la ventana inferior derecha de la pantalla de Rstudio. Finalmente con dev.off() se cierra el dispositivo y se graban los gráficos. Los gráficos del R base son útiles para escribir de forma rápida y obtener alguna información mientras trabajamos. Muchos paquetes estadísticos permiten mostrar los resultados de forma gráfica con el comando plot (por ejemplo, las regresiones lineales lm()). Sin embargo, existen librerías mucho mejores para crear gráficos de nivel de publicación. La más importante es ggplot2, que a su vez tiene extensiones mediante otras librerías. 4.1.2 Ggplot2 ggplot tiene su sintaxis propia. La idea central es pensar los gráficos como una sucesión de capas, que se construyen una a la vez. El operador + nos permite incorporar nuevas capas al gráfico. El comando ggplot() nos permite definir la fuente de datos y las variables que determinaran los ejes del grafico (x,y), así como el color y la forma de las líneas o puntos,etc. Las sucesivas capas nos permiten definir: Uno o más tipos de gráficos (de columnas, geom_col(), de línea, geom_line(), de puntos, geom_point(), boxplot, geom_boxplot()) Títulos labs() Estilo del gráfico theme() Escalas de los ejes scale_y_continuous,scale_x_discrete División en subconjuntos facet_wrap(),facet_grid() ggplot tiene muchos comandos, y no tiene sentido saberlos de memoria, es siempre útil reutilizar gráficos viejos y tener a mano el machete. 4.1.2.1 Dimensiones del gráfico Esta forma de pensar los gráficos nos permite repenser los distintos atributos como potenciales aliados a la hora de mostrar información multidimensional. Por ejemplo: color color = rellenofill = forma shape = tamaño size = transparencia alpha = Abrir un mismo gráfico según alguna variable discreta: facet_wrap() Los atributos que queremos que mapeen una variable, deben ir dentro del aes(), aes(... color = variable) Cuando queremos simplemente mejorar el diseño (es fijo), se asigna por fuera, o dentro de cada tipo de gráficos, geom_col(color = 'green'). 4.1.2.2 Gráfico de Puntos A continuación se muestra un gráfico de varias capas de construcción, con su correspondiente porción de código. En el mismo se buscará visualizar, a partir de la base de datos iris la relación entre el ancho y el largo de los petalos, mediante un gráfico de puntos. library(tidyverse) # cargamos la librería ggplot(data = iris, aes(x = Petal.Length, Petal.Width, color = Species))+ geom_point(alpha=0.75)+ labs(title = &quot;Medidas de los pétalos por especie&quot;)+ theme(legend.position = &#39;none&#39;)+ facet_wrap(~Species) 4.1.2.3 Capas del Gráfico Veamos ahora, el “paso a paso” del armado del mismo. En primera instancia solo defino los ejes. Y en este caso un color particular para cada Especie. g &lt;- ggplot(data = iris, aes(x = Petal.Length, Petal.Width, color = Species)) g Luego, defino el tipo de gráfico. El alpha me permite definir la intensidad de los puntos g &lt;- g + geom_point(alpha=0.25) g Las siguientes tres capas me permiten respectivamente: Definir el título del gráfico Quitar la leyenda Abrir el gráfico en tres fragmentos, uno para cada especie g &lt;- g + labs(title = &quot;Medidas de los pétalos por especie&quot;)+ theme(legend.position = &#39;none&#39;)+ facet_wrap(~Species) g 4.1.2.4 Extensiones de GGplot. La librería GGplot tiene a su vez muchas otras librerías que extienden sus potencialidades. Entre nuestras favoritas están: gganimate: Para hacer gráficos animados. ggridge: Para hacer gráficos de densidad faceteados ggally: Para hacer varios gráficos juntos. treemapify library(GGally) ggpairs(iris, mapping = aes(color = Species)) library(ggridges) ggplot(iris, aes(x = Sepal.Length, y = Species, fill=Species)) + geom_density_ridges() También hay extensiones que te ayudan a escribir el código, como esquisse iris &lt;- iris #Correr en la consola esquisse::esquisser() 4.1.3 Ejemplo con datos reales 4.1.3.1 Boxplot de ingresos de la ocupación principal, según nivel educativo Individual_t119 &lt;- read.table(&quot;fuentes/usu_individual_t119.txt&quot;, sep=&quot;;&quot;, dec=&quot;,&quot;, header = TRUE, fill = TRUE) Hacemos un procesamiento simple: Sacamos los ingresos iguales a cero y las no respuestas de nivel educativo. Es importante que las variables sean del tipo que conceptualmente les corresponde (el nivel educativo es una variable categórica, no continua), para que el ggplot pueda graficarlo correctamente. # Las variables sexo( CH04 ) y Nivel educativo están codificadas como números, y el R las entiende como numéricas. class(Individual_t119$NIVEL_ED) ## [1] &quot;integer&quot; class(Individual_t119$CH04) ## [1] &quot;integer&quot; ggdata &lt;- Individual_t119 %&gt;% filter(P21&gt;0, !is.na(NIVEL_ED)) %&gt;% mutate(NIVEL_ED = as.factor(NIVEL_ED), CH04 = as.factor(CH04)) ggplot(ggdata, aes(x = NIVEL_ED, y = P21)) + geom_boxplot()+ scale_y_continuous(limits = c(0, 40000))#Restrinjo el gráfico hasta ingresos de $40000 Si queremos agregar la dimensión sexo, podemos hacer un facet_wrap() ggplot(ggdata, aes(x= NIVEL_ED, y = P21, group = NIVEL_ED, fill = NIVEL_ED )) + geom_boxplot()+ scale_y_continuous(limits = c(0, 40000))+ facet_wrap(~ CH04, labeller = &quot;label_both&quot;) Por la forma en que está presentado el gráfico, el foco de atención sigue puesto en las diferencias de ingresos entre niveles educativo. Simplemente se agrega un corte por la variable de sexo. Si lo que queremos hacer es poner el foco de atención en las diferencias por sexo, simplemente basta con invertir la variable x especificada con la variable utilizada en el facet_wrap ggplot(ggdata, aes(x= CH04, y = P21, group = CH04, fill = CH04 )) + geom_boxplot()+ scale_y_continuous(limits = c(0, 40000))+ facet_grid(~ NIVEL_ED, labeller = &quot;label_both&quot;) + theme(legend.position = &quot;none&quot;) "],
["practica-guiada-3.html", "4.2 Práctica Guiada", " 4.2 Práctica Guiada 4.2.1 Caso práctico: Gráficos de ingresos - EPH Para esta práctica utilizaremos las variables de ingresos captadas por la Encuesta Permanente de Hogares A continuación utilzaremos los conceptos abordados, para realizar gráficos a partir de las variables de ingresos. #Cargamos las librerías a utilizar library(tidyverse) # tiene ggplot, dplyr, tidyr, y otros library(ggthemes) # estilos de gráficos library(ggrepel) # etiquetas de texto más prolijas que las de ggplot library(scales) Individual_t119 &lt;- read.table(&quot;fuentes/usu_individual_t119.txt&quot;, sep=&quot;;&quot;, dec=&quot;,&quot;, header = TRUE, fill = TRUE) 4.2.1.1 Histogramas Por ejemplo, si observamos el ingreso de la ocupación principal: hist_data &lt;-Individual_t119 %&gt;% filter(P21&gt;0) ggplot(hist_data, aes(x = P21,weights = PONDIIO))+ geom_histogram(fill=&#39;salmon&#39;, color=&#39;grey25&#39;)+ scale_x_continuous(limits = c(0,50000)) En este gráfico, los posibles valores de p21 se dividen en 30 bins consecutivos y el gráfico muestra cuantas observaciones caen en cada uno de ellos 4.2.1.2 Kernels La función geom_density() nos permite construir kernels de la distribución. Es particularmente útil cuando tenemos una variable continua, dado que los histogramas rompen esa sensación de continuidad. Veamos un ejemplo sencillo con los ingresos de la ocupación principal. Luego iremos complejizandolo kernel_data &lt;-Individual_t119 %&gt;% filter(P21&gt;0) ggplot(kernel_data, aes(x = P21,weights = PONDIIO))+ geom_density(fill=&#39;salmon&#39;, color=&#39;grey25&#39;)+ scale_x_continuous(limits = c(0,50000)) El eje y no tiene demasiada interpretabilidad en los Kernel, porque hace a la forma en que se construyen las distribuciones. El parametro adjust, dentro de la función geom_densitynos permite reducir o ampliar el rango de suavizado de la distribución. Su valor por default es 1. Veamos que sucede si lo seteamos en 2 ggplot(kernel_data, aes(x = P21,weights = PONDIIO))+ geom_density(adjust = 2,fill=&#39;salmon&#39;, color=&#39;grey25&#39;)+ scale_x_continuous(limits = c(0,50000)) Como es esperable, la distribución del ingreso tiene “picos” en los valores redondos, ya que la gente suele declarar un valor aproximado al ingreso efectivo que percibe. Nadie declara ingresos de 30001. Al suavizar la serie con un kernel, eliminamos ese efecto.Si seteamos el rango para el suavizado en valores menores a 1, podemos observar estos picos. ggplot(kernel_data, aes(x = P21,weights = PONDIIO))+ geom_density(adjust = 0.01,fill=&#39;salmon&#39;, color=&#39;grey25&#39;)+ scale_x_continuous(limits = c(0,50000)) 4.2.1.3 Geom Smooth Para realizar estos gráficos, vamos a modificar un poco los datos: filtramos los ingresos iguales a 0. eliminamos las no respuestas de nivel educativo y las personas con educación especial. eliminamos las respuestas de tipo de establecimiento = ‘otros’. recodificamos las variables para que tengan nombres más sugestivos: Nivel educativo además la convertimos a factor, porque queremos explicitarle el orden de los valores con levels(). El “\\n”&quot; es un caracter especial que permite que el string continúe en la siguiente línea. Sexo. Tipo de establecimiento. ggdata &lt;- Individual_t119 %&gt;% filter(P21&gt;0, !is.na(NIVEL_ED), NIVEL_ED!=7, PP04A !=3) %&gt;% mutate(NIVEL_ED = factor(case_when(NIVEL_ED == 1 ~ &#39;Primaria \\n Incompleta&#39;, # &#39;\\n&#39; significa carriage return, o enter NIVEL_ED == 2 ~ &#39;Primaria \\n Completa&#39;, NIVEL_ED == 3 ~ &#39;Secundaria \\nIncompleta&#39;, NIVEL_ED == 4 ~ &#39;Secundaria \\nCompleta&#39;, NIVEL_ED == 5 ~ &#39;Superior \\nUniversitaria \\nIncompleta&#39;, NIVEL_ED == 6 ~ &#39;Superior \\nUniversitaria \\nCompleta&#39;, FALSE ~ &#39;Otro&#39;), levels= c(&#39;Primaria \\n Incompleta&#39;, &#39;Primaria \\n Completa&#39;, &#39;Secundaria \\nIncompleta&#39;, &#39;Secundaria \\nCompleta&#39;, &#39;Superior \\nUniversitaria \\nIncompleta&#39;, &#39;Superior \\nUniversitaria \\nCompleta&#39;)), Sexo = case_when(CH04 == 1 ~ &#39;Varón&#39;, CH04 == 2 ~ &#39;Mujer&#39;), Establecimiento = case_when(PP04A == 1 ~ &#39;Estatal&#39;, PP04A == 2 ~ &#39;Privado&#39;, FALSE ~ &#39;Otro&#39;)) ggdata %&gt;% sample_n(10) ## CODUSU ANO4 TRIMESTRE NRO_HOGAR COMPONENTE H15 ## 1 TQRMNORTQHMMKSCDEHNHB00632095 2019 1 3 1 1 ## 2 TQRMNOSSQHJOLLCDEGIBJ00630141 2019 1 1 1 1 ## 3 TQRMNOPRXHJKKTCDEHMHF00625868 2019 1 1 1 1 ## 4 TQRMNOSYTHKMKMCDEIKAH00610989 2019 1 1 2 1 ## 5 TQRMNOSWQHLLLTCDEFLID00601113 2019 1 1 4 1 ## 6 TQRMNORSWHKMKPCDEGOIH00631056 2019 1 1 2 1 ## 7 TQRMNOPSVHKMKMCDEGOIH00631100 2019 1 1 1 1 ## 8 TQRMNORXRHKOLMCDEHNHB00609376 2019 1 1 3 1 ## 9 TQRMNOQRUHLOLUCDEHPJB00603972 2019 1 1 3 1 ## 10 TQRMNORQUHKOKMCDEFPCH00607232 2019 1 1 1 1 ## REGION MAS_500 AGLOMERADO PONDERA CH03 CH04 CH05 CH06 CH07 CH08 ## 1 42 N 27 339 1 1 05/02/1979 39 2 1 ## 2 41 N 12 266 1 2 09/06/1978 40 3 1 ## 3 42 N 26 180 1 2 19/04/1997 21 5 1 ## 4 43 S 34 534 2 2 08/06/1971 47 1 1 ## 5 43 S 5 378 3 1 14/06/1989 29 5 1 ## 6 40 N 18 52 2 1 24/11/1978 40 1 4 ## 7 40 N 18 44 1 2 10/05/1972 46 3 1 ## 8 42 N 27 242 3 2 09/03/1987 32 5 4 ## 9 40 S 29 328 3 1 08/08/1998 20 5 1 ## 10 44 N 9 270 1 1 28/11/1969 49 1 1 ## CH09 CH10 CH11 CH12 CH13 CH14 CH15 CH15_COD CH16 CH16_COD ## 1 1 2 0 4 2 4 1 NA 1 NA ## 2 1 2 0 7 1 NA 1 NA 1 NA ## 3 1 1 1 7 2 1 2 NA 1 NA ## 4 1 2 0 7 1 NA 1 NA 1 NA ## 5 1 2 0 4 1 NA 1 NA 1 NA ## 6 1 2 0 2 2 3 2 NA 1 NA ## 7 1 2 0 2 1 NA 2 NA 1 NA ## 8 1 2 0 7 2 3 1 NA 1 NA ## 9 1 2 0 4 1 NA 1 NA 1 NA ## 10 1 2 0 4 1 NA 4 225 2 NA ## NIVEL_ED ESTADO CAT_OCUP CAT_INAC IMPUTA ## 1 Secundaria \\nIncompleta 1 3 0 0 ## 2 Superior \\nUniversitaria \\nCompleta 1 3 0 0 ## 3 Superior \\nUniversitaria \\nIncompleta 1 3 0 0 ## 4 Superior \\nUniversitaria \\nCompleta 1 3 0 0 ## 5 Secundaria \\nCompleta 1 3 0 0 ## 6 Primaria \\n Incompleta 1 3 0 0 ## 7 Primaria \\n Completa 1 3 0 0 ## 8 Superior \\nUniversitaria \\nIncompleta 1 2 0 0 ## 9 Secundaria \\nCompleta 1 3 0 0 ## 10 Secundaria \\nCompleta 1 3 0 0 ## PP02C1 PP02C2 PP02C3 PP02C4 PP02C5 PP02C6 PP02C7 PP02C8 PP02E PP02H ## 1 0 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 0 ## PP02I PP03C PP03D PP3E_TOT PP3F_TOT PP03G PP03H PP03I PP03J INTENSI ## 1 0 1 0 60 0 2 0 2 2 3 ## 2 0 2 2 0 30 1 1 1 1 1 ## 3 0 1 0 20 0 2 0 2 2 2 ## 4 0 2 2 25 8 1 1 1 2 1 ## 5 0 1 0 44 0 2 0 2 2 2 ## 6 0 1 0 32 0 2 0 2 2 2 ## 7 0 1 0 30 0 2 0 2 2 2 ## 8 0 1 0 60 0 2 0 2 2 3 ## 9 0 1 0 20 0 1 1 1 1 1 ## 10 0 1 0 54 0 2 0 2 2 3 ## PP04A PP04B_COD PP04B1 PP04B2 PP04B3_MES PP04B3_ANO PP04B3_DIA PP04C ## 1 2 9503 2 0 0 0 0 2 ## 2 1 8501 2 0 0 0 0 7 ## 3 1 8401 2 0 0 0 0 12 ## 4 2 2202 2 0 0 0 0 6 ## 5 2 4808 2 0 0 0 0 99 ## 6 2 8000 2 0 0 0 0 2 ## 7 2 8509 2 0 0 0 0 99 ## 8 2 3200 2 0 0 0 0 1 ## 9 1 8401 2 0 0 0 0 99 ## 10 2 4903 2 0 0 0 0 99 ## PP04C99 PP04D_COD PP04G PP05B2_MES PP05B2_ANO PP05B2_DIA PP05C_1 ## 1 0 72313 1 0 0 0 0 ## 2 0 41332 1 0 0 0 0 ## 3 0 62313 9 0 0 0 0 ## 4 0 30333 1 0 0 0 0 ## 5 2 32314 4 0 0 0 0 ## 6 0 47314 8 0 0 0 0 ## 7 2 56314 1 0 0 0 0 ## 8 0 80113 6 0 0 0 1 ## 9 3 10312 1 0 0 0 0 ## 10 3 36203 1 0 0 0 0 ## PP05C_2 PP05C_3 PP05E PP05F PP05H PP06A PP06C PP06D PP06E PP06H PP07A ## 1 0 0 0 0 0 0 0 0 0 0 6 ## 2 0 0 0 0 0 0 0 0 0 0 6 ## 3 0 0 0 0 0 0 0 0 0 0 6 ## 4 0 0 0 0 0 0 0 0 0 0 6 ## 5 0 0 0 0 0 0 0 0 0 0 5 ## 6 0 0 0 0 0 0 0 0 0 0 5 ## 7 0 0 0 0 0 0 0 0 0 0 6 ## 8 3 3 1 7 6 2 5000 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 2 ## 10 0 0 0 0 0 0 0 0 0 0 6 ## PP07C PP07D PP07E PP07F1 PP07F2 PP07F3 PP07F4 PP07F5 PP07G1 PP07G2 ## 1 2 0 0 2 2 2 2 5 1 1 ## 2 2 0 0 2 2 2 2 5 1 1 ## 3 1 5 1 2 2 2 2 5 2 2 ## 4 2 0 0 2 2 2 2 5 1 1 ## 5 2 0 0 2 2 2 2 5 1 1 ## 6 9 9 4 2 2 2 2 5 2 2 ## 7 2 0 0 2 2 2 2 5 1 1 ## 8 0 0 0 0 0 0 0 0 0 0 ## 9 1 2 4 2 2 2 2 5 2 2 ## 10 2 0 0 2 2 2 2 5 1 1 ## PP07G3 PP07G4 PP07G_59 PP07H PP07I PP07J PP07K PP08D1 PP08D4 PP08F1 ## 1 1 1 0 1 0 1 1 15000 0 0 ## 2 1 1 0 1 0 1 1 35500 0 0 ## 3 1 1 0 2 2 1 4 9000 0 0 ## 4 1 1 0 1 0 1 1 13000 0 0 ## 5 1 1 0 1 0 1 1 20000 0 0 ## 6 2 2 5 2 2 1 4 6000 0 0 ## 7 1 1 0 1 0 1 1 16000 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 ## 9 2 1 0 1 0 1 1 6000 0 0 ## 10 1 1 0 1 0 3 1 40000 0 0 ## PP08F2 PP08J1 PP08J2 PP08J3 PP09A PP09A_ESP PP09B PP09C PP09C_ESP PP10A ## 1 0 0 0 0 0 0 0 NA ## 2 0 12000 0 0 0 2 1 SAN ROQUE NA ## 3 0 0 0 0 0 0 0 NA ## 4 0 0 0 0 0 0 0 NA ## 5 0 0 0 0 0 1 0 NA ## 6 0 0 0 0 0 0 0 NA ## 7 0 0 0 0 0 0 0 NA ## 8 0 0 0 0 0 0 0 NA ## 9 0 0 0 0 0 0 0 NA ## 10 0 20000 0 0 0 0 0 NA ## PP10C PP10D PP10E PP11A PP11B_COD PP11B1 PP11B2_MES PP11B2_ANO ## 1 NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## 7 NA NA NA NA NA NA NA NA ## 8 NA NA NA NA NA NA NA NA ## 9 NA NA NA NA NA NA NA NA ## 10 NA NA NA NA NA NA NA NA ## PP11B2_DIA PP11C PP11C99 PP11D_COD PP11G_ANO PP11G_MES PP11G_DIA PP11L ## 1 NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA ## 7 NA NA NA NA NA NA NA NA ## 8 NA NA NA NA NA NA NA NA ## 9 NA NA NA NA NA NA NA NA ## 10 NA NA NA NA NA NA NA NA ## PP11L1 PP11M PP11N PP11O PP11P PP11Q PP11R PP11S PP11T P21 DECOCUR ## 1 NA NA NA NA NA NA NA NA NA 15000 5 ## 2 NA NA NA NA NA NA NA NA NA 35500 9 ## 3 NA NA NA NA NA NA NA NA NA 9000 3 ## 4 NA NA NA NA NA NA NA NA NA 13000 4 ## 5 NA NA NA NA NA NA NA NA NA 20000 6 ## 6 NA NA NA NA NA NA NA NA NA 6000 2 ## 7 NA NA NA NA NA NA NA NA NA 16000 5 ## 8 NA NA NA NA NA NA NA NA NA 5000 2 ## 9 NA NA NA NA NA NA NA NA NA 6000 2 ## 10 NA NA NA NA NA NA NA NA NA 40000 10 ## IDECOCUR RDECOCUR GDECOCUR PDECOCUR ADECOCUR PONDIIO TOT_P12 P47T ## 1 5 6 NA 5 6 339 0 15000 ## 2 10 10 NA 9 10 264 10000 77500 ## 3 3 3 NA 3 4 220 0 9000 ## 4 5 4 4 NA 4 624 5000 18000 ## 5 7 6 6 NA 6 445 0 20000 ## 6 2 2 NA 2 2 55 0 6000 ## 7 5 7 NA 5 7 45 0 20000 ## 8 2 2 NA 2 2 242 0 5000 ## 9 2 3 2 NA 2 331 0 6000 ## 10 10 9 NA 10 8 277 0 60000 ## DECINDR IDECINDR RDECINDR GDECINDR PDECINDR ADECINDR PONDII V2_M V3_M ## 1 5 5 6 NA 6 6 342 0 0 ## 2 10 10 10 NA 10 10 289 0 0 ## 3 3 3 3 NA 3 3 209 0 0 ## 4 6 6 6 6 NA 5 782 0 0 ## 5 6 7 6 6 NA 7 450 0 0 ## 6 2 2 2 NA 2 2 55 0 0 ## 7 6 7 7 NA 7 8 45 0 0 ## 8 2 2 1 NA 2 2 243 0 0 ## 9 2 2 2 2 NA 2 340 0 0 ## 10 10 10 10 NA 10 9 281 0 0 ## V4_M V5_M V8_M V9_M V10_M V11_M V12_M V18_M V19_AM V21_M T_VI ITF ## 1 0 0 0 0 0 0 0 0 0 0 0 37500 ## 2 0 0 0 0 0 0 20000 0 0 0 20000 77500 ## 3 0 0 0 0 0 0 0 0 0 0 0 14000 ## 4 0 0 0 0 0 0 0 0 0 0 0 74000 ## 5 0 0 0 0 0 0 0 0 0 0 0 73000 ## 6 0 0 0 0 0 0 0 0 0 0 0 14300 ## 7 0 0 0 0 0 0 4000 0 0 0 4000 20000 ## 8 0 0 0 0 0 0 0 0 0 0 0 29200 ## 9 0 0 0 0 0 0 0 0 0 0 0 26500 ## 10 0 0 0 0 0 0 0 0 0 0 0 105000 ## DECIFR IDECIFR RDECIFR GDECIFR PDECIFR ADECIFR IPCF DECCFR IDECCFR ## 1 6 7 7 NA 7 7 12500.00 6 6 ## 2 9 10 10 NA 10 10 25833.33 9 9 ## 3 2 2 2 NA 2 2 7000.00 3 3 ## 4 9 10 9 9 NA 9 18500.00 7 8 ## 5 9 9 9 9 NA 10 18250.00 7 8 ## 6 2 2 2 NA 2 2 3575.00 1 1 ## 7 3 3 4 NA 4 3 6666.67 3 3 ## 8 5 5 5 NA 5 6 7300.00 3 3 ## 9 5 5 5 5 NA 5 8833.33 4 4 ## 10 10 10 10 NA 10 10 26250.00 9 9 ## RDECCFR GDECCFR PDECCFR ADECCFR PONDIH Sexo Establecimiento ## 1 6 NA 6 7 342 Varón Privado ## 2 10 NA 9 10 277 Mujer Estatal ## 3 3 NA 3 3 207 Mujer Estatal ## 4 7 7 NA 7 871 Mujer Privado ## 5 7 7 NA 8 637 Varón Privado ## 6 2 NA 1 2 49 Varón Privado ## 7 4 NA 3 4 44 Mujer Privado ## 8 3 NA 3 3 242 Mujer Privado ## 9 5 4 NA 5 354 Varón Estatal ## 10 8 NA 9 8 272 Varón Privado Para graficar un suavizado de las series, se utiliza la función geom_smooth(). Con suavizado nos referimos al gráfico de un modelo realizado sobre los datos, que estima el valor en el punto x,y (para el grupo). Las regresiones lineales son un ejemplo de esto, aunque no el único, ni el que viene por default. ggplot(ggdata, aes(CH06, P21, colour = Sexo, shape = Sexo, alpha = P21))+ geom_smooth() + labs( x = &#39;Edad&#39;, y = &#39;ingreso&#39;, title = &#39;Ingreso por ocupación principal&#39;, subtitle = &#39;Según edad, nivel educativo y sexo&#39;) + theme_minimal()+ scale_y_continuous(labels = comma)+ scale_alpha(guide = FALSE)+ facet_grid(.~NIVEL_ED) Si corremos el comando geom_smooth() por default, nos advierte que esta utilizando el método GAM, de general additive models. el sombreado gris que envuelve cada línea es el intervalo de confianza de dicho punto (95% por default). También podemos utilizar métodos lineales, agregando el parámetro method = 'lm'. Haciendo esto, el gráfico muestra una regresión lineal simple. Si queremos otro tipo de regresión lineal, le podemos explicitar la fórmula. En el ejemplo siguiente, utilizamos la formula \\(y = \\beta_0 +\\beta_1x +\\beta_2 x^2\\). ggplot(ggdata, aes(CH06, P21, colour = Sexo, weight = PONDIIO)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2)) + labs(x = &#39;Edad&#39;, y = &#39;ingreso&#39;, title = &#39;Regresion cuadrática del Ingreso por ocupación principal respecto de la Edad&#39;, subtitle = &#39;Según Nivel educativo y sexo&#39;) + theme_minimal()+ facet_grid(. ~ NIVEL_ED) Si quisiéramos, además de ver la relación entre ingreso, Edad, Sexo y Nivel educativo, incorporar el tipo de establecimiento,público o privado. Podemos facetear el gráfico por dos variables en lugar de una, lo que crea una matriz de gráficos según los cruces. ggplot(ggdata, aes(CH06, P21, colour = Establecimiento, weight = PONDIIO)) + geom_smooth(method = &quot;lm&quot;) + labs( x = &#39;Edad&#39;, y = &#39;ingreso&#39;, title = &#39;Tendencia del ingreso por ocupación principal&#39;, subtitle = &#39;Según edad, nivel educativo, sexo y tipo de establecimiento&#39;) + theme_minimal()+ facet_grid(Sexo ~ NIVEL_ED) ggsave(filename = paste0(&quot;resultados/&quot;, &quot;regresion lineal.png&quot;),scale = 2) 4.2.1.4 Treemaps (bonus track) library(treemapify) Trabajo doméstico no remunerado trabajo_no_remunerado &lt;- read_csv(&#39;fuentes/prom_t_simul_dom_16_sexo__annio__g_edad_limpio.csv&#39;) trabajo_no_remunerado %&gt;% filter(sexo != &#39;TOTAL&#39;, grupo_edad != &#39;TOTAL&#39;) %&gt;% mutate(promedio_hs_diarias = as.numeric(promedio_hs_diarias), sexo = case_when(sexo==&#39;m&#39;~&#39;Mujer&#39;, sexo==&#39;v&#39;~&#39;Varón&#39;)) %&gt;% ggplot(., aes(area = promedio_hs_diarias, fill = promedio_hs_diarias, label = grupo_edad, subgroup = sexo)) + geom_treemap() + geom_treemap_subgroup_border() + geom_treemap_subgroup_text(place = &quot;centre&quot;, grow = T, alpha = 0.5, colour = &quot;black&quot;, fontface = &quot;italic&quot;, min.size = 0) + geom_treemap_text(colour = &quot;white&quot;, place = &quot;topleft&quot;, reflow = T)+ theme(legend.position = &#39;none&#39;) "],
["documentacion-en-r.html", "Capítulo-5 Documentación en R ", " Capítulo-5 Documentación en R "],
["explicacion-4.html", "5.1 Explicación", " 5.1 Explicación 5.1.1 R Markdown: Introducción Los archivos R Markdown nos permiten combinar código, resultados y texto. El objetivo de esta clase es aprender a trabajar bajo dicho entorno para facilitar 3 aplicaciones: Documentar el trabajo que realizamos, incluyendo comentarios sobre los procedimientos. Compartir código y resultados con gente que también trabaja en R. Compartir resultados con gente que no trabaja en R, y simplemente necesita enfocarse en las conclusiones. Las presentes notas de clase están basadas en el libro R4DS y las cheatsheets. También se recomienda el libro R Markdown: The Definitive Guide. 5.1.2 Requisitos Necesitamos instalar y cargar el paquete rmarkdown, pero por lo general no hace falta hacerlo explíticamente porque RStudio realiza esto automáticamente cuando es necesario. 5.1.3 Markdown básico Se trata de un archivo de extensión .Rmd. Contiene en su estructura tres tipos importantes de contenido: Un encabezado YAML (“Yet another markup lenguage”) rodeado de - - - --- title: &quot;El título de nuestro informe&quot; date: Septiembre 2019 output: html_document --- Bloques de código de R rodeado de ```. Texto con formato (que veremos en unos minutos) Cuando abrimos un archivo .Rmd, obtenemos una interfaz de notebook donde el código y el output se encuentran intercalados (en lugar de aparecer el output sólo en la consola, panel de Plots y/o modificaciones en el entorno de trabajo). Los bloques de código se pueden ejecutar haciendo click en el ícono ejecutar (el botón de Play en la parte superior/derecha del bloque), o presionando Cmd/Ctrl + Shift + Enter. RStudio ejecuta el código y muestra los resultados incustrados en el código. Para producir un reporte completo que contenga todo el texto, código y resultados, podemos clickear en Knit o presionar Cmd/Ctrl + Shift + K. Esto mostrará el reporte en el panel Viewer y creará un archivo HTML independiente que podremos compartir con otros. 5.1.4 Formateo de texto La prosa en los archivos .Rmd está escrita en Markdown, una colección simple de convenciones para dar formato a archivos de texto plano. Markdown está diseñado para ser fácil de leer y fácil de escribir, siendo también muy fácil de aprender. Del Cheatsheet: 5.1.5 Bloques de código Como ya mencionamos, para ejecutar código dentro de un documento R Markdown, necesitamos insertar un bloque (Chunk). Hay tres maneras para hacerlo: El atajo de teclado Cmd/Ctrl + Alt + I El icono “Insertar” en la barra de edición (Insert &gt; R) Tipear manualmente los delimitadores de bloque ```{r} y ```. Obviamente, recomendamos usar el atajo de teclado porque, a largo plazo, ahorra mucho tiempo. El código se puede seguir corriendo con Cmd/Ctrl + Enter línea a línea. Sin embargo, los bloques de código tienen otro atajo de teclado: Cmd/Ctrl + Shift + Enter, que ejecuta todo el código en el bloque. Un bloque debería ser relativamente autónomo, y enfocado alrededor de una sola tarea. Las siguientes secciones decriben el encabezado de bloque que consiste en ```{r, seguido por un nombre opcional para el bloque, seguido entonces por opciones separadas por comas, y concluyendo con }. Inmediatamente después sigue tu código de R el bloque y el fin del bloque se indica con un ``` final. Hay un nombre de bloque que tiene comportamiento especial: setup. Cuando nos encontramos en modo notebook, el bloque llamado setup se ejecutará automáticamente una vez, antes de ejecutar cualquier otro código. 5.1.6 Opciones en los bloques de código La salida de los bloques puede personalizarse con options, argumentos suministrados en el encabezado del bloque. Knitr provee casi 60 opciones para que puedas usar para personalizar tus bloques de código, la lista completa puede verse en http://yihui.name/knitr/options/. Las que más utilizamos nosotros son: eval = FALSE evita que código sea evaluado. (Y obviamente si el código no es ejecutado no se generaran resultados). Esto es útil para mostrar códigos de ejemplo, o para deshabilitar un gran bloque de código sin comentar cada línea. include = FALSE ejecuta el código, pero no muestra el código o los resultados en el documento final. Usa esto para el código de configuracion que no quieres que abarrote tu reporte. echo = FALSE evita que se vea el código, pero no los resultados en el archivo final. Utiliza esto cuando quieres escribir reportes enfocados a personas que no quieren ver el código subyacente de R. message = FALSE o warning = FALSE evita que aparezcan mensajes o advertencias en el archivo final. results = 'hide' oculta el output impreso; fig.show = 'hide' oculta gráficos. error = TRUE causa que el render continúe incluso si el código devuelve un error. Esto es algo que raramente querés incluir en la version final de tu reporte. Contamos con algunas de estas opciones en el menú de Configuración en la parte superior-derecha del Chunk de código. 5.1.7 Tablas Por defecto, las tablas se imprimen tal como salen como en la consola. Si queremos que los datos tengan un formato adicional podemos usar la función knitr::kable(). Aún más, recomendamos mirar los paquetes kableExtra y formattable. Para una mayor personalización, se pueden considerar también los paquetes xtable, stargazer, pander, tables, y ascii. Cada uno provee un set de herramientas para generar tablas con formato a partir código de R. 5.1.8 Opciones globales Algunas de las opciones default que tienen los bloques de código pueden no ajustarse a tus necesidades. Podemos setear cambios incluyendo knitr::opts_chunk$set() en un bloque de código. Por ejemplo: knitr::opts_chunk$set(echo = FALSE) Ocultará el código por defecto, así que sólo mostrará los bloques que deliberadamente elegimos mostrar con echo = TRUE. 5.1.9 Código en la línea Otra forma de incluir código R en un documento R Markdown es insertarlo directamente en el texto, encerrando entre `` &quot;r codigo&quot;. Esto puede ser muy útil si queremos mencionar propiedades o atributos de los datos o resultados en el texto. Cuando insertamos números en el texto, format() nos va a ser de mucha ayuda. Esto permite establecer el número de digitos para que no imprimas con un grado rídiculo de precisión, y una big.mark para hacer que los números sean mas fáciles de leer. Por ejemplo, en una función de ayuda: formato &lt;- function(x){ format(x, digits = 2, big.mark = &quot;.&quot;, decimal.mark = &quot;,&quot;) } formato(3452345) ## [1] &quot;3.452.345&quot; formato(.12358124331) ## [1] &quot;0,12&quot; 5.1.10 Formatos Hasta ahora vimos R Markdown para producir documentos HTML: --- title: &quot;Clase&quot; output: html_document --- Para sobrescribir los parámetros predeterminados se necesita usar un campo de output extendido. Por ejemplo, si queremos generar un html_document con una tabla de contenido flotante, usamos: --- title: &quot;Clase&quot; output: html_document: toc: true toc_float: true --- Para los html_document otra opción es hacer que los fragmentos de código estén escondidos por defecto, pero visibles con un click: --- title: &quot;Clase&quot; output: html_document: code_folding: hide --- 5.1.11 Otros formatos Hay todo un número de variaciones básicas para generar diferentes tipos de documentos: pdf_document crea un PDF con LaTeX (un sistema de código abierto de composición de textos), que necesitarás instalar. RStudio te notificará si no lo tienes. word_document para documentos de Microsoft Word (.docx). odt_document para documentos de texto OpenDocument (.odt). y más! 5.1.12 Notebooks Un notebook, html_notebook (“cuaderno” en español), es una variación de un html_document. Las salidas de los dos documentos son muy similares, pero tienen propósitos distintos. Un html_document está enfocado en la comunicación con los encargados de la toma de decisiones, mientras que un notebook está enfocado en colaborar con otros científicos de datos. Estos propósitos diferentes llevan a usar la salida HTML de diferentes maneras. Ambas salidas HTML contendrán la salida renderizada, pero el notebook también contendrá el código fuente completo. Esto significa que podemos usar el archivo .nb.html generado por el notebook de dos maneras: Podemos verlo en un navegador web, y ver la salida generada. A diferencia del html_document, esta renderización siempre incluye una copia incrustada del código fuente que lo generó. Podemos editarlo en RStudio. Cuando abramos un archivo .nb.html, RStudio automáticamente recreará el archivo .Rmd que lo creó. 5.1.13 Publicar Desde RStudio tenemos la posibilidad de publicar nuestros Markdown en RPubs de forma gratuita, desde el botón Publish document. Todo lo que subamos a nuestra cuenta de RPubs será público. 5.1.14 FlexDashboard Los dashboards (“tableros de control” en español) son una forma útil de comunicar grandes cantidades de información de forma visual y rápida. Flexdashboard hace que sea particularmente fácil crear dashboards usando R Markdown y proporciona una convención de cómo los encabezados afectan el diseño: Cada encabezado de Nivel 1 (#) comienza una nueva página en el dashboard. Cada encabezado de Nivel 2 (##) comienza una nueva columna. Cada encabezado de Nivel 3 (###) comienza una nueva fila. Flexdashboard también proporciona herramientas simples para crear barras laterales, tabuladores, cuadros de valores y medidores. Podemos obtener más información (en inglés) acerca de Flexdashboard en http://rmarkdown.rstudio.com/flexdashboard/. "],
["practica-guiada-4.html", "5.2 Práctica Guiada", " 5.2 Práctica Guiada 5.2.1 Introducción El objetivo de esta clase es comenzar a trabajar utilizando el formato RNotebook para realizar reportes compilados directamente en RStudio, de forma tal que nuestro trabajo pueda quedar documentado y ser fácilmente compartido con otras personas. Para esto utilizaremos un dataframe del paquete datos. Deberemos instalarlo en caso de no contar con el mismo, y luego cargarlo con la función library(). En particular, utilizaremos los datos de encuesta, que consiste en una muestra de variables categóricas de la Encuesta Social General de EE.UU. encuesta &lt;- datos::encuesta El dataframe cuenta con 21.483 observaciones y 9 variables. 5.2.2 Explorando los datos La muestra refiere a información obtenida entre 2000 y 2014. Se presentan datos sobre estado civil, raza, ingresos, partido político de pertenencia, religión, y cantidad de horas dedicadas a mirar televisión, para personas de entre 18 y 89 años. 5.2.2.1 Religión En primer lugar, nos interesa ver la distribución en términos de la religión de las personas, haciendo énfasis en aquellas más populares. ## # A tibble: 15 x 2 ## religion cantidad ## &lt;fct&gt; &lt;int&gt; ## 1 Protestante 10846 ## 2 Católica 5124 ## 3 Ninguna 3523 ## 4 Cristiana 689 ## 5 Judía 388 ## 6 Otra 224 ## 7 Budismo 147 ## 8 Inter o no confesional 109 ## 9 Musulmana/Islam 104 ## 10 Cristiana ortodoxa 95 ## 11 Sin respuesta 93 ## 12 Hinduismo 71 ## 13 Otra religión oriental 32 ## 14 Nativa americana 23 ## 15 No sabe 15 Puede verse que aquella que cuenta con más seguidores es la religión Protestante, con 10.846 fieles. 5.2.2.2 Estado Civil También podemos visualizar la distribución del estado civil de las personas. Vemos que la mayoría de las personas (10.117 en total) responde “Casado” cuando se indaga sobre su estado civil. 5.2.2.3 Partido político La encuesta también nos permite conocer sobre las pertenencias partidarias de los individuos. 5.2.2.4 Horas de exposición a la televisión A partir de los datos, sabemos que los individuos miran la televisión, en promedio, durante 3 horas por día. A continuación se presenta toda la distribución de la variable: "],
["shiny-apps.html", "Capítulo-6 Shiny apps", " Capítulo-6 Shiny apps En este módulo veremos la utilización de Shinyapss para elaborar reportes reactivos. Entre otras cosas, veremos: Shiny como reportes dinámicos Su utilidad para el análisis exploratorio Lógica de servidor- interfaz de usuario Inputs- Outputs, funciones reactivas, widgets. "],
["explicacion-5.html", "6.1 Explicación", " 6.1 Explicación 6.1.1 ¿Qué es un shiny app? Shiny es un paquete de R que facilita la creación de aplicaciones web interactivas directamente desde R. Permite a quienes no son versados en diseño web construir rápidamente una página reactiva para explorar la información. 6.1.2 Galería de ejemplos Veamos algunos ejemplos de la página: https://shiny.rstudio.com/gallery/ 6.1.3 Componentes fundamentales de un Shiny app Un Shiny App tiene dos componentes Interfaz de Usuario (UI): Contiene los widgets para recibir el input del usuario y mostrar los outputs Server: Recibe los inputs del UI y con ellos genera los outputs Un widget es un elemento web que le permite al usuario enviar un mensaje. 6.1.3.1 Ejemplo 1. Old Faithful Geyser Abrir el archivo ejemplo_1/app.R Comencemos con el ejemplo más básico. Cuando creamos un nuevo shiny, nos genera este ejemplo como template. Primero cargamos la librería. library(shiny) Luego definimos la interfaz de usuario. Elementos del ui: fluidPage: La función con que definimos el layout general titlePanel: Para definir el título sidebarLayout: Definimos que el diseño de la app va a ser con una barra lateral y un panel central sidebarPanel: Dentro del sidebarPanel definimos los elementos que van en la barra lateral sliderInput: Definimos que el input será ingresado desde un widget de tipo slider, y sus parámetros mainPanel: Dentro del mainPanel definimos los elementos que van en el panel central plotOutput: con esta función indicamos que el output es un gráfico ui &lt;- fluidPage( titlePanel(&quot;Old Faithful Geyser Data&quot;), sidebarLayout( sidebarPanel( sliderInput(inputId = &quot;bins&quot;, label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 30) ), mainPanel( plotOutput(&quot;distPlot&quot;) ) ) ) Elementos del server input: Es una lista de elementos que recibimos del ui. en este caso sólo contiene bins (el inputId) output: Es una lista que generamos dentro del server. En este caso definimos el elemento distPlot renderPlot: Es una función reactiva, que observa cada vez que cambia el input y vuelve a generar el output. Noten que lo que hace es envolver una porción del código entre llaves. server &lt;- function(input, output) { output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) }) } shinyApp(ui = ui, server = server) "],
["practica-guiada-5.html", "6.2 Práctica Guiada", " 6.2 Práctica Guiada 6.2.1 Ejemplo 2. Gapminder Construyamos nuestro propio ejemplo con los datos de Gapminder. Para eso, vamos a ver que la manera más cómoda de escribir una shiny app no es en el orden en que aparece el código final. Al código hay que comerlo de a pedacitos. Pensamos qué queremos mostrar Escribimos código estático para un caso particular. Pensamos qué partes queremos generalizar. Armamos una función que tome como parámetros aquello que generalizamos Armamos un shiny estático que nos muestre el resultado de la función con parámetros fijos Agregamos los inputs en el ui reemplazamos los parámetros fijos por los de input en el server Agregamos texto y otros elementos ‘cosméticos’ A cada paso vamos armando un código que no falle. De esta forma es más fácil detectar los errores. library(tidyverse) library(gapminder) gapminder &lt;- gapminder gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows 6.2.1.1 1. qué queremos mostrar tenemos tres variables que podrían ser agrupadoras: País, continente y año y tres variables que puede ser interesante representar: Esperanza de vida, población y PBI per cápita Podríamos mostrar por ejemplo la serie de tiempo de algún país para alguna variable 6.2.1.2 2. código estático para un caso particular. gapminder %&gt;% filter(country == &#39;Argentina&#39;) %&gt;% ggplot(aes(year, lifeExp))+ geom_line()+ geom_point() 6.2.1.3 3. partes que queremos generalizar. El gráfico podría ser para cualquier país (o para un conjunto de países!) podríamos elegir qué variable ver 6.2.1.4 4. función que tome como parámetros aquello que generalizamos graficar &lt;- function(pais, variable){ gapminder %&gt;% filter(country %in% pais) %&gt;% ## reemplace el == por %in% para que me reciba más de un país. ggplot(aes_string(&quot;year&quot;, variable, color= &quot;country&quot;))+ ## Le cambio aes por aes_string para que me reciba el texto del input geom_line()+ geom_point() } graficar(pais = &quot;Argentina&quot;, variable = &quot;lifeExp&quot;) graficar(pais = c(&quot;Argentina&quot;,&quot;Angola&quot;), variable = &quot;lifeExp&quot;) 6.2.1.5 5. shiny estático con parámetros fijos ver ejemplo_2_a 6.2.1.6 6. Agregamos los inputs en el ui Necesitamos agregar dos inputs: País y variable. Para opciones podemos usar selectize selectizeInput(inputId, label, choices, selected = NULL, multiple = FALSE, options = NULL) Podemos crear la lista de opciones de países automaticamente unique(gapminder$country)[1:10] ## [1] Afghanistan Albania Algeria Angola Argentina ## [6] Australia Austria Bahrain Bangladesh Belgium ## 142 Levels: Afghanistan Albania Algeria Angola Argentina ... Zimbabwe ver ejemplo_2_b 6.2.1.7 7. reemplazamos los parámetros fijos por los de input en el server ver ejemplo_2_c 6.2.1.8 8. Tuneamos a discreción Una vez que tenemos un shiny funcionando como queríamos, podemos agregar tags y texto para agregar explicaciones y emprolijar los resultados. # Headers # shiny::tags$h1(&#39;Nivel 1&#39;) # shiny::tags$h2(&#39;Nivel 2&#39;) # shiny::tags$h3(&#39;Nivel 3&#39;) # shiny::tags$h4(&#39;Nivel 4&#39;) # shiny::tags$h5(&#39;Nivel 5&#39;) # shiny::tags$h6(&#39;Nivel 6&#39;) shiny::br() # espacio en blanco shiny::hr() # linea horizontal shiny::helpText(&#39;texto para ayudas&#39;) texto para ayudas 6.2.1.9 Multiples pestañas También puede ocurrir que queremos mostrar varios resultados en un mismo shiny. En nuestro ejemplo, podríamos querer mostrar una tabla con los datos. Para eso podemos usar tabsetPanel en el ui Imaginemos que queremos tener dos tabs: Una con el gráfico, y otra con una tabla de resultados: Entonces, en el shiny debemos agregar: mainPanel( tabsetPanel(type = &quot;tabs&quot;, tabPanel(&quot;Gráfico&quot;, plotOutput(&quot;grafico&quot;)), tabPanel(&quot;Tabla&quot;, tableOutput(&quot;tabla&quot;)) ) ) Mientras que en el server debemos generar un nuevo resultado, llamado tabla con los datos output$tabla &lt;- renderTable({ gapminder %&gt;% filter(country %in% input$inputPais) }) ver ejemplo_2_d "],
["probabilidad-y-estadistica.html", "Capítulo-7 Probabilidad y Estadística", " Capítulo-7 Probabilidad y Estadística Introducción a probabilidad Introducción a distribuciones El problema de la inversión: Probabilidad vs. Estadística Población y muestra Estimadores puntuales, tests de hipótesis Boxplots, histogramas y kernels "],
["explicacion-6.html", "7.1 Explicación", " 7.1 Explicación 7.1.1 Probabilidad Previo a estudiar las herramientas de la estadística descriptiva, es necesario hacer un breve resumen de algunos conceptos fundamentales de probabilidad 7.1.1.1 Marco conceptual El análisis de las probabilidades parte de un proceso generador de datos entendido como cualquier fenómeno que produce algún tipo de información de forma sistemática. Cada iteración de este proceso produce información, que podemos interpretar como un resultado. Existe un conjunto de posibles resultados, que definimos como espacio muestral. Un evento es el conjunto de resultados ocurridos. En este marco, la probabilidad es un atributo de los eventos. Es la forma de medir los eventos tal que, siguiendo la definición moderna de probabilidad: \\(P(A) \\geq 0 \\ \\forall \\ A \\subseteq \\Omega\\) \\(P(\\Omega)=1\\) \\(P(A\\cup B) = P(A) + P(B)\\ si\\ A \\cap B = \\emptyset\\) ejemplo, tiramos un dado y sale tres Espacio muestral: 1,2,3,4,5,6 Resultado: 3 Evento: impar (el conjunto 1,3,5) 7.1.1.2 Distribución de probabilidad La distribución de probabilidad hace referencia a los posibles valores teóricos de cada uno de los resultados pertenecientes al espacio muestral. Existen dos tipos de distribuciones, dependiendo si el espacio muestral es o no numerable. 7.1.1.2.1 Distribuciones discretas Sigamos con el ejemplo de dado. Podríamos definir la distribución de probabilidad, si el dado no está cargado, como: valor probabilidad 1 1/6 2 1/6 3 1/6 4 1/6 5 1/6 6 1/6 Como el conjunto de resultados posibles es acotado, podemos definirlo en una tabla, esta es una distribución discreta. 7.1.1.2.2 Distribuciones continuas ¿Qué pasa cuando el conjunto de resultados posibles es tan grande que no se puede enumerar la probabilidad de cada caso? Si, por definición o por practicidad, no se puede enumerar cada caso, lo que tenemos es una distribución continua. Por ejemplo, la altura de la población En este caso, no podemos definir en una tabla la probabilidad de cada uno de los posibles valores. de hecho, la probabilidad puntual es 0. Sin embargo, sí podemos definir una función de probabilidad, la densidad. Según qué función utilicemos, cambiará la forma de la curva. Por ejemplo: Una distribución de probabilidad se caracteriza por sus parámetros. Por ejemplo, la distribución normal se caracteriza por su esperanza y su varianza (o desvío estándar) 7.1.2 Estadística 7.1.2.1 El problema de la inversión El problema de la probabilidad se podría pensar de la siguiente forma: Vamos a partir de un proceso generador de datos Para calcular su distribución de probabilidad, los parámetros que caracterizan a ésta, y a partir de allí, Calcular la probabilidad de que, al tomar una muestra, tenga ciertos eventos. El problema de la estadística es exactamente el contrario: Partimos de una muestra para Inferir cuál es la distribución de probabilidad, y los parámetros que la caracterizan Para finalmente poder sacar conclusiones sobre el proceso generador de datos 7.1.2.1.1 Población y muestra En este punto podemos hacer la distinción entre población y muestra Población: El universo en estudio. Puede ser: finita: Los votantes en una elección. infinita: El lanzamiento de una moneda. Muestra: subconjunto de n observaciones de una población. Solemos utilizar las mayúsculas (N) para la población y las minúsculas (n) para las muestras 7.1.2.1.2 Parámetros y Estimadores Como dijimos, los parámetros describen a la función de probabilidad. Por lo tanto hacen referencia a los atributos de la población. Podemos suponer que son constantes. Un estimador es un estadístico (esto es, una función de la muestra) usado para estimar un parámetro desconocido de la población. 7.1.2.1.3 Ejemplo. La media Esperanza o Media Poblacional: \\[ \\mu = E(x)= \\sum_{i=1}^N x_ip(x_i) \\] Media muestral: \\[ \\bar{X}= \\sum_{i=1}^n \\frac{Xi}{n} \\] Como no puedo conocer \\(\\mu\\), lo estimo mediante \\(\\bar{X}\\) 7.1.2.2 Estimación puntual, Intervalos de confianza y Tests de hipótesis El estimador \\(\\bar{X}\\) nos devuelve un número. Esto es una inferencia de cuál creemos que es la media. Pero no es seguro que esa sea realmente la media. Esto es lo que denominamos estimación puntual. También podemos estimar un intervalo, dentro del cual consideramos que se encuentra la media poblacional. La ventaja de esta metodología es que podemos definir la probabilidad de que el parámetro poblacional realmente esté dentro de este intervalo. Esto se conoce como intervalos de confianza. Por su parte, también podemos calcular la probabilidad de que el parámetro poblacional sea mayor, menor o igual a un cierto valor. Esto es lo que se conoce como test de hipótesis. En el fondo, los intervalos de confianza y los tests de hipótesis se construyen de igual manera. Son funciones que se construyen a partir de los datos, que se comparan con distribuciones conocidas, teóricas. 7.1.2.2.1 Definición de los tests Los tests se construyen con dos hipótesis: La hipótesis nula \\(H_0\\), y la hipótesis alternativa, \\(H_1\\). Lo que buscamos es ver si hay evidencia suficiente para rechazar la hipótesis nula. Por ejemplo, si queremos comprobar si la media poblacional, \\(\\mu\\) de una distribución es mayor a \\(X_i\\), haremos un test con las siguientes hipótesis: \\(H_0: \\mu = X_i\\) \\(H_1: \\mu &gt; X_i\\) Si la evidencia es lo suficientemente fuerte, podremos rechazar la hipótesis \\(H_0\\), pero no afirmar la hipótesis \\(H_1\\) 7.1.2.2.2 Significatividad en los tests Muchas veces decimos que algo es “estadísticamente significativo”. Detrás de esto se encuentra un test de hipótesis que indica que hay una suficiente significativdad estadística. La significatividad estadística, representada con \\(\\alpha\\), es la probabilidad de rechazar \\(H_0\\) cuando en realidad es cierta. Por eso, cuanto más bajo el valor de \\(\\alpha\\), más seguros estamos de no equivocarnos. Por lo general testeamos con valores de alpha de 1%, 5% y 10%, dependiendo del área de estudio. El p-valor es la mínima significatividad para la que rechazo el test. Es decir, cuanto más bajo es el p-valor, más seguros estamos de rechazar \\(H_0\\). El resultado de un test está determinado por: La fuerza de la evidencia empírica: Si nuestra duda es si la media poblacional es mayor a, digamos, 10, y la media muestral es 11, no es lo mismo que si es 100, 1000 o 10000. El tamaño de la muestra: En las fórmulas que definen los test siempre juega el tamaño de la muestra: cuanto más grande es, más seguros estamos de que el resultado no es producto del mero azar. La veracidad de los supuestos: Otra cosa importante es que los test asumen ciertas cosas: Normalidad en los datos. Que conocemos algún otro parámetro de la distribución, como la varianza. Que los datos son independientes entre sí, Etc. Cada Test tiene sus propios supuestos. Por eso a veces, luego de hacer un test, hay que hacer otros tests para validar que los supuestos se cumplen. Lo primero, la fuerza de la evidencia, es lo que más nos importa, y no hay mucho por hacer. El tamaño de la muestra es un problema, porque si la muestra es muy chica, entonces podemos no llegar a conclusiones significativas aunque sí ocurra aquello que queríamos probar. Sin embargo, el verdadero problema en La era del big data es que tenemos muestras demasiado grandes, por lo que cualquier test, por más mínima que sea la diferencia, puede dar significativo. Por ejemplo, podemos decir que la altura promedio en Argentina es 1,74. Pero si hacemos un test, utilizando como muestra 40 millones de personas, vamos a rechazar que ese es el valor, porque en realidad es 1,7401001. En términos de lo que nos puede interesar, 1,74 sería válido, pero estadísticamente rechazaríamos. Finalmente, según la información que tengamos de la población y cuál es el problema que queremos resolver, vamos a tener que utilizar distintos tipos de tests. La cantidad de tests posibles es ENORME, y escapa al contenido de este curso, así como sus fórmulas. A modo de ejemplo, les dejamos el siguiente machete: 7.1.3 Algunos estimadores importantes 7.1.3.1 Medidas de centralidad Media \\[ \\bar{X}= \\sum_{i=1}^n \\frac{Xi}{n} \\] Mediana: Es el valor que parte la distribución a la mitad Moda La moda es el valor más frecuente de la distribución 7.1.3.2 Cuantiles Así como dijimos que la mediana es el valor que deja al 50% de los datos de un lado y al 50% del otro, podemos generalizar este concepto a cualquier X%. Esto son los cuantiles. El cuantil x, es el valor tal que queda un x% de la distribución a izquierda, y 1-x a derecha. Algunos de los más utilizados son el del 25%, también conocido como \\(Q_1\\) (el cuartil 1), el \\(Q_2\\) (la mediana) y el \\(Q_3\\) (el cuartil 3), que deja el 75% de los datos a su derecha. Veamos cómo se ven en la distribución de arriba. 7.1.3.3 Desvío estándar El desvío estándar es una medida de dispersión de los datos, que indica cuánto se suelen alejar de la media. 7.1.4 Gráficos estadísticos Cerramos la explicación con algunos gráficos que resultan útiles para entender las propiedades estadísticas de los datos. 7.1.4.1 Boxplot El Boxplot es muy útil para describir una distribución y para detectar outliers. Reúne los principales valores que caracterizan a una distribución: \\(Q_1\\) \\(Q_2\\) (la mediana) \\(Q_3\\) el rango intercuarítlico \\(Q_3 - Q_1\\), que define el centro de la distribución Outliers, definidos como aquellos puntos que se encuentran a más de 1,5 veces el rango intercuartílico del centro de la distribución. Veamos qué pinta tienen los boxplot de números generados aleatoriamente a partir de tres distribuciones que ya vimos. En este caso, sólo tomaremos 15 valores de cada distribución Algunas cosas que resaltan: la distribución \\(\\chi^2\\) no toma valores en los negativos. La normal esta más concentrada en el centro de la distribución. Podemos generar 100 números aleatorios en lugar de 15: Cuando generamos 100 valores en lugar de 15, tenemos más chances de agarrar un punto alejado en la distribución. De esta forma podemos apreciar las diferencias entre la distribución normal y la T-student. También podemos volver a repasar qué efecto generan los distintos parámetros. Por ejemplo: 7.1.4.2 Histograma Otra forma de analizar una distribución es mediante los histogramas: En un histograma agrupamos las observaciones en rangos fijos de la variable y contamos la cantidad de ocurrencias. Cuanto más alta es una barra, es porque más observaciones se encuentran en dicho rango. Veamos el mismo ejemplo que arriba, pero con histogramas: 7.1.4.3 Kernel Los Kernels son simplemente un suavizado sobre los histogramas. 7.1.4.4 Violin plots Combinando la idea de Kernels y Boxplots, se crearon los violin plots, que simplemente muestran a los kernels duplicados. 7.1.5 Bibliografía de consulta Quién quiera profundizar en estos temas, puede ver los siguientes materiales: https://seeing-theory.brown.edu/ https://lagunita.stanford.edu/courses/course-v1:OLI+ProbStat+Open_Jan2017/about Jay L. Devore, “Probabilidad y Estadística para Ingeniería y Ciencias”, International Thomson Editores. https://inferencialitm.files.wordpress.com/2018/04/probabilidad-y-estadistica-para-ingenieria-y-ciencias-devore-7th.pdf "],
["practica-guiada-6.html", "7.2 Práctica Guiada", " 7.2 Práctica Guiada library(tidyverse) 7.2.1 Generación de datos aleatorios Para generar datos aleatorios, usamos las funciones: rnorm para generar datos que surgen de una distribución normal rt para generar datos que surgen de una distribución T-student rchisq para generar datos que surgen de una distribución Chi cuadrado runif para generar datos que surgen de una distribución uniforme &gt; Pero antes, tenemos que fijar la semilla para que los datos sean reproducibles set.seed(1234) rnorm(n = 15,mean = 0, sd = 1 ) ## [1] -1.20706575 0.27742924 1.08444118 -2.34569770 0.42912469 ## [6] 0.50605589 -0.57473996 -0.54663186 -0.56445200 -0.89003783 ## [11] -0.47719270 -0.99838644 -0.77625389 0.06445882 0.95949406 rt(n = 15,df=1 ) ## [1] -0.363717710 -1.603466805 -0.388596796 -0.588007490 0.007839245 ## [6] 14.690527710 -1.863488555 0.022667470 -2.084247299 -0.249237745 ## [11] -1.311594174 -3.569055208 -2.490838240 -3.848779244 -4.271087169 rchisq(n = 15,df=1) ## [1] 0.5317744 1.4263809 4.2797098 0.2184660 0.6923773 0.0455256 3.1902100 ## [8] 0.2949942 0.5403827 0.1543732 0.8639196 0.1417290 1.1386091 0.2966193 ## [15] 0.5110879 runif(15,0,1) ## [1] 0.75911999 0.42403021 0.56088725 0.11613577 0.30302180 0.47880269 ## [7] 0.34483055 0.60071414 0.07608332 0.95599261 0.02220682 0.84171063 ## [13] 0.63244245 0.31009417 0.74256937 hagamos un ggplot para visualizar la info tibble(normal = rnorm(n = 15,mean = 0, sd = 1 ), tstudent = rt(n = 15,df=1 ), chi = rchisq(n = 15,df=1), uniforme = runif(15,0,1)) %&gt;% gather(distribucion,valor) %&gt;% ggplot(aes(distribucion,valor,fill=distribucion))+ geom_violin()+ facet_wrap(.~distribucion,scales = &#39;free&#39;) Qué pasa si lo corremos varias veces? 7.2.2 Tests dist1 &lt;- rnorm(100, 10,sd = 1) dist2 &lt;- rnorm(100, 15, sd = 1) t.test(dist1,dist2, paired = F,var.equal = TRUE) ## ## Two Sample t-test ## ## data: dist1 and dist2 ## t = -33.391, df = 198, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.059234 -4.494982 ## sample estimates: ## mean of x mean of y ## 10.17864 14.95575 dist1 &lt;- rnorm(10, 10,sd = 1) dist2 &lt;- rnorm(10, 15, sd = 1) t.test(dist1,dist2, paired = F,var.equal = TRUE) ## ## Two Sample t-test ## ## data: dist1 and dist2 ## t = -8.9832, df = 18, p-value = 4.529e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.963134 -3.702580 ## sample estimates: ## mean of x mean of y ## 10.05722 14.89008 dist1 &lt;- rnorm(5, 10,sd = 1) dist2 &lt;- rnorm(5, 15, sd = 1) t.test(dist1,dist2, paired = F,var.equal = TRUE) ## ## Two Sample t-test ## ## data: dist1 and dist2 ## t = -6.8836, df = 8, p-value = 0.0001266 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.335167 -2.657580 ## sample estimates: ## mean of x mean of y ## 10.07898 14.07535 dist1 &lt;- rnorm(20, 10,sd = 2) dist2 &lt;- rnorm(20, 11, sd = 1) t.test(dist1,dist2, paired = F,var.equal = F) ## ## Welch Two Sample t-test ## ## data: dist1 and dist2 ## t = -3.618, df = 29.93, p-value = 0.00108 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.5848637 -0.7194778 ## sample estimates: ## mean of x mean of y ## 9.68726 11.33943 7.2.3 Descripción estadística de los datos Volvamos a ver los datos de sueldos de funcionarios sueldos &lt;- read_csv(&#39;fuentes/sueldo_funcionarios_2019.csv&#39;) Con el comando summary podemos ver algunos de los principales estadísticos de resumen summary(sueldos$asignacion_por_cargo_i) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 197746 210061 226866 225401 231168 249662 7.2.4 Gráficos estadísticos A modo de ejemplo, dejamos los comandos de R base para realizar gráficos. mamiferos &lt;- openintro::mammals ggplot(mamiferos, aes(y=LifeSpan))+ geom_boxplot() ggplot(mamiferos, aes(BodyWt, BrainWt,label=Species))+ geom_point() ggplot(mamiferos, aes(log(BodyWt), log(BrainWt),label=Species))+ geom_point() "],
["modelo-lineal.html", "Capítulo-8 Modelo Lineal", " Capítulo-8 Modelo Lineal Análisis de correlación. Presentación conceptual del modelo lineal El modelo lineal desde una perspectiva computacional Supuestos del modelo lineal Modelo lineal en R Modelo lineal en el tidyverse "],
["explicacion-7.html", "8.1 Explicación", " 8.1 Explicación En este módulo vamos a ver cómo analizar la relación entre dos variables. Primero, veremos los conceptos de covarianza y correlación, y luego avanzaremos hasta el modelo lineal. knitr::opts_chunk$set(warning = FALSE, message = FALSE) library(tidyverse) library(modelr) library(GGally) library(plot3D) 8.1.1 Covarianza y Correlación. La covarianza mide cómo varían de forma conjunta dos variables, en promedio. Se define como: \\[ \\text{cov}(x,y)=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y) \\] Esto es: La covarianza entre dos variables, \\(x\\) e \\(y\\) es el promedio (noten que hay una sumatoria y un dividido n) de las diferencias de los puntos a sus medias en \\(x\\) e \\(y\\). tratemos de entender el trabalenguas con la ayuda del siguiente gráfico: Aquí marcamos \\(\\bar x\\) y \\(\\bar y\\) y dividimos el gráfico en cuatro cuadrantes. En el primer cuadrante los puntos son más chicos a sus medias en \\(x\\) y en \\(y\\), \\((x-\\hat x)\\) es negativo y \\((y-\\hat y)\\) también. Por lo tanto, su producto es positivo. En el segundo cuadrante la diferencia es negativa en x, pero positiva en y. Por lo tanto el producto es negativo. En el tercer cuadrante la diferencia es negativa en y, pero positiva en x. Por lo tanto el producto es negativo. Finalmente, en el cuarto cuadrante las diferencias son positivas tanto en x como en y, y por lo tanto también el producto. Si la covarianza es positiva y grande, entonces valores chicos en una de las variables suceden en conjunto con valores chicos en la otra,y viceversa. Al contrario, si la covarianza es negativa y grande, entonces valores altos de una variable suceden en conjunto con valores pequeños de la otra y viceversa. La correlación se define como sigue: \\[\\rho_{x,y}=\\frac{cov(x,y)}{\\sigma_x \\sigma_y}\\] Es decir, normalizamos la covarianza por el desvío en \\(x\\) y en \\(y\\). de esta forma, la correlación se define entre -1 y 1. 8.1.1.1 ggpairs Para ver una implementación práctica de estos conceptos, vamos a utilizar la librería GGally para graficar la correlación por pares de variables. Con ggpairs(), podemos graficar todas las variables, y buscar las correlaciones. Coloreamos por: -\\(am\\): Tipo de transmisión: automático (am=0) o manual (am=1) mtcars %&gt;% select(-carb,-vs) %&gt;% mutate(cyl = factor(cyl), am = factor(am)) %&gt;% ggpairs(., title = &quot;Matriz de correlaciones&quot;, mapping = aes(colour= am)) Veamos la correlación entre: \\(mpg\\): Miles/(US) gallon. Eficiencia de combustible \\(hp\\): Gross horsepower: Potencia del motor cor(mtcars$mpg, mtcars$hp) ## [1] -0.7761684 nos da negativa y alta. Si quisiéramos testear la significatividad de este estimador, podemos realizar un test: \\(H_0\\) : ρ =0 \\(H_1\\) : ρ \\(\\neq\\) 0 cor.test(mtcars$mpg,mtcars$hp) ## ## Pearson&#39;s product-moment correlation ## ## data: mtcars$mpg and mtcars$hp ## t = -6.7424, df = 30, p-value = 1.788e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8852686 -0.5860994 ## sample estimates: ## cor ## -0.7761684 Con este p-value rechazamos \\(H_0\\) 8.1.2 Modelo Lineal sigamos utilizando los datos de sim1 ggplot(sim1, aes(x, y)) + geom_point() Se puede ver un patrón fuerte en los datos. Pareciera que el modelo lineal y = a_0 + a_1 * x podría servir. 8.1.2.1 Modelos al azar Para empezar, generemos aleatoriamente varios modelos lineales para ver qué pinta tienen. Para eso, podemos usar geom_abline () que toma una pendiente e intercepto como parámetros. models &lt;- tibble( a1 = runif(250, -20, 40), a2 = runif(250, -5, 5) ) ggplot(sim1, aes(x, y)) + geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) + geom_point() A simple vista podemos apreciar que algunos modelos son mejores que otros. Pero necesitamos una forma de cuantificar cuales son los mejores modelos. 8.1.2.2 distancias Una forma de definir mejor es pensar en aquel modelo que minimiza la distancia vertical con cada punto: Para eso, eligamos un modelo cualquiera: \\[ y= 7 + 1.5*x\\] (para que se vean mejor las distancias, corremos un poquito cada punto sobre el eje x) dist1 &lt;- sim1 %&gt;% mutate( dodge = rep(c(-1, 0, 1) / 20, 10), x1 = x + dodge, pred = 7 + x1 * 1.5 ) ggplot(dist1, aes(x1, y)) + geom_abline(intercept = 7, slope = 1.5, colour = &quot;grey40&quot;) + geom_point(colour = &quot;grey40&quot;) + geom_linerange(aes(ymin = y, ymax = pred), colour = &quot;#3366FF&quot;) La distancia de cada punto a la recta es la diferencia entre lo que predice nuestro modelo y el valor real Para computar la distancia, primero necesitamos una función que represente a nuestro modelo: Para eso, vamos a crear una función que reciba un vector con los parámetros del modelo, y el set de datos, y genere la predicción: model1 &lt;- function(a, data) { a[1] + data$x * a[2] } model1(c(7, 1.5), sim1) ## [1] 8.5 8.5 8.5 10.0 10.0 10.0 11.5 11.5 11.5 13.0 13.0 13.0 14.5 14.5 ## [15] 14.5 16.0 16.0 16.0 17.5 17.5 17.5 19.0 19.0 19.0 20.5 20.5 20.5 22.0 ## [29] 22.0 22.0 Ahora, necesitamos una forma de calcular los residuos y agruparlos. Esto lo vamos a hacer con el error cuadrático medio \\[ECM = \\sqrt\\frac{\\sum_i^n{(\\hat{y_i} - y_i)^2}}{n}\\] measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) sqrt(mean(diff ^ 2)) } measure_distance(c(7, 1.5), sim1) ## [1] 2.665212 8.1.2.3 Evaluando los modelos aleatorios Ahora podemos calcular el ECM para todos los modelos del dataframe models. Para eso utilizamos el paquete purrr, para ejecutar varias veces la misma función sobre varios elementos. Tenemos que pasar los valores de a1 y a2 (dos parámetros –&gt; map2), pero como nuestra función toma sólo uno (el vector a), nos armamos una función de ayuda para wrapear a1 y a2 sim1_dist &lt;- function(a1, a2) { measure_distance(c(a1, a2), sim1) } models &lt;- models %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) models ## # A tibble: 250 x 3 ## a1 a2 dist ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26.6 -3.45 17.8 ## 2 -2.24 -3.72 41.7 ## 3 -4.55 -3.32 41.4 ## 4 8.28 4.20 17.2 ## 5 -15.8 1.82 21.4 ## 6 -6.40 -4.88 52.7 ## 7 -13.1 1.37 21.3 ## 8 -15.2 3.29 13.3 ## 9 6.45 -0.426 13.6 ## 10 38.9 -0.238 23.2 ## # … with 240 more rows A continuación, superpongamos los 10 mejores modelos a los datos. Coloreamos los modelos por -dist: esta es una manera fácil de asegurarse de que los mejores modelos (es decir, los que tienen la menor distancia) obtengan los colores más brillantes. ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(models, rank(dist) &lt;= 10) ) También podemos pensar en estos modelos como observaciones y visualizar con un gráfico de dispersión de a1 vsa2, nuevamente coloreado por -dist. Ya no podemos ver directamente cómo se compara el modelo con los datos, pero podemos ver muchos modelos a la vez. Nuevamente, destacamos los 10 mejores modelos, esta vez dibujando círculos rojos debajo de ellos. ggplot(models, aes(a1, a2)) + geom_point(data = filter(models, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) 8.1.2.4 Grid search En lugar de probar muchos modelos aleatorios, podríamos ser más sistemáticos y generar una cuadrícula de puntos uniformemente espaciada (esto se denomina grid search). Elegimos los parámetros de la grilla aproximadamente mirando dónde estaban los mejores modelos en el gráfico anterior. grid &lt;- expand.grid( a1 = seq(-5, 20, length = 25), a2 = seq(1, 3, length = 25) ) %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) grid %&gt;% ggplot(aes(a1, a2)) + geom_point(data = filter(grid, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) Cuando superponemos los 10 mejores modelos en los datos originales, todos se ven bastante bien: ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(grid, rank(dist) &lt;= 10) ) 8.1.2.5 óptimo por métodos numéricos Podríamos imaginar este proceso iterativamente haciendo la cuadrícula más fina y más fina hasta que nos centramos en el mejor modelo. Pero hay una forma mejor de abordar ese problema: una herramienta de minimización numérica llamada búsqueda de Newton-Raphson. La intuición de Newton-Raphson es bastante simple: Se elige un punto de partida y se busca la pendiente más inclinada. Luego, desciende por esa pendiente un poco, y se repite una y otra vez, hasta que no se puede seguir bajando. En R, podemos hacer eso con optim (): necesitamos pasarle un vector de puntos iniciales. Elegimos 4 y 2, porque los mejores modelos andan cerca de esos valores le pasamos nuestra función de distancia, y los parámetros que nuestra función necesita (data) best &lt;- optim(c(4,2), measure_distance, data = sim1) best ## $par ## [1] 4.221029 2.051528 ## ## $value ## [1] 2.128181 ## ## $counts ## function gradient ## 49 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline(intercept = best$par[1], slope = best$par[2]) 8.1.2.6 Óptimo para el modelo lineal Este procedimiento es válido para muchas familias de modelos. Pero para el caso del modelo lineal, conocemos otras formas de resolverlo Si nuestro modelo es \\[ y = a_1 + a_2x + \\epsilon \\] La solución del óptima que surge de minimizar el Error Cuadrático Medio es: \\[ \\hat{a_1} = \\bar{y} - \\hat{a_2}\\bar{x} \\] \\[ \\hat{a_2} = \\frac{\\sum_i^n (y_i -\\bar{y})(x_i -\\bar{x})}{\\sum_i^n (x_i- \\bar{x})} \\] R tiene una función específica para el modelo lineal lm(). Cómo esta función sirve tanto para regresiones lineales simples como múltiples, debemos especificar el modelo en las formulas: y ~ x sim1_mod &lt;- lm(y ~ x, data = sim1) 8.1.2.7 Interpretando la salida de la regresión summary(sim1_mod) ## ## Call: ## lm(formula = y ~ x, data = sim1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1469 -1.5197 0.1331 1.4670 4.6516 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.2208 0.8688 4.858 4.09e-05 *** ## x 2.0515 0.1400 14.651 1.17e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.203 on 28 degrees of freedom ## Multiple R-squared: 0.8846, Adjusted R-squared: 0.8805 ## F-statistic: 214.7 on 1 and 28 DF, p-value: 1.173e-14 Analicemos los elementos de la salida: Residuals: La distribución de los residuos. Hablaremos más adelante. Coefficients: Los coeficientes del modelo. El intercepto y la variable explicativa Estimate: Es el valor estimado para cada parámetro Pr(&gt;|t|): Es el p-valor asociado al test que mide que el parámetro sea mayor que 0. Si el p-valor es cercano a 0, entonces el parámetro es significativamente mayor a 0. Multiple R-squared: El \\(R^2\\) indica que proporción del movimiento en \\(y\\) es explicado por \\(x\\). F-statistic: Es el resultado de un test de significatividad global del modelo. Con un p-valor bajo, rechazamos la hipótesis nula, que indica que el modelo no explicaría bien al fenómeno. interpretación de los parámetros: El valor estimado del parámetro se puede leer como “cuanto varía \\(y\\) cuando \\(x\\) varía en una unidad”. Es decir, es la pendiente de la recta 8.1.2.8 Análisis de los residuos Los residuos del modelo indican cuanto le erra el modelo en cada una de las observaciones. Es la distancia que intentamos minimizar de forma agregada. Podemos agregar los residuos al dataframe con add_residuals () de la librería modelr. sim1 &lt;- sim1 %&gt;% add_residuals(sim1_mod) sim1 %&gt;% sample_n(10) ## # A tibble: 10 x 3 ## x y resid ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 11.9 -0.534 ## 2 1 7.51 1.24 ## 3 3 10.5 0.136 ## 4 3 10.5 0.130 ## 5 6 16.0 -0.574 ## 6 6 16.9 0.365 ## 7 2 10.2 1.92 ## 8 8 18.4 -2.24 ## 9 9 22.8 0.120 ## 10 1 4.20 -2.07 Si cuando miramos los residuos notamos que tienen una estructura, eso significa que nuestro modelo no esta bien especificado. En otros términos, nos olvidamos de un elemento importante para explicar el fenómeno. Lo que debemos buscar es que los residuos estén homogéneamente distribuidos en torno al 0. Hay muchas maneras de analizar los residuos. Una es con las estadísticas de resumen que muestra el summary. Otra forma es graficándolos. ggplot(sim1, aes(x, resid)) + geom_ref_line(h = 0, size = 2,colour = &quot;firebrick&quot;) + geom_point() 8.1.3 Regresión lineal múltiple Si bien escapa a los alcances de esta clase ver en detalle el modelo lineal múltiple, podemos ver alguna intuición. Notemos que el modelo ya no es una linea en un plano, sino que ahora el modelo es un plano, en un espacio de 3 dimensiones: Para cada par de puntos en \\(x_1\\) y \\(x_2\\) vamos a definir un valor para \\(y\\) El criterio para elegir el mejor modelo va a seguir siendo minimizar las distancias verticales. Esto quiere decir, respecto de la variable que queremos predecir. interpretación de los parámetros: El valor estimado del parámetro se puede leer como “cuanto varía \\(y\\) cuando \\(x\\) varía en una unidad, cuando todo lo demás permanece constante”. Noten que ahora para interpretar los resultados tenemos que hacer la abstracción de dejar todas las demás variables constantes Adjusted R-squared: Es similar a \\(R^2\\), pero ajusta por la cantidad de variables del modelo (nosotros estamos utilizando un modelo de una sola variable), sirve para comparar entre modelos de distinta cantidad de variables. 8.1.4 Para profundizar Estas notas de clase estan fuertemente inspiradas en los siguientes libros/notas: R para Cienca de Datos Apuntes regresión lineal Un punto pendiente de estas clases que es muy importante son los supuestos que tiene detrás el modelo lineal. "],
["practica-guiada-7.html", "8.2 Práctica Guiada", " 8.2 Práctica Guiada library(tidyverse) 8.2.1 Datos de Properati Para este ejercicio utilizaremos los datos provistos por Properati: https://www.properati.com.ar/data/ Primero acondicionamos la base original, para quedarnos con una base más fácil de trabajar, y que contiene unicamente los datos interesantes. (no es necesario correrlo) ar_properties &lt;- read_csv(&quot;~/Downloads/ar_properties.csv&quot;) ar_properties %&gt;% filter(operation_type==&#39;Venta&#39;, property_type %in% c(&#39;Casa&#39;,&#39;PH&#39;,&#39;Departamento&#39;), currency==&#39;USD&#39;, l1==&#39;Argentina&#39;, l2==&#39;Capital Federal&#39;, !is.na(rooms), !is.na(surface_total), !is.na(surface_covered), !is.na(bathrooms), !is.na(l3)) %&gt;% select(-c(lat,lon, title,description, ad_type,start_date, end_date,operation_type,currency, l1, l2,l4,l5,l6,price_period,bedrooms)) %&gt;% saveRDS(&#39;fuentes/datos_properati.RDS&#39;) df &lt;- read_rds(&#39;fuentes/datos_properati.RDS&#39;) glimpse(df) ## Observations: 52,246 ## Variables: 9 ## $ id &lt;chr&gt; &quot;OgLe3YSDR0da+JUQZgmTtA==&quot;, &quot;Z3j1BtQN1kzuJr20com… ## $ created_on &lt;date&gt; 2019-05-09, 2019-05-09, 2019-05-09, 2019-05-09,… ## $ l3 &lt;chr&gt; &quot;Nuñez&quot;, &quot;Nuñez&quot;, &quot;Almagro&quot;, &quot;Belgrano&quot;, &quot;Flores… ## $ rooms &lt;dbl&gt; 3, 3, 3, 5, 5, 3, 3, 2, 5, 5, 4, 2, 3, 5, 3, 3, … ## $ bathrooms &lt;dbl&gt; 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 4, 2, 3, … ## $ surface_total &lt;dbl&gt; 77, 97, 69, 230, 168, 65, 95, 50, 181, 180, 89, … ## $ surface_covered &lt;dbl&gt; 68, 65, 69, 200, 168, 65, 92, 38, 110, 120, 118,… ## $ price &lt;dbl&gt; 180000, 265000, 230000, 380000, 255000, 119000, … ## $ property_type &lt;chr&gt; &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, &quot;PH&quot;, … summary(df$price) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6000 119000 170000 251944 272000 6000000 df[df$price&lt;10000,] ## # A tibble: 4 x 9 ## id created_on l3 rooms bathrooms surface_total surface_covered ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 uZe6… 2019-03-28 Pale… 5 4 340 320 ## 2 +JnI… 2019-04-01 Parq… 1 1 31 31 ## 3 MEQM… 2019-03-15 Puer… 3 3 275 220 ## 4 o6Qf… 2019-04-30 Reco… 3 2 340 200 ## # … with 2 more variables: price &lt;dbl&gt;, property_type &lt;chr&gt; df &lt;- df %&gt;% filter(price&gt;10000) Tenemos un par de outliers que no tienen mucho sentido. Es posible que el precio este mal cargado. df[df$price&gt;5000000,] ## # A tibble: 11 x 9 ## id created_on l3 rooms bathrooms surface_total surface_covered ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Z0NE… 2019-04-13 Reco… 6 3 600 600 ## 2 gRZz… 2019-01-25 Reco… 6 3 600 600 ## 3 sP/J… 2019-05-18 Reco… 8 5 677 568 ## 4 VVkm… 2019-04-05 Reco… 10 3 978 489 ## 5 h6gp… 2019-06-15 Reco… 6 3 600 600 ## 6 HWNt… 2019-06-19 San … 3 1 60 56 ## 7 e2Wf… 2019-01-28 Pale… 4 4 404 404 ## 8 OzkE… 2019-01-28 Pale… 4 4 404 404 ## 9 6DhC… 2019-02-01 Caba… 1 1 41 37 ## 10 Jz4a… 2019-03-01 Caba… 1 1 41 37 ## 11 1R9Q… 2019-01-17 Caba… 1 1 41 37 ## # … with 2 more variables: price &lt;dbl&gt;, property_type &lt;chr&gt; Los precios más alto tienen algunas cosas sorprendentes, pero sería arriesgado descartarlos por errores. lm_fit &lt;- lm(price~ l3+ rooms + bathrooms + surface_total + property_type,data = df) summary(lm_fit) ## ## Call: ## lm(formula = price ~ l3 + rooms + bathrooms + surface_total + ## property_type, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2152714 -72322 -4147 46114 5284489 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.599e+05 1.388e+04 -11.520 &lt; 2e-16 *** ## l3Agronomía 2.024e+04 2.605e+04 0.777 0.437102 ## l3Almagro -7.962e+03 1.295e+04 -0.615 0.538764 ## l3Balvanera -2.616e+04 1.360e+04 -1.924 0.054387 . ## l3Barracas 5.459e+02 1.591e+04 0.034 0.972629 ## l3Barrio Norte 6.416e+04 1.331e+04 4.821 1.43e-06 *** ## l3Belgrano 1.212e+05 1.290e+04 9.396 &lt; 2e-16 *** ## l3Boca -4.934e+04 2.020e+04 -2.443 0.014559 * ## l3Boedo -1.421e+04 1.553e+04 -0.915 0.360345 ## l3Caballito 6.359e+03 1.298e+04 0.490 0.624125 ## l3Catalinas -2.566e+04 1.059e+05 -0.242 0.808533 ## l3Centro / Microcentro -3.333e+04 1.969e+04 -1.693 0.090542 . ## l3Chacarita 2.843e+04 1.609e+04 1.768 0.077139 . ## l3Coghlan 5.894e+04 1.643e+04 3.587 0.000335 *** ## l3Colegiales 3.945e+04 1.455e+04 2.710 0.006724 ** ## l3Congreso -2.853e+04 1.610e+04 -1.773 0.076275 . ## l3Constitución -2.953e+04 1.767e+04 -1.671 0.094633 . ## l3Flores -2.403e+04 1.363e+04 -1.763 0.077967 . ## l3Floresta -1.220e+04 1.516e+04 -0.804 0.421184 ## l3Las Cañitas 1.193e+05 1.758e+04 6.785 1.17e-11 *** ## l3Liniers -2.029e+04 1.592e+04 -1.275 0.202348 ## l3Mataderos -3.332e+04 1.612e+04 -2.067 0.038736 * ## l3Monserrat -9.560e+03 1.570e+04 -0.609 0.542461 ## l3Monte Castro 1.875e+04 1.793e+04 1.046 0.295781 ## l3Nuñez 9.191e+04 1.373e+04 6.695 2.18e-11 *** ## l3Once -2.203e+04 1.598e+04 -1.379 0.168006 ## l3Palermo 1.276e+05 1.272e+04 10.033 &lt; 2e-16 *** ## l3Parque Avellaneda -1.666e+04 2.199e+04 -0.758 0.448651 ## l3Parque Centenario -3.832e+04 1.523e+04 -2.515 0.011903 * ## l3Parque Chacabuco -1.329e+03 1.569e+04 -0.085 0.932517 ## l3Parque Chas 2.209e+04 2.267e+04 0.975 0.329726 ## l3Parque Patricios -1.126e+04 1.768e+04 -0.637 0.524163 ## l3Paternal -2.778e+03 1.550e+04 -0.179 0.857733 ## l3Pompeya -6.158e+04 2.211e+04 -2.786 0.005340 ** ## l3Puerto Madero 5.295e+05 1.457e+04 36.353 &lt; 2e-16 *** ## l3Recoleta 1.294e+05 1.309e+04 9.883 &lt; 2e-16 *** ## l3Retiro 7.507e+04 1.571e+04 4.779 1.76e-06 *** ## l3Saavedra 3.674e+04 1.485e+04 2.473 0.013387 * ## l3San Cristobal -1.197e+04 1.479e+04 -0.809 0.418323 ## l3San Nicolás -4.616e+03 1.534e+04 -0.301 0.763503 ## l3San Telmo 1.763e+04 1.460e+04 1.208 0.227176 ## l3Tribunales -4.234e+04 2.555e+04 -1.657 0.097553 . ## l3Velez Sarsfield 1.664e+03 2.487e+04 0.067 0.946644 ## l3Versalles 4.516e+03 1.988e+04 0.227 0.820295 ## l3Villa Crespo 1.072e+04 1.303e+04 0.823 0.410681 ## l3Villa del Parque 2.440e+04 1.470e+04 1.660 0.096951 . ## l3Villa Devoto 3.089e+04 1.440e+04 2.146 0.031896 * ## l3Villa General Mitre -2.567e+04 2.024e+04 -1.268 0.204656 ## l3Villa Lugano -1.002e+05 1.749e+04 -5.729 1.02e-08 *** ## l3Villa Luro 7.208e+03 1.617e+04 0.446 0.655849 ## l3Villa Ortuzar 2.826e+04 2.042e+04 1.383 0.166525 ## l3Villa Pueyrredón 2.686e+04 1.590e+04 1.689 0.091191 . ## l3Villa Real 1.343e+04 2.592e+04 0.518 0.604258 ## l3Villa Riachuelo -5.135e+04 4.988e+04 -1.029 0.303274 ## l3Villa Santa Rita 6.264e+03 1.909e+04 0.328 0.742874 ## l3Villa Soldati -9.211e+04 3.636e+04 -2.534 0.011295 * ## l3Villa Urquiza 4.076e+04 1.333e+04 3.058 0.002230 ** ## rooms 5.199e+04 8.989e+02 57.839 &lt; 2e-16 *** ## bathrooms 1.419e+05 1.461e+03 97.128 &lt; 2e-16 *** ## surface_total 5.808e+00 1.141e+00 5.092 3.56e-07 *** ## property_typeDepartamento 4.855e+03 5.267e+03 0.922 0.356609 ## property_typePH -4.780e+04 5.691e+03 -8.399 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 210300 on 52180 degrees of freedom ## Multiple R-squared: 0.4872, Adjusted R-squared: 0.4866 ## F-statistic: 812.7 on 61 and 52180 DF, p-value: &lt; 2.2e-16 ¿ Qué pasó con las variables no numéricas? ¿Son significativos los estimadores? ¿cuales? ¿Cómo se leen los valores de los estimadores? Dado que muchos de los barrios no explican significativamente los cambios en los precios, no esta bueno conservarlos todos. A su vez, no sabemos respecto a qué barrio se compara. Una solución puede ser agrupar los barrios en tres categorías respecto a su efecto en el precio: Alto Medio Bajo En particular, podemos notar de esta primera regresión que algunos barrios tienen un efecto significativo en subir el valor de la propiedad, como Belgrano o Recoleta. Para construir la nueva variable, podemos ver el precio promedio del metro cuadrado por barrio df_barrios &lt;- df %&gt;% group_by(l3) %&gt;% summarise(precio_m2 = mean(price/surface_total)) ggplot(df_barrios,aes(precio_m2)) + geom_histogram() summary(df_barrios$precio_m2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 871.2 2031.8 2147.3 2346.0 2560.0 6068.5 Con este gráfico vemos que que hay muchos barrios con un precio promedio cercano a 2500 dólares el \\(m^2\\). Podemos dividr los tres grupos al rededor de los quartiles 1 y 3. &lt;2000 bajo 2000-2500 medio 2500 alto df_barrios &lt;- df_barrios %&gt;% mutate(barrio= case_when(precio_m2&lt;2000 ~ &#39;bajo&#39;, precio_m2&gt;2000 &amp; precio_m2&lt;2500 ~ &#39;medio&#39;, precio_m2&gt;2500 ~ &#39;alto&#39;)) df_barrios %&gt;% sample_n(10) ## # A tibble: 10 x 3 ## l3 precio_m2 barrio ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Belgrano 3421. alto ## 2 Coghlan 2780. alto ## 3 Villa Real 2065. medio ## 4 San Telmo 2398. medio ## 5 Villa Luro 2147. medio ## 6 Puerto Madero 6069. alto ## 7 Constitución 1758. bajo ## 8 Recoleta 3357. alto ## 9 Parque Avellaneda 1616. bajo ## 10 Las Cañitas 3724. alto Con esta nueva variable podemos modificar la tabla original. df &lt;- df %&gt;% left_join(df_barrios, by=&#39;l3&#39;) y volvemos a calcular el modelo lm_fit &lt;- lm(price~ barrio+ rooms + bathrooms + surface_total + property_type,data = df) summary(lm_fit) ## ## Call: ## lm(formula = price ~ barrio + rooms + bathrooms + surface_total + ## property_type, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2145645 -71277 -11187 42472 5307946 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.041e+05 6.349e+03 -16.396 &lt; 2e-16 *** ## barriobajo -1.097e+05 4.231e+03 -25.939 &lt; 2e-16 *** ## barriomedio -9.342e+04 2.150e+03 -43.445 &lt; 2e-16 *** ## rooms 4.808e+04 9.293e+02 51.732 &lt; 2e-16 *** ## bathrooms 1.602e+05 1.499e+03 106.867 &lt; 2e-16 *** ## surface_total 5.485e+00 1.198e+00 4.580 4.67e-06 *** ## property_typeDepartamento 2.370e+04 5.352e+03 4.428 9.51e-06 *** ## property_typePH -3.906e+04 5.920e+03 -6.598 4.22e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 220800 on 52234 degrees of freedom ## Multiple R-squared: 0.4338, Adjusted R-squared: 0.4338 ## F-statistic: 5718 on 7 and 52234 DF, p-value: &lt; 2.2e-16 Si queremos que compare contra ‘barrio medio’ podemos convertir la variable en factor y explicitar los niveles df &lt;- df %&gt;% mutate(barrio = factor(barrio, levels = c(&#39;medio&#39;, &#39;alto&#39;,&#39;bajo&#39;))) lm_fit &lt;- lm(price~ barrio+ rooms + bathrooms + surface_total + property_type,data = df) summary(lm_fit) ## ## Call: ## lm(formula = price ~ barrio + rooms + bathrooms + surface_total + ## property_type, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2145645 -71277 -11187 42472 5307946 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.975e+05 6.215e+03 -31.783 &lt; 2e-16 *** ## barrioalto 9.342e+04 2.150e+03 43.445 &lt; 2e-16 *** ## barriobajo -1.632e+04 4.321e+03 -3.777 0.000159 *** ## rooms 4.808e+04 9.293e+02 51.732 &lt; 2e-16 *** ## bathrooms 1.602e+05 1.499e+03 106.867 &lt; 2e-16 *** ## surface_total 5.485e+00 1.198e+00 4.580 4.67e-06 *** ## property_typeDepartamento 2.370e+04 5.352e+03 4.428 9.51e-06 *** ## property_typePH -3.906e+04 5.920e+03 -6.598 4.22e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 220800 on 52234 degrees of freedom ## Multiple R-squared: 0.4338, Adjusted R-squared: 0.4338 ## F-statistic: 5718 on 7 and 52234 DF, p-value: &lt; 2.2e-16 8.2.1.1 Feature engineering. Lo que hicimos arriba con los barrios se conoce como feature engineerin: Generamos una nueva variable a partir de las anteriores para mejorar nuestro modelo. ¿Qué otras modificaciones podemos hacer? Hay una que ya hicimos: En lugar de pensar en el precio total, podemos pensar en el precio por \\(m^2\\). De esta manera ya no tendría sentido agregar la variable surface_total lm_fit &lt;- lm(precio_m2 ~ barrio + rooms + bathrooms + property_type,data = df) summary(lm_fit) ## ## Call: ## lm(formula = precio_m2 ~ barrio + rooms + bathrooms + property_type, ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2071.97 -241.41 55.51 214.05 2993.52 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1935.419 13.106 147.670 &lt; 2e-16 *** ## barrioalto 892.491 4.535 196.790 &lt; 2e-16 *** ## barriobajo -461.046 9.112 -50.595 &lt; 2e-16 *** ## rooms -22.684 1.959 -11.579 &lt; 2e-16 *** ## bathrooms 133.009 3.161 42.084 &lt; 2e-16 *** ## property_typeDepartamento 227.481 11.285 20.158 &lt; 2e-16 *** ## property_typePH 99.545 12.485 7.973 1.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 465.7 on 52235 degrees of freedom ## Multiple R-squared: 0.552, Adjusted R-squared: 0.5519 ## F-statistic: 1.073e+04 on 6 and 52235 DF, p-value: &lt; 2.2e-16 que pasó con rooms? Al normalizar el precio por los metros, rooms pasa de tomar valores positivos a negativos. Eso significa que rooms estaba correlacionado con el tamaño, y por lo tanto cuantos más cuartos, mayor el valor. Al normalizar podemos ver que, dado un metraje, más cuartos reducen el precio: Preferimos ambientes más grandes tal vez? predecir Para predecir un nuevo caso, podemos construir un dataframe con las variables. Por ejemplo caso_nuevo &lt;- tibble(barrio=&#39;alto&#39;, rooms=3, bathrooms=2, property_type=&#39;Departamento&#39;, surface_total=78) predict(lm_fit,newdata = caso_nuevo) ## 1 ## 3253.356 Pero debemos recordar que este es el valor por metro cuadrado. Para obtener lo que realmente nos interesa, tenemos que hacer el camino inverso del feature engenieering: predict(lm_fit,caso_nuevo)*caso_nuevo$surface_total ## 1 ## 253761.8 8.2.1.2 Para seguir practicando Un problema de lo que vimos en esta práctica es que las salidas de summary(lm_fit) es una impresión en la consola. Es muy difícil seguir trabajando con esos resultados. Para resolver esto hay un par de librerías que incorporan el modelado lineal al flujo del tidyverse: Broom Modelr "],
["analisis-de-encuestas.html", "Capítulo-9 Análisis de encuestas", " Capítulo-9 Análisis de encuestas En esta clase veremos distintos aspectos a tener en cuenta a la hora de trabajar con encuestas, ya sea porque necesitemos diseñar e implementar una, o porque se requiera trabajar con los datos de una encuesta ya realizada y publicada. Introducción al diseño de encuestas Presentación de la Encuesta Permanente de Hogares Generación de estadísticos de resumen en muestras estratificadas. Utilización de los ponderadores "],
["explicacion-8.html", "9.1 Explicación", " 9.1 Explicación 9.1.1 Población y muestra Población: El universo en estudio ( N ). Sobre esos elementos se observan variables, que son características que cambian de individuo a individuo. La población se define en relación al problema de investigación a abordar. Puede ser una población finita: Los votantes en una elección. Asalariados de CABA. O bien infinita: El lanzamiento de una moneda El lanzamiento de un dado En otros casos el límite es difuso: Si la población cambia en el tiempo, ¿los votantes en los meses previos a una elección son finitos o infinitos? Muestra: Subconjunto de n observaciones de una población. Son los elementos de la población que podemos medir, para realizar inferencia respecto a la población 9.1.2 Tipos de muestra Probabilísticas: La selección aleatoria de la muestra nos garantiza independencia en las observaciones, lo que nos permite saber cuántas observaciones necesito para obtener un cierto margen de error. Muestreo aleatorio simple Todos los individuos tienen la misma probabilidad de ser elegidos. Muestreo aleatorio estratificado Los individuos se dividen en grupos o estratos. La muestra se elige escogiendo en cada estrato individuos elegidos por muestro aleatorio simple. No probabilísticas: No surge de un proceso de selección aleatoria. Los sujetos en una muestra no probabilística generalmente son seleccionados en función de su accesibilidad o a criterio personal del investigador. Los eventos no son independientes No se puede extrapolar de la muestra a la población Sirve para captar atributos de un subconjunto de la población 9.1.3 El cuestionario Cuestionario: Es el instrumento con el que vamos a recolectar los datos. Ahí se encuentran las preguntas y también algunas aclaraciones que puede venir bien tener a mano. Diversas cuestiones a tener en cuenta: El propósito del cuestionario, hay que tener bien claros los objetivos principales El período de referencia Completitud del universo de estudio (¿Toda la población se puede clasificar en las respuestas o queda alguien afuera?) El flujo del cuestionario, pensar si hay subpoblaciones a las que ciertas preguntas no aplican, dado lo que ya respondieron anteriormente Las instrucciones deben ser claras y completas, los términos importantes tienen que estar bien definidos para encuestador y/o para el encuestado Establecer la mejor secuencia de aspectos o temas El cuestionario no tiene que ser innecesariamente largo (Problema de no-respuesta) No conviene empezar con preguntas difíciles/muy directas/que puedan incomodar Las preguntas no deben sugerir/inducir hacia lo que se desea como respuesta Para cada pregunta, preguntémonos: ¿Es demasiado general? ¿Es demasiado detallada? ¿Las palabras utilizadas son sencillas? ¿La estructura es breve y clara? ¿Sería más práctico subdividir la pregunta en otras más específicas? (y a la inversa) ¿Se refiere a una cuestión sobre la cual todos los encuestados deberian tener información para responder? ¿Es posible contestar la pregunta sin cometer un error? O, ¿Qué grado de error se puede tener? ¿Es necesario o sería útil complementar la pregunta con algún ejemplo? ¿Y con alguna ilustración? ¿La pregunta puede llegar a incomodar? ¿Conviene que sea una pregunta abierta o con categorías? ¿Puede la pregunta ser interpretada por diferentes personas de diferente manera? (Ej: Jefatura de hogar) Probar el cuestionario!! 9.1.4 Sesgos Tenemos un problema de sesgo cuando hay un peso desproporcionado a favor o en contra de una cosa, persona o grupo en comparación con otra, generalmente de una manera que se considera injusta. Es la diferencia entre lo que estimamos y lo que queríamos estimar. ¿De dónde puede salir ese sesgo? De la Muestra: tomamos una muestra que pensábamos que representaba a la población que queríamos estudiar pero resulta que hay factores que no tuvimos en cuenta. Por ejemplo: Hacer una encuesta sobre condiciones de vida por twitter, hacer una encuesta sobre uso de tecnologías en la puerta de un Shopping, etc. De la recolección de datos. Si en vez de tener una muestra clara, le pedimos al encuestador que elija gente “aleatoriamente” en una esquina, puede haber sesgos debidos a sus prejuicios, la verguenza para hablar con cierto tipo de gente, etc. Si le pedimos a la policía que elija gente para pedir DNIs al azar puede haber sesgos (el famoso “portación de cara”). De la limpieza y análisis de los datos: Si a la hora de emprolijar los datos que obtuvimos, nos preocupamos más por aquellas observaciones que tienen cierta característica en particular, podemos sesgar los indicadores que calculemos luego. Por ejemplo: Si en una encuesta de ingresos reviso sólo los montos que me parecen muy bajos, pero no los que me parecen muy altos. Al respecto, podemos pensar en: La formas en que se pregunta: tanto en el cuestionario como en los encuestadores, que todo sea lo más homogéneo posible, así la calidad de los datos no dependen del encuestador que tocó en cada caso. Sensibilización/perfiles del encuestador Ej: Sexo en censo (no es por observación) 9.1.5 No respuesta En las encuestas suele ocurrir que sea imposible obtener una respuesta por parte de algunos encuestados para una, más de una, o todas las variables que se procuran captar. Se dice entonces que se está en presencia de “no respuestas”. La falta de valores puede provenir de múltiples factores como pueden ser no haber encontrado a la persona a encuestar, la mala voluntad/desgano/miedo del encuestado o su ignorancia respecto del tema, que algún encuestador no se esfuerce mucho por que cada pregunta obtenga una respuesta, el tipo de pregunta, la longitud del cuestionario, etc. No respuesta sesgada Ej: Ingresos en la EPH tienen no respuesta sesgada en ingresos altos Corrección de no respuesta Con imputación Con ponderadores 9.1.6 Ponderadores / Expansores Los llamados ponderadores o factores de expansión son valores que se utilizan a fines de pesar la información de las variables captadas a través de una muestra para generar estadísticas. Sirven para reproducir los valores poblacionales de dichas estadísticas. Podemos pensar al expansor (cuando es una cantidad) como aquel que indica a cuántas personas se está representando con una observación de la muestra, o al ponderador (cuando es un porcentaje o proporción) como la proporción de la población a la que se representa con ese caso. Estos ponderadores se obtienen como el inverso de la probabilidad de que la observación haya sido seleccionada. O sea, si yo tengo la probabilidad 1/10 de ser seleccionada en una muestra, seguramente represente a 10 personas con mis respuestas. En la sección de práctica guiada veremos cómo operar con ponderadores en R. 9.1.7 Ejemplos 9.1.7.1 Encuesta Permanente de Hogares Comentamos: Objetivos de la encuesta Cobertura Frecuencia y periodicidad Muestra de viviendas Esquema de rotación 3 cuestionarios vivienda hogar individuo Ponderadores “PONDERA” Ponderadores de corrección de no-respuesta de ingresos Para más detalles sobre la Encuesta Permanente de Hogares: https://pablotis.github.io/Presentaciones/clase1_eph.html 9.1.7.2 Encuesta Nacional de Gastos de los Hogares (ENGHO) Comentamos: Objetivos de la encuesta Cobertura: 45.000 hogares, ubicados en centros urbanos de 2000 habitantes y más de todo el país Frecuencia y periodicidad octubre de 2017 hasta diciembre de 2018 Muestra de viviendas 5 cuestionarios Cuestionario 1. Características de los hogares Cuestionario 2. Gastos diarios Cuestionario 3. Gastos varios Cuestionario 4. Gastos personales Cuestionario 5. Ingresos Ponderadores Para más detalles sobre la ENGHO: Presentación Bases de datos "],
["encuesta-uso-del-tiempo-2016-ciudad-de-buenos-aires.html", "9.2 Encuesta Uso del Tiempo 2016 Ciudad de Buenos Aires", " 9.2 Encuesta Uso del Tiempo 2016 Ciudad de Buenos Aires Comentamos: Trabajo para el Mercado (Act. productiva) Trabajo No Remunerado (Act. productiva) Actividades Personales Medición del Tiempo cuando se hacen varias cosas simultáneamente hasta 3 Actividades Simultáneas Para más detalles sobre la Encuesta Uso del Tiempo CABA 2016: Presentación Doc. Metodologico Bases de datos "],
["practica-guiada-8.html", "9.3 Práctica Guiada", " 9.3 Práctica Guiada En esta sección trabajaremos con las bases de la Encuesta Permanente de Hogares del INDEC, para ejemplificar la forma de trabajar con encuestas en general. Realizaremos tres ejercicios que luego serán replicados, en la sección de práctica independiente, pero con la Encuesta Anual de Hogares de CABA. 9.3.1 Encuesta Permanente de Hogares Recomendaciones para trabajar con esta encuesta: Tener a mano el diseño de registro Utilizar el paquete eph Si queremos estar seguros sobre la definición de una categoría, apelar a los documentos metodológicos (por ejemplo: el caso de los asalariados encubiertos) tirar frecuencias o plots de las variables antes de usarlas (-9, 0, NAs, etc…) Cargamos las librerías # install.packages(&quot;eph&quot;) library(eph) # para trabajar con EPH library(tidyverse) library(expss) # para trabajar con la etiqueta de los datos Cargamos las bases con una función del paquete eph, que no va a descargar en disco la información pero sí la va a cargar en el entorno de trabajo de R. ?eph::get_microdata individual &lt;- get_microdata(year = 2019, trimester = 1, type = &#39;individual&#39;) hogar &lt;- get_microdata(year = 2019, trimester = 1, type = &#39;hogar&#39;) Sobre las bases: Qué tienen en común (CODUSU, NRO_HOGAR) Para qué nos pueden servir esas variables en común La función organize_labels() permite etiquetar los datos Seleccionamos variables relevantes para el ejercicio: información contenida en cada una individual &lt;- organize_labels(individual, type = &#39;individual&#39;) %&gt;% select(CODUSU, NRO_HOGAR, AGLOMERADO, COMPONENTE, CH04, ESTADO, CAT_OCUP, P21, P47T, PONDERA, PONDII, PONDIIO) hogar &lt;- organize_labels(hogar, type = &#39;hogar&#39;) %&gt;% select(CODUSU, NRO_HOGAR, IV12_2, IV12_3) ## IV12_2: zona inundable ## IV12_3: villa de emergencia 9.3.1.1 Ejercicio 1 Cuántas personas viven en viviendas ubicadas en villas de emergencias y en zonas inundables? Notemos que la pregunta refiere a las personas, pero la información sobre la ubicación de la vivienda figura en la base de hogares. Join de bases individual y hogar (y al revés?) Las variables de filtro y agrupamiento El rol de los ponderadores Función para mostrar los resultados etiquetados ejercicio1 &lt;- left_join(individual, hogar, by = c(&quot;CODUSU&quot;, &quot;NRO_HOGAR&quot;)) %&gt;% filter(IV12_2 == 1 | IV12_3 == 1) %&gt;% group_by(IV12_2, IV12_3) %&gt;% summarise(Cantidad = sum(PONDERA)) %&gt;% expss::values2labels(.) ejercicio1 ## # A tibble: 3 x 3 ## # Groups: IV12_2 [2] ## IV12_2 IV12_3 Cantidad ## &lt;labelled&gt; &lt;labelled&gt; &lt;int&gt; ## 1 Si Si 173476 ## 2 Si No. 3311861 ## 3 No. Si 411544 9.3.1.2 Ejercicio 2 Calcular el ingreso de la ocupación principal promedio, para las/os ocupadas/os por aglomerados y sexo La variable de filtro Las variables de agrupamiento La media ponderada y un nuevo ponderador Cálculo de la brecha Función para presentar en formato porcentual ejercicio2 &lt;- individual %&gt;% filter(ESTADO == 1) %&gt;% group_by(AGLOMERADO, CH04) %&gt;% summarise(promedio_iop = weighted.mean(P21, PONDIIO)) %&gt;% # ponderador spread(., CH04, promedio_iop) %&gt;% expss::values2labels(.) %&gt;% mutate(Brecha = ((Varon - Mujer)/Varon)) %&gt;% arrange(-Brecha) %&gt;% mutate(Brecha = scales::percent(Brecha)) ejercicio2 ## # A tibble: 32 x 4 ## # Groups: AGLOMERADO [32] ## AGLOMERADO Varon Mujer Brecha ## &lt;labelled&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Bahia Blanca - Cerri 27015. 16331. 39.5% ## 2 Mar del Plata - Batan 26720. 16759. 37.3% ## 3 Cdro. Rivadavia - R.Tilly 35599. 22464. 36.9% ## 4 San Nicolas - Villa Constitucion 21593. 13896. 35.6% ## 5 Rio Gallegos 30725. 20323. 33.9% ## 6 Rio Cuarto 21263. 14452. 32.0% ## 7 Posadas 19034. 13175. 30.8% ## 8 Partidos del GBA 21590. 15174. 29.7% ## 9 Ciudad de Buenos Aires 32350. 22803. 29.5% ## 10 Gran Cordoba 19962. 14603. 26.8% ## # … with 22 more rows 9.3.1.3 Ejercicio 3 Calcular el ingreso total individual promedio, para quienes perciben ingresos, por aglomerados y sexo La variable de filtro: Ingreso total individual positivo Las variables de agrupamiento: El aglomerado y el sexo Un nuevo ponderador ejercicio3 &lt;- individual %&gt;% filter(P47T &gt; 0) %&gt;% group_by(AGLOMERADO, CH04) %&gt;% summarise(promedio_iti = weighted.mean(P47T, PONDII)) %&gt;% # ponderador spread(., CH04, promedio_iti) %&gt;% expss::values2labels(.) %&gt;% mutate(Brecha = ((Varon - Mujer)/Varon)) %&gt;% arrange(-Brecha) %&gt;% mutate(Brecha = scales::percent(Brecha)) ejercicio3 ## # A tibble: 32 x 4 ## # Groups: AGLOMERADO [32] ## AGLOMERADO Varon Mujer Brecha ## &lt;labelled&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Cdro. Rivadavia - R.Tilly 37999. 22549. 40.7% ## 2 Bahia Blanca - Cerri 30233. 19515. 35.5% ## 3 San Nicolas - Villa Constitucion 24605. 15944. 35.2% ## 4 Gran Rosario 25308. 16809. 33.6% ## 5 Posadas 22053. 14888. 32.5% ## 6 Ushuaia - Rio Grande 36878. 25410. 31.1% ## 7 Partidos del GBA 24179. 16846. 30.3% ## 8 Gran Resistencia 17314. 12178. 29.7% ## 9 Neuquen - Plottier 30285. 21577. 28.8% ## 10 Gran La Plata 25823. 18996. 26.4% ## # … with 22 more rows 9.3.2 Ejemplo de utilización de la encuesta La desigualdad de género se puede medir "],
["mineria-de-textos.html", "Capítulo-10 Minería de Textos", " Capítulo-10 Minería de Textos Introducción al análisis de textos Limpieza Preprocesamiento BoW Stopwords TF-IDF Wordcloud Escrapeo de Twitter "],
["explicacion-9.html", "10.1 Explicación", " 10.1 Explicación 10.1.1 Introducción Junto con las imágenes y los audios, los textos son una fuente de datos no estructurados que se multiplicó en los últimos años. Para poder hacer uso de la información que contienen es necesario procesar los documentos originales a un formato lo suficientemente estandarizado como para que pueda alimentar algún tipo de modelo En la clase de hoy veremos un repaso de algunas de las técnicas más usuales para normalizar la información de documentos. 10.1.2 Bag of Words Los documentos se pueden caracterizar por las palabras que contienen. Esto esconde el supuesto fuerte de independencia. No estamos considerando el orden Para este tipo de técnicas, una buena representación de la información es una bolsa de palabras. Un formato que indica la cantidad de veces que aparece una palabra en un documento. También se conocen como Matrices Documento-Término o Término-Documento, según la orientación Ejemplo: opiniones de trip advisor: library(tidyverse) library(tm) doc1=&#39;Lugar espectacular e inolvidable&#39; doc2=&#39;Precioso lugar, la comida era espectacular, de 10, precioso!&#39; texto &lt;- c(doc1,doc2) myCorpus = VCorpus(VectorSource(texto)) myDTM = DocumentTermMatrix(myCorpus, control = list(minWordLength = 1)) m = as.matrix(myDTM) m ## Terms ## Docs 10, comida era espectacular espectacular, inolvidable lugar lugar, ## 1 0 0 0 1 0 1 1 0 ## 2 1 1 1 0 1 0 0 1 ## Terms ## Docs precioso precioso! ## 1 0 0 ## 2 1 1 Nosotros sabemos que el significado de “lugar” y “lugar,” es el mismo Al no estar normalizada la información, la BoW genera matrices muy grandes y esparsas, que son poco útiles para trabajar 10.1.3 Normalización Para construir el Bag of Words se debe considerar los siguientes procesos: Tokenization: Es el proceso de partir un string de texto en palabras y signos de puntuación. Eliminar puntuación. Stop Words: remover las palabras más comunes del idioma (“el”, “la”, “los”, “de”) ya que aparecen en todos los documentos y no aportan información valiosa para distinguirlos. Lemmatization: Es la representación de todas las formas flexionadas (plural, femenino, conjugado, etc.). Para esto, es necesario contar con una base de datos léxica. Para esto podemos usar koRpus que incluye el lexicón TreeTagger. Stemming: Es similar a la lematización, pero no se basa en las estructuras lexicales, sino que realiza una aproximación, quedándose con las primeras letras de la palabra. N-gramas: A veces los conceptos que permiten distinguir entre documentos se componen de más de una palabra, por ejemplo: “a duras penas” (trigrama), “Buenos Aires” (bigrama) Las expresiones idiomáticas o los nombres propios cambian radicalmente de sentido si se separan sus componentes. Imaginense si quisiéramos clasificar la posición política de izquierda a derecha de los “Nacional Socialistas”! Ejemplo: Limpiando el texto: doc1=&#39;Lugar espectacular e inolvidable&#39; doc2=&#39;Precioso lugar, la comida era espectacular, de 10, precioso!&#39; texto &lt;- c(doc1,doc2) myCorpus = VCorpus(VectorSource(texto)) myCorpus = tm_map(myCorpus, content_transformer(tolower)) myCorpus = tm_map(myCorpus, removePunctuation) myCorpus = tm_map(myCorpus, removeNumbers) myCorpus = tm_map(myCorpus, removeWords, stopwords(kind = &quot;es&quot;)) myDTM = DocumentTermMatrix(myCorpus, control = list(minWordLength = 1)) m = as.matrix(myDTM) m ## Terms ## Docs comida espectacular inolvidable lugar precioso ## 1 0 1 1 1 0 ## 2 1 1 0 1 2 10.1.4 Expresiones regulares. Un elemento fundamental para la manipulación del texto son las expresiones regulares. Éstas sirven para captar patrones que aparecen en el texto y luego operar sobre ellos (extraerlos, reemplazarlos, detectarlos, etc.) por ejemplo un_texto &lt;- &#39;una concatenación de caracteres&#39; str_detect(un_texto, &#39;una&#39;) ## [1] TRUE Para generar una expresión regular, utilizamos distintos elementos: 10.1.4.1 Caracteres especiales. Son formas de referirnos a tipos de caracteres por ejemplo str_detect(un_texto, &#39;[[:punct:]]&#39;) ## [1] FALSE str_detect(un_texto, &#39;[[:alnum:]]&#39;) ## [1] TRUE Los cuantificadores nos permiten decír “Este caracter, X veces” por ejemplo nchar(un_texto) ## [1] 31 str_detect(un_texto, &#39;(\\\\w|\\\\W){29,33}&#39;) ## [1] TRUE str_detect(un_texto, &#39;(\\\\w|\\\\W){32,35}&#39;) ## [1] FALSE Esto se lee como “caracteres de palabras (a,b,c…z ; A,B,C… Z ; 0,1,2..9) ó otros caracteres distintos, entre 29 y 33 veces”. A veces para extraer pedazos de texto nos conviene chequear que hay antes y después. Eso lo hacemos con los lookarounds Por ejemplo, si queremos recuerar el DNI de la persona entre muchas otras palabras: texto_con_dni &lt;- &#39;Lorem ipsum dolor sit amet, DNI 38765239,faucibus et dui tellus, eros mi elit...&#39; str_extract(texto_con_dni, pattern = &#39;(?&lt;=DNI )\\\\d{3,}&#39;) ## [1] &quot;38765239&quot; el patrón se lee “Luego de DNI tres o más dígitos” 10.1.5 Distancia de palabras La distancia de palabras se puede entender desde distintos lugares: Distancia de caracteres: Refiere a la similitud de escritura “Mueve” vs “Nueve” Distancia conceptual: Refiere a la similitud del concepto: “Perro” vs “Labrador” 10.1.5.1 Distancia de caracteres Distancia de Levenshtein o distancia de edición es el número mínimo de operaciones requeridas para transformar una cadena de caracteres en otra. Una operación puede ser una inserción, eliminación o sustitución de un carácter. Jaro Winkler: Esta medida de similitud da mejores puntajes a los strings que son similares en el principio de la oración. \\(0 &lt; sim_{jw}&lt;1\\), donde 1 significa que las palabras son idénticas (excepto que p=0.25 y compartan los primeros 4 caracteres). y 0 significa que no se parecen en nada 10.1.5.2 Distancia Conceptual Word Embeddings: Son una representación vectorial de las palabras que se construye a partir de observar una gran cantidad de documentos. Word2Vec fue la primera implementación de esta idea. Se entrena una red neuronal para predecir el contexto de una palabra, y luego se utiliza una matriz que se construye dentro de la red como representación de las palabras. ejemplo: Proyección en tres dimensiones 10.1.6 Distancia de Documentos 10.1.6.1 Similitud Coseno En el modelo de BoW representamos a todos los documentos como vectores n-dimensionales que toman valores en el espacio de los números enteros. La dimensión n del espacio está determinada por lo largo del vocabulario utilizado en el corpus. Para comparar la similitud entre dos documentos, podemos utilizar la similitud coseno entre sus representaciones vectoriales. Intuitivamente, la similitud coseno es una medida de correlación de vectores que representan atributos en lugar de variables que se mueven en un espacio continuo. recordemos primero algunas definiciones. El producto interno entre dos vectores x,y se define como: \\[ \\langle x,y \\rangle=\\sum_i x_i y_i = \\|x\\|\\ \\|y\\|\\cos(\\theta) \\] Por su parte, la norma 2 de un vector x se define como: \\[ ||x||=\\sqrt{\\sum_i x^2_i} \\] \\[ CosSim(x,y) = \\frac{\\langle x,y \\rangle}{\\|x\\|\\ \\|y\\|} = \\frac{\\sum_i x_iy_i}{\\sum_i x^2_i\\sum_i y^2_i} \\] Cuantas más palabras compartan los documentos, mayor es el producto punto. Este a su vez se normaliza por el tamaño de cada documento. Este valor va de 1 para los documentos son identicos a 0 cuando son totalmente distintos. 10.1.7 Topic Modelling Las técincas de Modelado de Tópicos tratan de captar los temas de los que habla un corpus de texto. Una de las técnicas más difundidas en la actualidad es Latent Dirichlet Allocation Models Éste es un modelo inferencial bayesiano. No vamos a poder estdiar el detalle del modelo en este curso, pero a grandes rasgos propone un proceso generativo donde cada palabra es el resultado de un encadenamiento de distribuciones, y luego se realiza inferencia hacia atrás para calcular la distribución más probable dada las palabras y los documentos El resultado del modelo es: Una distribución de palabras por tópico: Podemos caracterizar cada tópico por sus palabras más importantes. Una distribución de los tópicos por documento: Podemos caracterizar un documento por sus temas más importantes # install.packages(&quot;rtweet&quot;) library(rtweet) library(tidyverse) library(tm) library(wordcloud2) library(topicmodels) library(LDAvis) library(tsne) 10.1.8 Descargas de tweets con rtweet rt &lt;- search_tweets(q = &quot;metrovias OR bondi OR Subte OR autopista OR transporte público&quot;,type = &quot;mixed&quot;, n = 18000, include_rts = FALSE, lang=&#39;es&#39;) saveRDS(rt,&#39;fuentes/rt.RDS&#39;) rt &lt;- read_rds(&#39;fuentes/rt.RDS&#39;) rt %&gt;% sample_n(10) ## # A tibble: 10 x 90 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 225288… 11741366… 2019-09-18 01:42:55 edelamadrid Un s… Twitt… ## 2 141353… 11744968… 2019-09-19 01:34:13 microprosa &quot;@ar… Twitt… ## 3 672362… 11732417… 2019-09-15 14:26:53 djriveraros @mag… Twitt… ## 4 150072… 11745045… 2019-09-19 02:04:39 ElNacional… #ENC… Twitt… ## 5 943586… 11751139… 2019-09-20 18:26:05 Kukt8 @gav… Twitt… ## 6 1638691 11743557… 2019-09-18 16:13:12 Fotomaf He u… Twitt… ## 7 467564… 11750803… 2019-09-20 16:12:41 fercastill… @Sam… Twitt… ## 8 339036… 11740717… 2019-09-17 21:24:45 rutaarauca… 🇨🇱 🐎… Hoots… ## 9 267300… 11725036… 2019-09-13 13:33:55 shisusR &quot;Yo … Twitt… ## 10 104960… 11725291… 2019-09-13 15:15:09 amosbcn Cola… Twitt… ## # … with 84 more variables: display_text_width &lt;dbl&gt;, ## # reply_to_status_id &lt;chr&gt;, reply_to_user_id &lt;chr&gt;, ## # reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, is_retweet &lt;lgl&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, quote_count &lt;int&gt;, ## # reply_count &lt;int&gt;, hashtags &lt;list&gt;, symbols &lt;list&gt;, urls_url &lt;list&gt;, ## # urls_t.co &lt;list&gt;, urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;, ## # media_t.co &lt;list&gt;, media_expanded_url &lt;list&gt;, media_type &lt;list&gt;, ## # ext_media_url &lt;list&gt;, ext_media_t.co &lt;list&gt;, ## # ext_media_expanded_url &lt;list&gt;, ext_media_type &lt;chr&gt;, ## # mentions_user_id &lt;list&gt;, mentions_screen_name &lt;list&gt;, lang &lt;chr&gt;, ## # quoted_status_id &lt;chr&gt;, quoted_text &lt;chr&gt;, quoted_created_at &lt;dttm&gt;, ## # quoted_source &lt;chr&gt;, quoted_favorite_count &lt;int&gt;, ## # quoted_retweet_count &lt;int&gt;, quoted_user_id &lt;chr&gt;, ## # quoted_screen_name &lt;chr&gt;, quoted_name &lt;chr&gt;, ## # quoted_followers_count &lt;int&gt;, quoted_friends_count &lt;int&gt;, ## # quoted_statuses_count &lt;int&gt;, quoted_location &lt;chr&gt;, ## # quoted_description &lt;chr&gt;, quoted_verified &lt;lgl&gt;, ## # retweet_status_id &lt;chr&gt;, retweet_text &lt;chr&gt;, ## # retweet_created_at &lt;dttm&gt;, retweet_source &lt;chr&gt;, ## # retweet_favorite_count &lt;int&gt;, retweet_retweet_count &lt;int&gt;, ## # retweet_user_id &lt;chr&gt;, retweet_screen_name &lt;chr&gt;, retweet_name &lt;chr&gt;, ## # retweet_followers_count &lt;int&gt;, retweet_friends_count &lt;int&gt;, ## # retweet_statuses_count &lt;int&gt;, retweet_location &lt;chr&gt;, ## # retweet_description &lt;chr&gt;, retweet_verified &lt;lgl&gt;, place_url &lt;chr&gt;, ## # place_name &lt;chr&gt;, place_full_name &lt;chr&gt;, place_type &lt;chr&gt;, ## # country &lt;chr&gt;, country_code &lt;chr&gt;, geo_coords &lt;list&gt;, ## # coords_coords &lt;list&gt;, bbox_coords &lt;list&gt;, status_url &lt;chr&gt;, ## # name &lt;chr&gt;, location &lt;chr&gt;, description &lt;chr&gt;, url &lt;chr&gt;, ## # protected &lt;lgl&gt;, followers_count &lt;int&gt;, friends_count &lt;int&gt;, ## # listed_count &lt;int&gt;, statuses_count &lt;int&gt;, favourites_count &lt;int&gt;, ## # account_created_at &lt;dttm&gt;, verified &lt;lgl&gt;, profile_url &lt;chr&gt;, ## # profile_expanded_url &lt;chr&gt;, account_lang &lt;lgl&gt;, ## # profile_banner_url &lt;chr&gt;, profile_background_url &lt;chr&gt;, ## # profile_image_url &lt;chr&gt; range(rt$created_at) ## [1] &quot;2019-09-13 03:04:47 UTC&quot; &quot;2019-09-21 18:24:56 UTC&quot; Nos da los tweets de los últimos nueve días, o el máximo que indicamos más reciente rt %&gt;% ts_plot(&quot;3 hours&quot;) + ggplot2::theme_minimal() + ggplot2::theme(plot.title = ggplot2::element_text(face = &quot;bold&quot;)) + ggplot2::labs( x = NULL, y = NULL, title = &quot;Frecuencia de los tweets relacionados al tránsito&quot;, subtitle = &quot;Agregado a intervalos de tres horas&quot;) Me quedo con el texto texto &lt;- rt$text texto[1:10] ## [1] &quot;▶ Dos sujetos robaron a usuarios de un vehículo de transporte público https://t.co/3c0fSjKrWF https://t.co/uGYN8CyLuZ&quot; ## [2] &quot;Detienen a dos menores por asalto a transporte público https://t.co/R2VNKPLV0v https://t.co/MOVow5LUiE&quot; ## [3] &quot;Detienen a dos menores por asalto a transporte público\\n\\nhttps://t.co/6Ff1iewafX https://t.co/eRfyw8RLZy&quot; ## [4] &quot;Hombres armados incendian transporte público en #Acapulco https://t.co/WPPQqGckq9 https://t.co/LRzjM0mvPO&quot; ## [5] &quot;Hombres armados incendian transporte público en #Acapulco https://t.co/FbmZNWJPKN https://t.co/VSNemKS0S5&quot; ## [6] &quot;Según la Veeduría Distrital, la capital necesita con urgencia reducir las emisiones de CO2, producidos principalmente por el transporte público y los vehículos de carga pesada. https://t.co/oSJYsyMRu5&quot; ## [7] &quot;El #CMin aprueba un RD para mejorar la #accesibilidad al transporte de las personas con #discapacidad.\\n\\n♿️Acceso de las sillas de ruedas eléctricas y escúteres a los medios de transporte.\\n🐕Afectados por epilepsia y diabetes podrán viajar en transporte público con su perro. https://t.co/MW5BTsZzgB&quot; ## [8] &quot;Ósea el asunto no es solo así por así, hay que tener en cuenta otras medidas como el Transporte, la eficiencia del transporte público etc. \\nDejen de andar pidiendo desniveles como dulces porque aparentemente ni en Peru esa es la solución. \\nCierro hilo.&quot; ## [9] &quot;@cronicaglobal TODOS queremos un transporte Público sostenible, pero lo queremos eficiente...Esperas mas de 6 minutos en hora punta no es eficiencia...&quot; ## [10] &quot;Me caga cuando alguna persona se le queda viendo a mi cartera cuando la saco para pagar el transporte público. \\nQue esperan ver? Un chingo de billetes? …......\\nNo ven que estoy igual de jodido que ellos y por eso voy en la combi también.&quot; Este texto es necesario limpiarlo para que sea más fácil de utilizar. 10.1.9 Armado del corpus con tm Primero creo un objeto de tipo Corpus. Utilizamos algo distinto a los conocidos vectores of dataframes porque es un objeto optimizado para trabajar con texto. Esto nos permite que los procesos sean mucho más eficientes, y por lo tanto trabajar con grandes corpus de manera rápida myCorpus = Corpus(VectorSource(texto)) 10.1.10 Limpieza del Corpus Con la función tm_map podemos iterar sobre el corpus aplicando una transformación sobre cada documento (se acuerdan de la librería PURRR?) En este caso, para la limpieza utilizaremos las siguientes transformaciones. Pasar todo a minúscula (cómo la función que usamos no es de la librería tm tenemos que usar también content_transformer ) Sacar la puntuación Sacar los números Sacar las stopwords myCorpus = tm_map(myCorpus, content_transformer(tolower)) myCorpus = tm_map(myCorpus, removePunctuation) myCorpus = tm_map(myCorpus, removeNumbers) myCorpus = tm_map(myCorpus, removeWords, stopwords(kind = &quot;es&quot;)) También deberíamos sacar las palabras que utilizamos para descargar la información. # metrovias OR bondi OR Subte OR autopista OR transporte público myCorpus = tm_map(myCorpus, removeWords, c(&#39;metrovias&#39;, &#39;bondi&#39;,&#39;subte&#39;,&#39;autopista&#39;,&#39;transporte&#39;, &#39;público&#39; )) inspect(myCorpus[1:10]) ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 10 ## ## [1] ▶ dos sujetos robaron usuarios vehículo httpstcocfsjkrwf httpstcougyncyluz ## [2] detienen dos menores asalto httpstcorvnkplvv httpstcomovowluie ## [3] detienen dos menores asalto \\n\\nhttpstcoffiewafx httpstcoerfywrlzy ## [4] hombres armados incendian acapulco httpstcowppqqgckq httpstcolrzjmmvpo ## [5] hombres armados incendian acapulco httpstcofbmznwjpkn httpstcovsnemkss ## [6] según veeduría distrital capital necesita urgencia reducir emisiones co producidos principalmente vehículos carga pesada httpstcoosjysymru ## [7] cmin aprueba rd mejorar accesibilidad personas discapacidad\\n\\n♿️acceso sillas ruedas eléctricas escúteres medios \\n🐕afectados epilepsia diabetes podrán viajar perro httpstcomwbtszzgb ## [8] ósea asunto solo así así tener cuenta medidas eficiencia etc \\ndejen andar pidiendo desniveles dulces aparentemente peru solución \\ncierro hilo ## [9] cronicaglobal queremos sostenible queremos eficienteesperas mas minutos hora punta eficiencia ## [10] caga alguna persona queda viendo cartera saco pagar \\n esperan ver chingo billetes …\\n ven igual jodido voy combi podemos ver que nos quedaron unos que son la forma de representar el “enter”. Lo mejor sería eliminarlos. También queremos sacar los links. Para eso vamos a usar expresiones regulares para definir el patron que tiene un link, y luego crearemos una función que los elimine. 10.1.10.1 Expresiones regulares Para que sea más sencilla la construcción de la expresión regular, usamos la librería RVerbalExpressions # devtools::install_github(&quot;VerbalExpressions/RVerbalExpressions&quot;) library(RVerbalExpressions) expresion &lt;- rx() %&gt;% rx_find(&#39;http&#39;) %&gt;% rx_maybe(&#39;s&#39;) %&gt;% # rx_maybe(&#39;://&#39;) %&gt;% #como ya lo pasamos por los otros filtros, ya no hay puntuacion rx_anything_but(value = &#39; &#39;) expresion ## [1] &quot;(http)(s)?([^ ]*)&quot; Probamos la expresion con un ejemplo txt &lt;- &quot;detienen dos menores asalto transporte público\\n\\nhttpstcoffiewafx httpstcoerfywrlzy&quot; str_remove_all(txt, pattern = expresion) ## [1] &quot;detienen dos menores asalto transporte público\\n\\n &quot; Lo pasamos por el corpus myCorpus = tm_map(myCorpus, content_transformer(function(x) str_remove_all(x, pattern = expresion))) myCorpus = tm_map(myCorpus, content_transformer(function(x) str_remove_all(x, pattern = &#39;\\n&#39;))) inspect(myCorpus[1:10]) ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 10 ## ## [1] ▶ dos sujetos robaron usuarios vehículo ## [2] detienen dos menores asalto ## [3] detienen dos menores asalto ## [4] hombres armados incendian acapulco ## [5] hombres armados incendian acapulco ## [6] según veeduría distrital capital necesita urgencia reducir emisiones co producidos principalmente vehículos carga pesada ## [7] cmin aprueba rd mejorar accesibilidad personas discapacidad♿️acceso sillas ruedas eléctricas escúteres medios 🐕afectados epilepsia diabetes podrán viajar perro ## [8] ósea asunto solo así así tener cuenta medidas eficiencia etc dejen andar pidiendo desniveles dulces aparentemente peru solución cierro hilo ## [9] cronicaglobal queremos sostenible queremos eficienteesperas mas minutos hora punta eficiencia ## [10] caga alguna persona queda viendo cartera saco pagar esperan ver chingo billetes … ven igual jodido voy combi Creamos una matriz de Término-documento myDTM = DocumentTermMatrix(myCorpus, control = list(minWordLength = 1)) inspect(myDTM) ## &lt;&lt;DocumentTermMatrix (documents: 17697, terms: 43484)&gt;&gt; ## Non-/sparse entries: 227647/769308701 ## Sparsity : 100% ## Maximal term length: 137 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs así ciudad gente metro movilidad publico ser servicio sistema uso ## 10446 0 1 0 0 0 1 0 0 0 0 ## 11678 0 0 0 0 0 0 0 0 0 0 ## 11689 0 0 0 0 0 0 0 0 0 0 ## 12916 0 0 0 0 0 0 0 0 0 0 ## 13169 0 0 1 0 0 0 0 0 0 0 ## 13736 0 0 0 1 0 0 0 0 0 0 ## 16210 0 0 0 0 0 1 0 0 0 0 ## 2359 0 0 0 0 0 0 0 0 0 0 ## 3539 0 0 0 0 0 0 0 0 0 0 ## 5963 0 0 0 0 2 0 0 0 0 0 palabras_frecuentes &lt;- findMostFreqTerms(myDTM,n = 25, INDEX = rep(1,nDocs(myDTM)))[[1]] palabras_frecuentes ## publico gente ciudad metro servicio ser movilidad ## 1425 1081 1026 934 890 737 692 ## así sistema uso hoy mejor día solo ## 583 572 572 560 557 553 533 ## hacer personas mejorar menos hace usar bien ## 532 512 498 495 491 485 461 ## puede vez calidad tener ## 447 446 443 441 palabras_frecuentes &lt;- tibble(word = names(palabras_frecuentes), freq =palabras_frecuentes) wordcloud2(palabras_frecuentes, shuffle = FALSE) 10.1.11 Topic Modeling necesito eliminar los documentos vacíos (que luego de la limpieza quedaron sin ningúna palabra) ui = unique(myDTM$i) dtm = myDTM[ui,] dim(myDTM) ## [1] 17697 43484 dim(dtm) ## [1] 17677 43484 lda_fit &lt;- LDA(dtm, k = 10,method = &quot;Gibbs&quot;, control = list(delta=0.6,seed = 1234)) lda_fit saveRDS(lda_fit,&#39;resultados/lda_fit.rds&#39;) lda_fit &lt;- readRDS(&#39;resultados/lda_fit.rds&#39;) Terms &lt;- terms(lda_fit, 10) Terms ## Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 ## [1,] &quot;mejorar&quot; &quot;movilidad&quot; &quot;servicio&quot; &quot;ciudad&quot; &quot;día&quot; ## [2,] &quot;hace&quot; &quot;uso&quot; &quot;ser&quot; &quot;pasaje&quot; &quot;cada&quot; ## [3,] &quot;puede&quot; &quot;coche&quot; &quot;madrid&quot; &quot;bogotá&quot; &quot;hoy&quot; ## [4,] &quot;medio&quot; &quot;vehículos&quot; &quot;mejor&quot; &quot;sabe&quot; &quot;vez&quot; ## [5,] &quot;centro&quot; &quot;semana&quot; &quot;personas&quot; &quot;claudialopez&quot; &quot;hora&quot; ## [6,] &quot;años&quot; &quot;bici&quot; &quot;debe&quot; &quot;parís&quot; &quot;trabajo&quot; ## [7,] &quot;tránsito&quot; &quot;bicicleta&quot; &quot;buen&quot; &quot;tan&quot; &quot;menos&quot; ## [8,] &quot;algún&quot; &quot;ciudad&quot; &quot;vas&quot; &quot;huelga&quot; &quot;bien&quot; ## [9,] &quot;utilizar&quot; &quot;privado&quot; &quot;debería&quot; &quot;reforma&quot; &quot;horas&quot; ## [10,] &quot;evitar&quot; &quot;millones&quot; &quot;defensa&quot; &quot;precio&quot; &quot;voy&quot; ## Topic 6 Topic 7 Topic 8 Topic 9 Topic 10 ## [1,] &quot;gente&quot; &quot;pasajeros&quot; &quot;publico&quot; &quot;sistema&quot; &quot;usar&quot; ## [2,] &quot;metro&quot; &quot;vía&quot; &quot;seguridad&quot; &quot;calidad&quot; &quot;solo&quot; ## [3,] &quot;enciudad&quot; &quot;unidades&quot; &quot;casa&quot; &quot;hacer&quot; &quot;mejor&quot; ## [4,] &quot;luisdejesus&quot; &quot;usuarios&quot; &quot;auto&quot; &quot;problema&quot; &quot;van&quot; ## [5,] &quot;viajar&quot; &quot;aumento&quot; &quot;atención&quot; &quot;gobierno&quot; &quot;mal&quot; ## [6,] &quot;ahora&quot; &quot;tarifa&quot; &quot;llegar&quot; &quot;tener&quot; &quot;así&quot; ## [7,] &quot;caminar&quot; &quot;sep&quot; &quot;espacio&quot; &quot;parte&quot; &quot;buses&quot; ## [8,] &quot;país&quot; &quot;unidad&quot; &quot;luego&quot; &quot;claudiashein&quot; &quot;vida&quot; ## [9,] &quot;crisis&quot; &quot;falta&quot; &quot;seguro&quot; &quot;cdmx&quot; &quot;menos&quot; ## [10,] &quot;caracas&quot; &quot;dos&quot; &quot;cuenta&quot; &quot;autos&quot; &quot;bien&quot; topicmodels_json_ldavis &lt;- function(fitted, dtm){ svd_tsne &lt;- function(x) tsne(svd(x)$u) # Find required quantities phi &lt;- as.matrix(posterior(fitted)$terms) theta &lt;- as.matrix(posterior(fitted)$topics) vocab &lt;- colnames(phi) term_freq &lt;- slam::col_sums(dtm) # Convert to json json_lda &lt;- LDAvis::createJSON(phi = phi, theta = theta, vocab = vocab, mds.method = svd_tsne, plot.opts = list(xlab=&quot;tsne&quot;, ylab=&quot;&quot;), doc.length = as.vector(table(dtm$i)), term.frequency = term_freq) return(json_lda) } json_res &lt;- topicmodels_json_ldavis(lda_fit, dtm) #en consola # serVis(json_res) "],
["mapas.html", "Capítulo-11 Mapas", " Capítulo-11 Mapas Introducción a la utilización de información geográfica en R Elaboración de mapas "],
["explicacion-10.html", "11.1 Explicación", " 11.1 Explicación Hay varios paquetes en R con funciones que permiten manipular información espacial con facilidad. El objetivo de esta clase es introducir algunos de ellos a fin de combinarlos con algunas de las herramientas que ya vimos, para hacer análisis geográfico y crear nuestros propios mapas. Las presentes notas están basadas en: sf vignettes libro Ciencia de datos para gente sociable, de Antonio Vazquez Brust 11.1.1 Datos georreferenciados Sería ideal contar con un cuerpo que consista en una representación de la tierra en escala a la hora de trabajar con datos geográficos. Sin embargo, esto no sería práctico, por lo que normalmente trabajamos con representaciones en 2 dimensiones, como son los mapas. Para construir un mapa, primero necesitamos un modelo abstracto en 3 dimensiones del planeta: esfera elipsoide geoide (una forma teórica de la Tierra determinada por la geodesia en la cual se toma como superficie teórica el nivel medio de los mares) Fuente: GPS for Land Surveyors 11.1.2 Proyecciones y distorsiones Mercator: preserva ángulos Sinusoidal: preserva áreas, pero distorsiona formas y direcciones Equidistante cilíndrica: preserva distancias entre meridianos 11.1.3 Tipos de datos espaciales Vectoriales: puntos, líneas, polígonos Rasters: pixels, grillas gsp.humboldt.edu Palabras clave para consultas sobre datos espaciales: Distancia Largo Area Centroide Igual Disjunto Intersecta Toca Cruza Superpone Contiene Buffering 11.1.4 Paquete sf El paquete sf permite combinar datos de tipo vectorial y atributos asociados a dicho espacio. La implementación de los puntos, líneas y polígonos en este paquete se extienden para incorporar multipuntos, multilíneas, multipolígonos, etc. Los datos geográficos siempre van a estar en el objeto llamado geometry. bounding box: EPSG (SRID): el sistema de coordenadas Sistemas de coordenadas de referencia y proyecciones cartográficas: Sistema de números que definen ubicaciones sobre la superficie de la Tierra; funcionan como direcciones. El tipo más conocido es el que usa latitud y longitud. Existen muchísimas proyecciones distintas, cada una pensada para minimizar alguno de los tipos de distorsión. 11.1.5 Tipos de archivo shapefiles: guarda la información en varios archivos distintos, que suelen ser combinados en un archivo .zip. Los nombres de las variables en un shapefile deben tener 10 caracteres o menos. Fue inventado por la empresa ESRI (los creadores del software ArcGIS), y a pesar de las incomodidades mencionadas es muy común. GeoJSON: Es una alternativa más moderna; un estándar abierto que corrige los dos inconvenientes mencionados antes. Para nuestros ejercicios usaremos datos geográficos en este último formato. dataframes con columnas lat-long: Los datos también pueden ser presentados en un dataframe común, con una variable referida a la latitud y otra a la longitud. Podemos utilizar la función st_as_sf para realizar una conversión a tipo sf, y st_set_crs para indicar el sistema de coordenadas de referencia. Por ejemplo: base &lt;- st_as_sf(base, coords = c(&#39;long&#39;,&#39;lat&#39;))%&gt;% st_set_crs(4326) "],
["practica-guiada-9.html", "11.2 Práctica Guiada", " 11.2 Práctica Guiada Cargamos las librerías. En esta ocasión usaremos por primera vez el paquete sf. library(tidyverse) library(ggthemes) library(sf) 11.2.1 Representación de la información en mapas Trabajaremos con datos del portal de datos abiertos de la Ciudad de Buenos Aires. En este caso, usamos los radios censales de la ciudad y los hemos descargado de https://bitsandbricks.github.io/data/CABA_rc.geojson. La función st_read nos permite levantar un archivo de tipo .geojson. (también permite cargar otros con capas, pero no lo veremos en esta ocasión). Vemos que el mismo cuenta con 3.554 features y 8 campos. epsg (SRID): 4326 y proj4string: +proj=longlat +datum=WGS84 +no_defs refieren a que nuestros datos usan el sistema de coordenadas WGS84, también conocido por su código EPSG 4326 . Es el mismo que usan los sistemas GPS, Google Maps, y las aplicaciones de internet en general. radios &lt;- st_read(&quot;fuentes/CABA_rc.geojson&quot;) ## Reading layer `CABA_rc&#39; from data source `/home/diego/Documents/0_GIT/2_Ed/intro_ds_bookdown/fuentes/CABA_rc.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 3554 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -58.53092 ymin: -34.70574 xmax: -58.33455 ymax: -34.528 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs Como los 8 campos son equivalentes a columnas/variables, podemos pedir un summary de la información contenida en radios. summary(radios) ## RADIO_ID BARRIO COMUNA POBLACION ## 1_1_1 : 1 PALERMO : 295 1 : 329 Min. : 0.0 ## 1_10_1 : 1 CABALLITO: 215 13 : 305 1st Qu.: 646.2 ## 1_10_10: 1 RECOLETA : 198 14 : 295 Median : 786.0 ## 1_10_11: 1 BALVANERA: 191 3 : 254 Mean : 813.2 ## 1_10_12: 1 FLORES : 183 4 : 252 3rd Qu.: 928.0 ## 1_10_13: 1 BELGRANO : 170 7 : 250 Max. :3945.0 ## (Other):3548 (Other) :2302 (Other):1869 ## VIVIENDAS HOGARES HOGARES_NBI AREA_KM2 ## Min. : 0.0 Min. : 0.0 Min. : 0.00 Min. :0.004468 ## 1st Qu.: 311.2 1st Qu.: 259.0 1st Qu.: 2.00 1st Qu.:0.018626 ## Median : 377.0 Median : 310.0 Median : 6.00 Median :0.035548 ## Mean : 401.4 Mean : 323.6 Mean : 19.35 Mean :0.057350 ## 3rd Qu.: 462.0 3rd Qu.: 371.0 3rd Qu.: 23.00 3rd Qu.:0.062847 ## Max. :1405.0 Max. :1093.0 Max. :403.00 Max. :3.804422 ## ## geometry ## MULTIPOLYGON :3554 ## epsg:4326 : 0 ## +proj=long...: 0 ## ## ## ## A la hora de graficar, podemos representar la información utilizando la combinación del famoso ggplot() en conjunto con geom_sf, que identifica la variable con la geometría y la grafica. ggplot() + geom_sf(data = radios) Podemos colorear los radios censales de acuerdo a la cantidad de viviendas que hay en cada una de ellas, convocando a la variable en el parámetro de relleno. A su vez, podemos “jugar” con el color y ancho de los bordes. ggplot(data = radios, aes(fill = VIVIENDAS)) + geom_sf() ggplot(data = radios, aes(fill = VIVIENDAS)) + geom_sf(color = &quot;white&quot;) ggplot(data = radios, aes(fill = VIVIENDAS)) + geom_sf(color = &quot;white&quot;, size=.1) O bien podemos omitir los bordes. ggplot(data = radios, aes(fill = POBLACION)) + geom_sf(color = NA) También podemos usar el campo del relleno para realizar una transformación de los datos. Por ejemplo, podemos calcular la densidad de la población utilizando los datos de POBLACION y AREA_KM2. Aprovechamos para mejorar otras cuestiones estéticas 🌈 ggplot(data = radios, aes(fill = POBLACION/AREA_KM2)) + geom_sf(color = NA) + scale_fill_viridis_c() + labs(title = &quot;Densidad de población&quot;, subtitle = &quot;Ciudad Autónoma de Buenos Aires&quot;, fill = &quot;hab/km2&quot;) + theme_void() También podemos rellenar de acuerdo a la variable BARRIO, que es una forma de graficar agregados. De todas formas, luego veremos como agruparlos con group_by. ggplot(data = radios, aes(fill = BARRIO)) + geom_sf(color = NA) + theme(legend.position = &#39;none&#39;) También podemos representar rápidamente la información de alguna de las variables realizando un select() de la misma y la geometría, y pidiendo un plot() (el parámetro lwd define el ancho de las líneas de cada geometría, en este caso, los radios censales). En términos visuales, es como haber agregado por BARRIO la información, no? radios %&gt;% select(BARRIO, geometry) %&gt;% plot(lwd=0.06) 11.2.1.1 Agrupando polígonos Sin embargo, si lo que queremos es reconstruir los polígonos de los barrios, sólo necesitamos hacer un group_by y un summarise, y automáticamente en la columna geometry se crea el polígono combinado. Además, podemos hacer una agregación de las demás variables. glimpse(radios) ## Observations: 3,554 ## Variables: 9 ## $ RADIO_ID &lt;fct&gt; 1_1_1, 1_12_1, 1_12_10, 1_12_11, 1_12_2, 1_12_3, 1_1… ## $ BARRIO &lt;fct&gt; RETIRO, SAN NICOLAS, SAN NICOLAS, SAN NICOLAS, SAN N… ## $ COMUNA &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ POBLACION &lt;dbl&gt; 336, 341, 296, 528, 229, 723, 393, 600, 472, 786, 32… ## $ VIVIENDAS &lt;dbl&gt; 82, 365, 629, 375, 445, 744, 341, 505, 504, 546, 275… ## $ HOGARES &lt;dbl&gt; 65, 116, 101, 136, 129, 314, 209, 275, 202, 347, 129… ## $ HOGARES_NBI &lt;dbl&gt; 19, 25, 1, 7, 16, 104, 110, 32, 49, 89, 15, 57, 1, 1… ## $ AREA_KM2 &lt;dbl&gt; 1.79899705, 0.01856469, 0.04438025, 0.36634000, 0.01… ## $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-58.37189 -..., MULTIPO… barrios_geo &lt;- radios %&gt;% group_by(BARRIO) %&gt;% summarise(POBLACION = sum(POBLACION), VIVIENDAS = sum(VIVIENDAS), HOGARES = sum(HOGARES), HOGARES_NBI = sum(HOGARES_NBI), AREA_KM2 = sum(AREA_KM2)) glimpse(barrios_geo) ## Observations: 48 ## Variables: 7 ## $ BARRIO &lt;fct&gt; AGRONOMIA, ALMAGRO, BALVANERA, BARRACAS, BELGRANO, B… ## $ POBLACION &lt;dbl&gt; 13912, 131699, 138926, 89452, 126267, 45113, 47306, … ## $ VIVIENDAS &lt;dbl&gt; 6262, 71216, 77981, 33058, 71363, 18133, 21687, 9348… ## $ HOGARES &lt;dbl&gt; 5284, 58327, 60387, 31249, 54666, 16287, 18519, 7518… ## $ HOGARES_NBI &lt;dbl&gt; 70, 3404, 7122, 3850, 542, 3460, 1248, 1656, 785, 10… ## $ AREA_KM2 &lt;dbl&gt; 2.122096, 4.050120, 4.342357, 7.956272, 7.729648, 5.… ## $ geometry &lt;GEOMETRY [°]&gt; POLYGON ((-58.48244 -34.598..., POLYGON ((-… Si pedimos un plot() del objeto, nos devuelve la representación geográfica con los datos de cada una de las variables. plot(barrios_geo) Si deseamos agregar otra capa al gráfico que incluya texto o etiquetas de los datos, contamos con geom_sf_text(), en la cual podemos también solicitar que se etiqueten sólo aquellos datos que cumplen con alguna condición. ggplot(data = barrios_geo, aes(fill = POBLACION/AREA_KM2)) + geom_sf(color = NA) + geom_sf_text(data = barrios_geo %&gt;% filter(POBLACION/AREA_KM2&gt;25000), aes(label = BARRIO), size=3)+ theme(legend.position = &#39;bottom&#39;)+ scale_fill_viridis_c(option = &#39;C&#39;) + labs(title = &quot;Densidad de población&quot;, subtitle = &quot;Ciudad Autónoma de Buenos Aires&quot;, fill = &quot;hab/km2&quot;)+ theme_void() 11.2.1.2 Volcando en el mapa información de múltiples fuentes: Subtes Ahora utilizaremos la información de líneas y estaciones de subte. Hemos descargado los .geojson de http://bitsandbricks.github.io/data/subte_lineas.geojson y http://bitsandbricks.github.io/data/subte_estaciones.geojson. subte_lineas &lt;- st_read(&#39;fuentes/subte_lineas.geojson&#39;) ## Reading layer `subte_lineas&#39; from data source `/home/diego/Documents/0_GIT/2_Ed/intro_ds_bookdown/fuentes/subte_lineas.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 80 features and 2 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: -58.48639 ymin: -34.64331 xmax: -58.36993 ymax: -34.55564 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs glimpse(subte_lineas) ## Observations: 80 ## Variables: 3 ## $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, … ## $ LINEASUB &lt;fct&gt; LINEA D, LINEA D, LINEA D, LINEA D, LINEA D, LINEA D, L… ## $ geometry &lt;MULTILINESTRING [°]&gt; MULTILINESTRING ((-58.45213..., MULTILI… subte_estaciones &lt;- st_read(&#39;fuentes/subte_estaciones.geojson&#39;) ## Reading layer `subte_estaciones&#39; from data source `/home/diego/Documents/0_GIT/2_Ed/intro_ds_bookdown/fuentes/subte_estaciones.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 86 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -58.48639 ymin: -34.64331 xmax: -58.36993 ymax: -34.55564 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs glimpse(subte_estaciones) ## Observations: 86 ## Variables: 4 ## $ ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, … ## $ ESTACION &lt;fct&gt; CASEROS, INCLAN, HUMBERTO 1�, VENEZUELA, ONCE - 30 DE D… ## $ LINEA &lt;fct&gt; H, H, H, H, H, D, D, D, D, D, D, D, C, C, C, C, C, C, C… ## $ geometry &lt;POINT [°]&gt; POINT (-58.39893 -34.63575), POINT (-58.40097 -34… Con geom_sf agregamos diferentes capas de información geográfica: los barrios: coloreados según su proporción de hogares con necesidades básicas insatisfechas, sin color de borde. las líneas de subte: líneas en amarillo las estaciones de subte: puntos en naranja Alguna reflexión sobre los resultados? ggplot() + geom_sf(data = barrios_geo, aes(fill = HOGARES_NBI/HOGARES), color = NA) + geom_sf(data = subte_lineas, color = &quot;yellow&quot;) + geom_sf(data = subte_estaciones, color = &quot;orange&quot;) + theme(legend.position = &#39;bottom&#39;)+ scale_fill_viridis_c()+ labs(title = &quot;Sistema de transporte subterráneo (SUBTE)&quot;, subtitle = &quot;Ciudad de Buenos Aires&quot;) 11.2.2 Mapeo de Palos borrachos rosados en Buenos Aires Ahora utilizaremos información del portal de datos abiertos de la Ciudad de Buenos Aires sobre arbolado. Nos hemos quedado con una selección de dicha base de datos, conservando sólo a los árboles de tipo “palo borracho rosado”. Notemos que en este caso la información está en formato .rds, es un dataframe donde los ejemplares están georreferenciados con variables long-lat, no hay una geometría como en los casos anteriores. palos_borrachos &lt;- read_rds(&#39;fuentes/arbolado_palo_borracho.rds&#39;) palos_borrachos %&gt;% sample_n(10) ## # A tibble: 10 x 20 ## long lat tipo_sitio id_arbol altura_tot diametro inclinacio ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -58.5 -34.6 Árbol 2.90e7 8 38 0 ## 2 -58.4 -34.7 Árbol 3.40e7 8 34 0 ## 3 -58.5 -34.6 Árbol 4.01e7 6 42 0 ## 4 -58.5 -34.6 Árbol 4.10e6 9 82 0 ## 5 -58.5 -34.5 Árbol 2.61e7 10 35 0 ## 6 -58.5 -34.7 Árbol 2.19e8 3 8 0 ## 7 -58.5 -34.6 Árbol 2.90e7 11 68 0 ## 8 -58.4 -34.6 Árbol 1.20e5 4 42 0 ## 9 -58.4 -34.6 Árbol 1.26e5 15 39 0 ## 10 -58.4 -34.6 Árbol 6.51e4 2 10 0 ## # … with 13 more variables: id_especie &lt;dbl&gt;, nombre_fam &lt;chr&gt;, ## # nombre_gen &lt;chr&gt;, nombre_cie &lt;chr&gt;, nombre_com &lt;chr&gt;, ## # tipo_folla &lt;chr&gt;, origen &lt;chr&gt;, codigo_man &lt;chr&gt;, barrio &lt;chr&gt;, ## # comuna &lt;dbl&gt;, calle &lt;chr&gt;, chapa1 &lt;dbl&gt;, chapa2 &lt;dbl&gt; 11.2.2.1 Transformación de datos tabulados a datos sf Para transformar los datos a tipo sf, tenemos la función st_as_sf(), a la cual le indicamos en el parámetro coords las variables que contienen las coordenadas. Con la función st_set_crs() podemos indicar el sistema de coordenadas de referencia. palos_borrachos &lt;- st_as_sf(palos_borrachos, coords = c(&#39;long&#39;,&#39;lat&#39;)) %&gt;% st_set_crs(4326) glimpse(palos_borrachos) ## Observations: 1,750 ## Variables: 19 ## $ tipo_sitio &lt;chr&gt; &quot;Árbol&quot;, &quot;Árbol&quot;, &quot;Árbol&quot;, &quot;Árbol&quot;, &quot;Árbol&quot;, &quot;Árbol&quot;,… ## $ id_arbol &lt;dbl&gt; 2836, 3009, 3156, 3615, 3619, 4017, 4018, 4307, 4594,… ## $ altura_tot &lt;dbl&gt; 2, 9, 11, 3, 10, 19, 24, 11, 8, 8, 6, 10, 7, 10, 10, … ## $ diametro &lt;dbl&gt; 4, 60, 40, 10, 23, 75, 75, 50, 35, 70, 34, 65, 45, 60… ## $ inclinacio &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 27, 0… ## $ id_especie &lt;dbl&gt; 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 2… ## $ nombre_fam &lt;chr&gt; &quot;Bombacáceas&quot;, &quot;Bombacáceas&quot;, &quot;Bombacáceas&quot;, &quot;Bombacá… ## $ nombre_gen &lt;chr&gt; &quot;Ceiba&quot;, &quot;Ceiba&quot;, &quot;Ceiba&quot;, &quot;Ceiba&quot;, &quot;Ceiba&quot;, &quot;Ceiba&quot;,… ## $ nombre_cie &lt;chr&gt; &quot;Ceiba speciosa&quot;, &quot;Ceiba speciosa&quot;, &quot;Ceiba speciosa&quot;,… ## $ nombre_com &lt;chr&gt; &quot;Palo borracho rosado&quot;, &quot;Palo borracho rosado&quot;, &quot;Palo… ## $ tipo_folla &lt;chr&gt; &quot;Árbol Latifoliado Caducifolio&quot;, &quot;Árbol Latifoliado C… ## $ origen &lt;chr&gt; &quot;Nativo/Autóctono&quot;, &quot;Nativo/Autóctono&quot;, &quot;Nativo/Autóc… ## $ codigo_man &lt;chr&gt; &quot;12-017&quot;, &quot;05-008&quot;, &quot;11-002&quot;, &quot;11-021&quot;, &quot;11-021&quot;, &quot;11… ## $ barrio &lt;chr&gt; &quot;MONSERRAT&quot;, &quot;SAN NICOLAS&quot;, &quot;RECOLETA&quot;, &quot;RECOLETA&quot;, &quot;… ## $ comuna &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… ## $ calle &lt;chr&gt; &quot;Mexico&quot;, &quot;Cordoba Av.&quot;, &quot;Larrea&quot;, &quot;Pacheco De Melo, … ## $ chapa1 &lt;dbl&gt; 1558, 0, 942, 0, 2256, 1614, 1614, 1695, 0, 0, 1815, … ## $ chapa2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ geometry &lt;POINT [°]&gt; POINT (-58.38838 -34.61559), POINT (-58.39205 -… Ahora podemos mapear los palos borrachos en los barrios de CABA que ya estuvimos trabajando! ggplot() + geom_sf(data = barrios_geo) + geom_sf(data = palos_borrachos, color = &quot;darkgreen&quot;, size=.5) + labs(title = &#39;localización de los palos borrachos de Buenos Aires&#39;)+ theme_void()+ theme(legend.position = &#39;none&#39;) 11.2.2.2 Join espacial Ahora probemos la forma de unir la información contenida en palos_borrachos con la de barrios_geo. Para eso usamos st_join(), pero… en qué orden deberíamos unirlos? palos_borrachos %&gt;% st_join(barrios_geo) %&gt;% sample_n(10) ## Simple feature collection with 10 features and 24 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -58.52215 ymin: -34.6865 xmax: -58.37308 ymax: -34.54222 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 10 x 25 ## tipo_sitio id_arbol altura_tot diametro inclinacio id_especie nombre_fam ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Árbol 4.10e7 12 78 0 25 Bombacáce… ## 2 Árbol 1.60e7 10 70 0 25 Bombacáce… ## 3 Árbol 8.41e4 12 60 15 25 Bombacáce… ## 4 Árbol 1.10e8 11 28 0 25 Bombacáce… ## 5 Árbol 1.40e7 10 106 0 25 Bombacáce… ## 6 Árbol 1.70e7 6 36 0 25 Bombacáce… ## 7 Árbol 2.00e7 8 22 0 25 Bombacáce… ## 8 Árbol 1.80e7 7 28 0 25 Bombacáce… ## 9 Árbol 4.00e6 7 22 0 25 Bombacáce… ## 10 Árbol 4.01e7 8 51 10 25 Bombacáce… ## # … with 18 more variables: nombre_gen &lt;chr&gt;, nombre_cie &lt;chr&gt;, ## # nombre_com &lt;chr&gt;, tipo_folla &lt;chr&gt;, origen &lt;chr&gt;, codigo_man &lt;chr&gt;, ## # barrio &lt;chr&gt;, comuna &lt;dbl&gt;, calle &lt;chr&gt;, chapa1 &lt;dbl&gt;, chapa2 &lt;dbl&gt;, ## # geometry &lt;POINT [°]&gt;, BARRIO &lt;fct&gt;, POBLACION &lt;dbl&gt;, VIVIENDAS &lt;dbl&gt;, ## # HOGARES &lt;dbl&gt;, HOGARES_NBI &lt;dbl&gt;, AREA_KM2 &lt;dbl&gt; barrios_geo %&gt;% st_join(palos_borrachos) %&gt;% sample_n(10) ## Simple feature collection with 10 features and 24 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -58.52876 ymin: -34.69224 xmax: -58.35054 ymax: -34.54969 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 10 x 25 ## BARRIO POBLACION VIVIENDAS HOGARES HOGARES_NBI AREA_KM2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 VILLA… 126374 38915 39848 4150 9.29 ## 2 PALER… 226534 141710 103167 2425 16.0 ## 3 RETIRO 65413 34363 24147 3104 4.53 ## 4 PARQU… 17489 7530 6466 85 1.38 ## 5 VILLA… 126374 38915 39848 4150 9.29 ## 6 VILLA… 21736 9757 8285 291 1.85 ## 7 VILLA… 66521 29002 24035 280 6.37 ## 8 PARQU… 17489 7530 6466 85 1.38 ## 9 VILLA… 32502 14141 12002 340 2.57 ## 10 FLORES 164310 65408 60248 6234 8.59 ## # … with 19 more variables: geometry &lt;POLYGON [°]&gt;, tipo_sitio &lt;chr&gt;, ## # id_arbol &lt;dbl&gt;, altura_tot &lt;dbl&gt;, diametro &lt;dbl&gt;, inclinacio &lt;dbl&gt;, ## # id_especie &lt;dbl&gt;, nombre_fam &lt;chr&gt;, nombre_gen &lt;chr&gt;, ## # nombre_cie &lt;chr&gt;, nombre_com &lt;chr&gt;, tipo_folla &lt;chr&gt;, origen &lt;chr&gt;, ## # codigo_man &lt;chr&gt;, barrio &lt;chr&gt;, comuna &lt;dbl&gt;, calle &lt;chr&gt;, ## # chapa1 &lt;dbl&gt;, chapa2 &lt;dbl&gt; Si queremos visualizar la densidad de borrachos por barrio: palos_borrachos_barrio &lt;- barrios_geo %&gt;% st_join(palos_borrachos) palos_borrachos_barrio %&gt;% group_by(BARRIO) %&gt;% summarise(densidad=n()/unique(AREA_KM2)) %&gt;% ggplot(aes(fill=densidad, label=BARRIO))+ geom_sf()+ geom_sf_text( color=&#39;white&#39;)+ theme_void() "],
["mapa-de-data-science.html", "Capítulo-12 Mapa de Data Science", " Capítulo-12 Mapa de Data Science Disclaimer: Muchas de las imágenes, aunque no todas, fueron tomadas de búsquedas de google imágenes y no se desconoce su autoría. "],
["tecnicas.html", "Capítulo-13 Técnicas ", " Capítulo-13 Técnicas "],
["segun-tipo-de-aprendizaje.html", "13.1 Según tipo de aprendizaje:", " 13.1 Según tipo de aprendizaje: 13.1.1 Aprendizaje Supervisado: 13.1.1.1 Desde estadística: Modelo Lineal Ridge, Lasso, Elastic Net, Splines, etc. Regresión logísitca set.seed(123) df &lt;- tibble(x = c(rchisq(20, 2),rchisq(20, 10)), y = rep(c(0,1), each=20)) ggplot(df, aes(x, y))+ geom_point()+ geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), se = FALSE) Support Vector Machine Naive Bayes Markov chains … 13.1.1.2 Desde computación: Sistema experto K nearest neighbors Árboles de decisión Ensambles de árboles Random Forest (Bagging) XG-BOOST (Boosting) Y muchos más… 13.1.1.3 Deep Learning: Multi-layer perceptron network Convolutional Neural Network Long-short Term Memory Neural Network … 13.1.2 Aprendizaje No supervisado: 13.1.2.1 Clustering: K-means Patition around medioids DBSCAN 13.1.2.2 Reducción de dimensionalidad: PCA LSI T-SNE 13.1.3 Aprendizaje Semi-supervisado 13.1.4 Modelos generativos Por ejemplo: https://transformer.huggingface.co/doc/gpt2-large "],
["segun-el-tipo-de-datos.html", "13.2 Según el tipo de datos:", " 13.2 Según el tipo de datos: 13.2.1 Datos estructurados: 13.2.1.1 Datos tabulados iris ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 1 5.1 3.5 1.4 0.2 setosa ## 2 2 4.9 3.0 1.4 0.2 setosa ## 3 3 4.7 3.2 1.3 0.2 setosa ## 4 4 4.6 3.1 1.5 0.2 setosa ## 5 5 5.0 3.6 1.4 0.2 setosa ## 6 6 5.4 3.9 1.7 0.4 setosa ## 7 7 4.6 3.4 1.4 0.3 setosa ## 8 8 5.0 3.4 1.5 0.2 setosa ## 9 9 4.4 2.9 1.4 0.2 setosa ## 10 10 4.9 3.1 1.5 0.1 setosa ## 11 11 5.4 3.7 1.5 0.2 setosa ## 12 12 4.8 3.4 1.6 0.2 setosa ## 13 13 4.8 3.0 1.4 0.1 setosa ## 14 14 4.3 3.0 1.1 0.1 setosa ## 15 15 5.8 4.0 1.2 0.2 setosa ## 16 16 5.7 4.4 1.5 0.4 setosa ## 17 17 5.4 3.9 1.3 0.4 setosa ## 18 18 5.1 3.5 1.4 0.3 setosa ## 19 19 5.7 3.8 1.7 0.3 setosa ## 20 20 5.1 3.8 1.5 0.3 setosa ## 21 21 5.4 3.4 1.7 0.2 setosa ## 22 22 5.1 3.7 1.5 0.4 setosa ## 23 23 4.6 3.6 1.0 0.2 setosa ## 24 24 5.1 3.3 1.7 0.5 setosa ## 25 25 4.8 3.4 1.9 0.2 setosa ## 26 26 5.0 3.0 1.6 0.2 setosa ## 27 27 5.0 3.4 1.6 0.4 setosa ## 28 28 5.2 3.5 1.5 0.2 setosa ## 29 29 5.2 3.4 1.4 0.2 setosa ## 30 30 4.7 3.2 1.6 0.2 setosa ## 31 31 4.8 3.1 1.6 0.2 setosa ## 32 32 5.4 3.4 1.5 0.4 setosa ## 33 33 5.2 4.1 1.5 0.1 setosa ## 34 34 5.5 4.2 1.4 0.2 setosa ## 35 35 4.9 3.1 1.5 0.2 setosa ## 36 36 5.0 3.2 1.2 0.2 setosa ## 37 37 5.5 3.5 1.3 0.2 setosa ## 38 38 4.9 3.6 1.4 0.1 setosa ## 39 39 4.4 3.0 1.3 0.2 setosa ## 40 40 5.1 3.4 1.5 0.2 setosa ## 41 41 5.0 3.5 1.3 0.3 setosa ## 42 42 4.5 2.3 1.3 0.3 setosa ## 43 43 4.4 3.2 1.3 0.2 setosa ## 44 44 5.0 3.5 1.6 0.6 setosa ## 45 45 5.1 3.8 1.9 0.4 setosa ## 46 46 4.8 3.0 1.4 0.3 setosa ## 47 47 5.1 3.8 1.6 0.2 setosa ## 48 48 4.6 3.2 1.4 0.2 setosa ## 49 49 5.3 3.7 1.5 0.2 setosa ## 50 50 5.0 3.3 1.4 0.2 setosa ## 51 51 7.0 3.2 4.7 1.4 versicolor ## 52 52 6.4 3.2 4.5 1.5 versicolor ## 53 53 6.9 3.1 4.9 1.5 versicolor ## 54 54 5.5 2.3 4.0 1.3 versicolor ## 55 55 6.5 2.8 4.6 1.5 versicolor ## 56 56 5.7 2.8 4.5 1.3 versicolor ## 57 57 6.3 3.3 4.7 1.6 versicolor ## 58 58 4.9 2.4 3.3 1.0 versicolor ## 59 59 6.6 2.9 4.6 1.3 versicolor ## 60 60 5.2 2.7 3.9 1.4 versicolor ## 61 61 5.0 2.0 3.5 1.0 versicolor ## 62 62 5.9 3.0 4.2 1.5 versicolor ## 63 63 6.0 2.2 4.0 1.0 versicolor ## 64 64 6.1 2.9 4.7 1.4 versicolor ## 65 65 5.6 2.9 3.6 1.3 versicolor ## 66 66 6.7 3.1 4.4 1.4 versicolor ## 67 67 5.6 3.0 4.5 1.5 versicolor ## 68 68 5.8 2.7 4.1 1.0 versicolor ## 69 69 6.2 2.2 4.5 1.5 versicolor ## 70 70 5.6 2.5 3.9 1.1 versicolor ## 71 71 5.9 3.2 4.8 1.8 versicolor ## 72 72 6.1 2.8 4.0 1.3 versicolor ## 73 73 6.3 2.5 4.9 1.5 versicolor ## 74 74 6.1 2.8 4.7 1.2 versicolor ## 75 75 6.4 2.9 4.3 1.3 versicolor ## 76 76 6.6 3.0 4.4 1.4 versicolor ## 77 77 6.8 2.8 4.8 1.4 versicolor ## 78 78 6.7 3.0 5.0 1.7 versicolor ## 79 79 6.0 2.9 4.5 1.5 versicolor ## 80 80 5.7 2.6 3.5 1.0 versicolor ## 81 81 5.5 2.4 3.8 1.1 versicolor ## 82 82 5.5 2.4 3.7 1.0 versicolor ## 83 83 5.8 2.7 3.9 1.2 versicolor ## 84 84 6.0 2.7 5.1 1.6 versicolor ## 85 85 5.4 3.0 4.5 1.5 versicolor ## 86 86 6.0 3.4 4.5 1.6 versicolor ## 87 87 6.7 3.1 4.7 1.5 versicolor ## 88 88 6.3 2.3 4.4 1.3 versicolor ## 89 89 5.6 3.0 4.1 1.3 versicolor ## 90 90 5.5 2.5 4.0 1.3 versicolor ## 91 91 5.5 2.6 4.4 1.2 versicolor ## 92 92 6.1 3.0 4.6 1.4 versicolor ## 93 93 5.8 2.6 4.0 1.2 versicolor ## 94 94 5.0 2.3 3.3 1.0 versicolor ## 95 95 5.6 2.7 4.2 1.3 versicolor ## 96 96 5.7 3.0 4.2 1.2 versicolor ## 97 97 5.7 2.9 4.2 1.3 versicolor ## 98 98 6.2 2.9 4.3 1.3 versicolor ## 99 99 5.1 2.5 3.0 1.1 versicolor ## 100 100 5.7 2.8 4.1 1.3 versicolor ## 101 101 6.3 3.3 6.0 2.5 virginica ## 102 102 5.8 2.7 5.1 1.9 virginica ## 103 103 7.1 3.0 5.9 2.1 virginica ## 104 104 6.3 2.9 5.6 1.8 virginica ## 105 105 6.5 3.0 5.8 2.2 virginica ## 106 106 7.6 3.0 6.6 2.1 virginica ## 107 107 4.9 2.5 4.5 1.7 virginica ## 108 108 7.3 2.9 6.3 1.8 virginica ## 109 109 6.7 2.5 5.8 1.8 virginica ## 110 110 7.2 3.6 6.1 2.5 virginica ## 111 111 6.5 3.2 5.1 2.0 virginica ## 112 112 6.4 2.7 5.3 1.9 virginica ## 113 113 6.8 3.0 5.5 2.1 virginica ## 114 114 5.7 2.5 5.0 2.0 virginica ## 115 115 5.8 2.8 5.1 2.4 virginica ## 116 116 6.4 3.2 5.3 2.3 virginica ## 117 117 6.5 3.0 5.5 1.8 virginica ## 118 118 7.7 3.8 6.7 2.2 virginica ## 119 119 7.7 2.6 6.9 2.3 virginica ## 120 120 6.0 2.2 5.0 1.5 virginica ## 121 121 6.9 3.2 5.7 2.3 virginica ## 122 122 5.6 2.8 4.9 2.0 virginica ## 123 123 7.7 2.8 6.7 2.0 virginica ## 124 124 6.3 2.7 4.9 1.8 virginica ## 125 125 6.7 3.3 5.7 2.1 virginica ## 126 126 7.2 3.2 6.0 1.8 virginica ## 127 127 6.2 2.8 4.8 1.8 virginica ## 128 128 6.1 3.0 4.9 1.8 virginica ## 129 129 6.4 2.8 5.6 2.1 virginica ## 130 130 7.2 3.0 5.8 1.6 virginica ## 131 131 7.4 2.8 6.1 1.9 virginica ## 132 132 7.9 3.8 6.4 2.0 virginica ## 133 133 6.4 2.8 5.6 2.2 virginica ## 134 134 6.3 2.8 5.1 1.5 virginica ## 135 135 6.1 2.6 5.6 1.4 virginica ## 136 136 7.7 3.0 6.1 2.3 virginica ## 137 137 6.3 3.4 5.6 2.4 virginica ## 138 138 6.4 3.1 5.5 1.8 virginica ## 139 139 6.0 3.0 4.8 1.8 virginica ## 140 140 6.9 3.1 5.4 2.1 virginica ## 141 141 6.7 3.1 5.6 2.4 virginica ## 142 142 6.9 3.1 5.1 2.3 virginica ## 143 143 5.8 2.7 5.1 1.9 virginica ## 144 144 6.8 3.2 5.9 2.3 virginica ## 145 145 6.7 3.3 5.7 2.5 virginica ## 146 146 6.7 3.0 5.2 2.3 virginica ## 147 147 6.3 2.5 5.0 1.9 virginica ## 148 148 6.5 3.0 5.2 2.0 virginica ## 149 149 6.2 3.4 5.4 2.3 virginica ## 150 150 5.9 3.0 5.1 1.8 virginica 13.2.1.2 Series de Tiempo: ARIMA Wavelets 13.2.1.3 Grafos: 13.2.1.4 Datos geográficos 13.2.2 Datos no estructurados: 13.2.2.1 texto: LDA Word Embeddings 13.2.2.2 imagenes 13.2.2.3 sonido "],
["desde-el-punto-de-vista-del-remuestreo.html", "13.3 Desde el punto de vista del remuestreo:", " 13.3 Desde el punto de vista del remuestreo: Test-train Cross-Validation Bootstraping "],
["desde-el-punto-de-vista-de-la-optimizacion.html", "13.4 Desde el punto de vista de la optimización:", " 13.4 Desde el punto de vista de la optimización: Gradient Descent Algorítmos Genéticos Inferencia Variacional "],
["flujo-de-trabajo.html", "Capítulo-14 Flujo de trabajo", " Capítulo-14 Flujo de trabajo Preprocesamiento Entrenamiento Validación "],
["meta-flujo-de-trabajo.html", "Capítulo-15 Meta flujo de trabajo", " Capítulo-15 Meta flujo de trabajo Ejemplo: https://github.com/DiegoKoz/intro_ds "],
["otros-temas.html", "Capítulo-16 Otros temas", " Capítulo-16 Otros temas Sobreajuste Trade-off sesgo varianza Trade-off Precision-recall "],
["implementaciones.html", "Capítulo-17 Implementaciones:", " Capítulo-17 Implementaciones: Caret Tidymodels Sci-kit learn H20 Google Cloud Watson IBM "],
["references.html", "References", " References "]
]
